{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "3b_total.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3vVHx6AVGrtM",
        "dMoJ-G9mqDqa",
        "152zECujzPMk",
        "ailCy7DO4--y",
        "bpJm0H4y5lgz",
        "1Tx8nxpt52wi"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivantor0/colab-usefulness/blob/master/Train%20heptabot%20scaled%20to%203b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YONnGjpAYUdU"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrtR2urJV3ST"
      },
      "source": [
        "##### Copyright 2020 The T5 Authors\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWdCSqJ6WHBh"
      },
      "source": [
        "# Copyright 2019 The T5 Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSeyoqE7WMwu"
      },
      "source": [
        "# Fine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\n",
        "## _Or: What does T5 know?_\n",
        "\n",
        "*The following tutorial guides you through the process of fine-tuning a pre-trained T5 model, evaluating its accuracy, and using it for prediction,\n",
        "all on a free Google Cloud TPU <a href=\"https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>.*\n",
        "\n",
        "### Background\n",
        "\n",
        "T5 was introduced in the paper [_Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer_](https://arxiv.org/abs/1910.10683). In that paper, we provided a comprehensive picture of how we pre-trained a standard text-to-text Transformer model on a large text corpus, achieving state-of-the-art results on many NLP tasks after fine-tuning.\n",
        "\n",
        "We pre-trained T5 on a mixture of supervised and unsupervised tasks with the majoriy of data coming from an unlabeled dataset we developed called [C4](https://www.tensorflow.org/datasets/catalog/c4). C4 is based on a massive scrape of the web produced by [Common Crawl](https://commoncrawl.org). Loosely speaking, pre-training on C4 ideally gives T5 an understanding of natural language in addition to general world knowledge.\n",
        "\n",
        "### How can we assess what T5 knows?\n",
        "\n",
        "As the name implies, T5 is a text-to-text model, which enables us to train it on arbitrary tasks involving a textual input and output. As we showed in our paper, a huge variety of NLP tasks can be cast in this format, including translation, summarization, and even classification and regression tasks.\n",
        "\n",
        "One way to use this text-to-text framework is on reading comprehension problems, where the model is fed some context along with a orig_text and is trained to predict the orig_text's corr_text. For example, we might feed the model the text from the Wikipedia article about [Hurrican Connie](https://en.wikipedia.org/wiki/Hurricane_Connie) along with the orig_text \"On what date did Hurricane Connie occur?\" and train the model to predict the corr_text \"August 3rd, 1955\".\n",
        "A related task is open-domain orig_text answering (QA) where the model is not provided with this oracle context. Typically, open-domain QA systems include a mechanism to look up information in an external knowledge source. This setting is similar to an \"open-book\" exam.\n",
        "\n",
        "In this notebook, we'll be training T5 on a variant of this task which we call **closed-book orig_text answering**. In closed-book QA, we feed the model a orig_text *without any context or access to external knowledge* and train it to predict the corr_text. Since the model doesn't receive any context, the primary way it can learn to corr_text these orig_texts is based on the \"knowledge\" it obtained during pre-training. We don't expect T5 to contain super specific information, so we will be focusing on two orig_text-answering datasets which largely include trivia orig_texts (i.e. facts about well-known subjects). [Similar](https://arxiv.org/abs/1909.01066) [investigations](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) have recently been done to test the knowledge stored by BERT and GPT-2.\n",
        "\n",
        "T5 was not pre-trained on closed-book QA, so in this notebook we'll first create two new tasks and then use the [`t5`](https://github.com/google-research/text-to-text-transfer-transformer) library to fine-tune, evaluate, and obtain predictions from T5. In the end, T5's performance on closed-book QA can give us a sense of what kind (and how much) information T5 managed to learn during pre-training.\n",
        "\n",
        "## State-of-the-art Results\n",
        "We published a [more in-depth investigation](https://arxiv.org/abs/2002.08910) of closed-book QA with T5 where we achieved SOTA on open-domain variants of WebQuestions and TriviaQA in addition to surpisingly strong results on Natural Questions. The code in this notebook is a simplified version of those experiments but still produces good results.\n",
        "\n",
        "For code to reproduce our best results, please see the [t5_closed_book_qa](https://github.com/google-research/google-research/tree/master/t5_closed_book_qa) repo.\n",
        "\n",
        "\n",
        "### Caveats\n",
        "\n",
        "* While we provide instructions for running on a [Cloud TPU](https://cloud.google.com/tpu/) via Colab for free, a [Google Cloud Storage (GCS)](http://console.cloud.google.com/storage) bucket is required for storing model parameters and data. The [GCS free tier](https://cloud.google.com/free/) provides 5 GB of storage, which should be enough to train the `large` model and smaller but not the `3B` or `11B` parameter models. You can use part of your initial $300 credit to get more space.\n",
        "* The Cloud TPU provided by Colab (a `v2-8`) does not have enough memory to fine-tune the `11B` parameter model. For this model, you will need to fine-tune inside of a GCP instance (see [README](https://github.com/google-research/text-to-text-transfer-transformer/)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAb_APDrefs6"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDeE_yVuHMYg"
      },
      "source": [
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   1. Create a Cloud Storage bucket for your data and model checkpoints at http://console.cloud.google.com/storage, and fill in the `BASE_DIR` parameter in the following form. There is a [free tier](https://cloud.google.com/free/) if you do not yet have an account.\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   1. Run the following cell and follow instructions to:\n",
        "    *  Set up a Colab TPU running environment\n",
        "    *   Verify that you are connected to a TPU device\n",
        "    *   Upload your credentials to TPU to access your GCS bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h86a4THsRv29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "298cc08d-d39b-43f9-a030-347962c4f5b4"
      },
      "source": [
        "!pip install spacy==1.9.0\n",
        "!python -m spacy download -d en_core_web_sm-1.2.0\n",
        "!python -m spacy link en_core_web_sm en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/ce/afee53c365617e5f3e58825d71421bce14949a15f7150742d2a7b8859c53/spacy-1.9.0.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (1.18.5)\n",
            "Collecting murmurhash<0.27,>=0.26\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/53/1f428861e59c2382e22b8839d03cc315e1a7633a827497b3d389b8d8772d/murmurhash-0.26.4.tar.gz\n",
            "Collecting cymem<1.32,>=1.30\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/0f/d29aa68c55db37844c77e7e96143bd96651fd0f4453c9f6ee043ac846b77/cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/88/57a818051f3d71e800bfb7ba4df56d3ea5793482ef11f1d2109b726f3bac/preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.6MB/s \n",
            "\u001b[?25hCollecting thinc<6.6.0,>=6.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/9b/78fab962e0c8b55e3a745ebf2458708dfc7922c55eca3a9bff0233b25294/thinc-6.5.2.tar.gz (926kB)\n",
            "\u001b[K     |████████████████████████████████| 931kB 28.8MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting pip<10.0.0,>=9.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (1.15.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (1.0.1)\n",
            "Collecting ujson>=1.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/1a/36ead6ae1bc3b82ea864ef87f8c8e1e06bec3a745dc65915f47b46f241d0/ujson-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (175kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 50.9MB/s \n",
            "\u001b[?25hCollecting dill<0.3,>=0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 36.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (2.23.0)\n",
            "Collecting regex<2017.12.1,>=2017.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/fb/e894cf877cc3ad788892899c23e2a73e3e54524c4ec6e5d3826fe153af80/regex-2017.11.09.tar.gz (608kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 32.1MB/s \n",
            "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy==1.9.0) (1.12.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy==1.9.0) (4.41.1)\n",
            "Collecting cytoolz<0.9,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/e6/ccc124714dcc1bd511e64ddafb4d5d20ada2533b92e3173a4cf09e0d0831/cytoolz-0.8.2.tar.gz (386kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy==1.9.0) (1.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (2020.6.20)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (0.2.5)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy==1.9.0) (0.10.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (0.5.1)\n",
            "Building wheels for collected packages: spacy, murmurhash, thinc, dill, regex, ftfy, cytoolz\n",
            "  Building wheel for spacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-1.9.0-cp36-cp36m-linux_x86_64.whl size=7776464 sha256=9f5024443c98b4787fa9037ac0ed49697f092752f5dcd9b5c387ba1e82e6f152\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/b1/94/5e66dac91b157627f0dfc81b3af926e16919e7c0ef9f7e0616\n",
            "  Building wheel for murmurhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for murmurhash: filename=murmurhash-0.26.4-cp36-cp36m-linux_x86_64.whl size=41066 sha256=eb64e327af22dfa632475a64d890b2f2995d745b53fc36be219e6caf9c9c55a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/af/51/9efd49862c6dcb6439baaa235714fc4de5cecf3e01613b2fef\n",
            "  Building wheel for thinc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thinc: filename=thinc-6.5.2-cp36-cp36m-linux_x86_64.whl size=2313260 sha256=2c00f72118486ae510dd0f81b38978e63bae163056bf655ddaae32ecae91d26b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/36/8b/306d475aa414ff9fcec211da3da5d5e59582219d57f7ea9fa1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.2.9-cp36-none-any.whl size=77404 sha256=727004acece482b2bbe01f7c9e6e40341a6192f93ce67d9d3376d6cbf58c59f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.11.9-cp36-cp36m-linux_x86_64.whl size=538848 sha256=668c01c8fa8b73a778d352976102d3703ed4fe4677382b316c9bc818c0f31c3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/ae/8f/cab02cdf653ac0a2e9d9ec302721ae3c20052a36951addd111\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-4.4.3-cp36-none-any.whl size=41068 sha256=4ec1da25cd1047ffcd3f946c1b2481c430aa305199686b9d2dd96ce7a157dbf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.8.2-cp36-cp36m-linux_x86_64.whl size=1093018 sha256=4f6ecbba168447fd2b28438ed200cedd68e6d8855b0473432443694f78e22b25\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b1/86/c92e4d36b690208fff8471711b85eaa6bc6d19860a86199a09\n",
            "Successfully built spacy murmurhash thinc dill regex ftfy cytoolz\n",
            "\u001b[31mERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement spacy>=2.0.18; python_version < \"3.8\", but you'll have spacy 1.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 1.9.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: murmurhash, cymem, preshed, cytoolz, plac, dill, thinc, pip, ujson, regex, ftfy, spacy\n",
            "  Found existing installation: murmurhash 1.0.2\n",
            "    Uninstalling murmurhash-1.0.2:\n",
            "      Successfully uninstalled murmurhash-1.0.2\n",
            "  Found existing installation: cymem 2.0.3\n",
            "    Uninstalling cymem-2.0.3:\n",
            "      Successfully uninstalled cymem-2.0.3\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: dill 0.3.2\n",
            "    Uninstalling dill-0.3.2:\n",
            "      Successfully uninstalled dill-0.3.2\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed cymem-1.31.2 cytoolz-0.8.2 dill-0.2.9 ftfy-4.4.3 murmurhash-0.26.4 pip-9.0.3 plac-0.9.6 preshed-1.0.1 regex-2017.11.9 spacy-1.9.0 thinc-6.5.2 ujson-3.1.0\n",
            "\n",
            "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
            "\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 52.2MB 76.4MB/s \n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "\u001b[?25hRequirement already satisfied: spacy<2.0.0,>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: murmurhash<0.27,>=0.26 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-1.2.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
            "    --> /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en').\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA4ygKpsSQz1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "c12656b9-5231-4ce6-8294-6348c9fd64e2"
      },
      "source": [
        "!pip install mosestokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mosestokenizer\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/b3/c0af235b16c4f44a2828ef017f7947d1262b2646e440f85c6a2ff26a8c6f/mosestokenizer-1.1.0.tar.gz\n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from mosestokenizer)\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e6/805db6867faacb488b44ba8e0829ef4de151dd0499f3c5da5f4ad11698a7/openfile-0.0.7-py3-none-any.whl\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/04/cb/70ed842d9a43460eedaa11f7503b4ab6537b43b63f0d854d59d8e150fac1/uctools-1.3.0.tar.gz\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/41/7b/34bf8fb69426d8a18bcc61081e9d126f4fcd41c3c832072bef39af1602cd/toolwrapper-2.1.0.tar.gz\n",
            "Building wheels for collected packages: mosestokenizer, uctools, toolwrapper\n",
            "  Running setup.py bdist_wheel for mosestokenizer ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a2/e7/48/48d5e0f9c0cd5def2dfd7cb8543945f906448ed1313de24a29\n",
            "  Running setup.py bdist_wheel for uctools ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/06/b6/8f/935d5bf5bca85d47c6f5ec31641879bba057d336ab36b1e773\n",
            "  Running setup.py bdist_wheel for toolwrapper ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/84/ea/29/e02f3b855bf19344972092873a1091b329309bbc3d3d0cbaef\n",
            "Successfully built mosestokenizer uctools toolwrapper\n",
            "Installing collected packages: openfile, uctools, toolwrapper, mosestokenizer\n",
            "Successfully installed mosestokenizer-1.1.0 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQQH8r2VXnDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "437c6143-6b30-4998-cd1f-ff0661aaff93"
      },
      "source": [
        "!wget https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\n",
        "!tar -xzf m2scorer.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-09 12:12:05--  https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\n",
            "Resolving www.comp.nus.edu.sg (www.comp.nus.edu.sg)... 45.60.31.225\n",
            "Connecting to www.comp.nus.edu.sg (www.comp.nus.edu.sg)|45.60.31.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22836 (22K) [application/x-gzip]\n",
            "Saving to: ‘m2scorer.tar.gz’\n",
            "\n",
            "m2scorer.tar.gz     100%[===================>]  22.30K  83.0KB/s    in 0.3s    \n",
            "\n",
            "2020-08-09 12:12:07 (83.0 KB/s) - ‘m2scorer.tar.gz’ saved [22836/22836]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNaiuRTRx5ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b2162275-104b-40fa-d453-d67a31472820"
      },
      "source": [
        "!git clone https://github.com/keisks/jfleg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'jfleg'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (170/170), 777.12 KiB | 6.42 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfTs7kzNyxmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a3f6023e-00ba-4feb-f6aa-3e13d092d879"
      },
      "source": [
        "!wget https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
        "!tar -xzf conll14st-test-data.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-09 12:12:13--  https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
            "Resolving www.comp.nus.edu.sg (www.comp.nus.edu.sg)... 45.60.31.225\n",
            "Connecting to www.comp.nus.edu.sg (www.comp.nus.edu.sg)|45.60.31.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 643482 (628K) [application/x-gzip]\n",
            "Saving to: ‘conll14st-test-data.tar.gz’\n",
            "\n",
            "conll14st-test-data 100%[===================>] 628.40K   321KB/s    in 2.0s    \n",
            "\n",
            "2020-08-09 12:12:16 (321 KB/s) - ‘conll14st-test-data.tar.gz’ saved [643482/643482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM1P8KYJqbSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "688f28e4-0646-4058-f6bd-b83235ef99f4"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o68EireWQAp8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "76c638eb-9abf-4360-e57c-d62a6f5f041c"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5==0.6.0\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "BASE_DIR = \"gs://ml-bucket-isikus/t5-base-model\" #@param { type: \"string\" }\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 4.1MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 307kB 3.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 2.6MB 464kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 778kB 1.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 371kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 1.1MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 12.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 396kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 890kB 2.0MB/s \n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "\u001b[?25h  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
            "\u001b[?25hSetting up GCS access...\n",
            "Running on TPU: grpc://10.90.169.50:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7HvbeSKi_b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4bf3cca3-2c8b-4ee7-fcb6-da01d3e0afc3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAN3xsIX78eM"
      },
      "source": [
        "t5.data.utils.set_global_cache_dirs([BASE_DIR, os.getcwd()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XH9lh-gQAp_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "442618d3-25d4-4791-f49f-453a3dbd0a5f"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'better-record'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://ml-bucket-isikus/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao9KKRSzDX8l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d3c576a4-3d09-44fd-b670-59bd7c9b1afc"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vVHx6AVGrtM"
      },
      "source": [
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3c-wp6n_8O1"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMoJ-G9mqDqa"
      },
      "source": [
        "# Creating new Tasks and Mixture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoLPQhE6bef"
      },
      "source": [
        "Two core components of the T5 library are `Task` and `Mixture` objects.\n",
        "\n",
        "A `Task` is a dataset along with preprocessing functions and evaluation metrics. A `Mixture` is a collection of `Task` objects along with a mixing rate or a function defining how to compute a mixing rate based on the properties of the constituent `Tasks`.\n",
        "\n",
        "For this example, we will fine-tune the model to do closed-book orig_text answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152zECujzPMk"
      },
      "source": [
        "### correct\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjEonhK3zNRu"
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "corr_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"correct-train.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"correct-target.tsv\")\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Ja8akCX1dR"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPOteeqctpzw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "8a2ed55a-963e-497e-8960-6cd6c21b49fb"
      },
      "source": [
        "def corr_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(corr_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(corr_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'Armageddon is my favourite science fiction movie. The plot is about how to survive in the bad situation. It presented the cooperation from everybody such as new technology, joining between american and russian astronauts and the private people who had great experience in digging. I felt of the bravehearts who make a sacrifice. The soundtrack was pretty good. I felt sad when the character played by Bruce Willis called his daughter on the earth, prior to exploding a main meteor. Armageddon was directed by Michael Bay.', 'corr_text': b'Armageddon is my favourite science fiction movie. The plot is about how to survive in a bad situation. It presented the cooperation from everybody such as new technology, cooperation between American and Russian astronauts and the private people who had great experience in digging. I felt the bravehearts who make a sacrifice. The soundtrack was pretty good. I felt sad when the character played by Bruce Willis called his daughter on the earth, prior to exploding a main meteor. Armageddon was directed by Michael Bay.'}\n",
            "{'orig_text': b'This apartment, that is my home, is too large for a single retired person and so its renting. Its located in the prime Pacific Eights Area and it has a breathtaking view. It was completely remodeled five years ago and so every fixtures are in good order. Walls was repainted last year. Its quite a roomy house. In fact it has a large entrance hall, tree bedrooms, a dining room with an island kitchen, a living room, two bathrooms. There is a walk-in closet near to every bedroom and a great all-wall wood bookshelf in the entrance hall. It has quite high ceilings and marble floors. Furthermore, all rooms have a balcony and the living room has a veranda with a barbeque. This veranda is the very nicest particular of the apartment, because its quite large and supplied with rustic design furniture, so that you can have there your dining on summer. The rent is very affordable at only $1850 per month. Not for nothing, but its really a business. See and believe!', 'corr_text': b'This apartment, that is my home, is too large for a single retired person and so its renting. Its located in the prime Pacific Heights Area and it has a breathtaking view. It was completely remodeled five years ago and so all fixtures are in good order. Walls were repainted last year. Its quite a roomy house. In fact it has a large entrance hall, three bedrooms, a dining room with an island kitchen, a living room and two bathrooms. There is a walk-in closet near to every bedroom and a great all-wall wood bookshelf in the entrance hall. It has quite high ceilings and marble floors. Furthermore, all rooms have a balcony and the living room has a veranda with a barbeque. This veranda is the very nicest part of the apartment, because its quite large and supplied with rustic design furniture, so that you can have your dinner there in summer. The rent is very affordable at only $1850 per month. Not for nothing, but its really a business. See and believe!'}\n",
            "{'orig_text': b\"I am a doctor. I work at a hospital. I work long hours and take short breaks. I take care of my patients. When I finish my work, I feel tired. However, I love my job because I think it's rewarding.I feel very proud to be a doctor.\", 'corr_text': b\"I am a doctor. I work in a hospital. I work long hours and take short breaks. I take care of my patients. When I finish my work, I feel tired. However, I love my job because I think it's rewarding.I feel very proud to be a doctor.\"}\n",
            "{'orig_text': b\"My father is short and fat.He wears just a brown pants.He has a short gray hair.He has a good sense of humour,so he's funny.My mother wears purple singlet, dark blue pants and glasses,because she's very clever.My sister wears red T-shirt and blue pants.She's funny and sociable,but she's naughty.She has a long,straight red hair,like and mother.I love them.\", 'corr_text': b\"My father is short and fat.He wears just a brown pants.He has a short gray hair.He has a good sense of humour,so he's funny.My mother wears purple singlet, dark blue pants and glasses,because she's very clever.My sister wears red T-shirt and blue pants.She's funny and sociable,but she's naughty.She has a long,straight red hair,like and mother.I love them.\"}\n",
            "{'orig_text': b\"Jane,the bus stop is on Gold street,walk along Gold st. then turn left at London Road,turn right Green avenue. My home is next to the supermarket and next to the restaurant.It's opposite to the park. The park is on the corner of Green avenue and Liverpool Road.\", 'corr_text': b\"Jane,the bus stop is on Gold street,walk along Gold st. then turn left at London Road,turn right Green avenue. My home is next to the supermarket and next to the restaurant.It's opposite to the park. The park is on the corner of Green avenue and Liverpool Road.\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imbEYc-tQ2gB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "bcdd4306-d19f-43e7-ab17-461961a93b5d"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/correct-train.tsv correct_train.tsv\n",
        "\n",
        "with open(\"correct_train.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  correct_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/correct-train.tsv...\n",
            "\\ [1/1 files][ 53.0 MiB/ 53.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/53.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUYT7JmX9Tj"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8tNn6HMYLMb"
      },
      "source": [
        "def correction_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"correction: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm1Pm2aRZ9Ow"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJyRavOpZ7UW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74120e4d-886f-42f5-93fb-f8b533ea7eda"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"correct\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=corr_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[correction_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # metric_fns=[t5.evaluation.metrics.accuracy]\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe4o_0jFbP-p"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I64TqHGxbOJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "165d70e1-043d-44f9-d014-f7977f854f73"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"correct\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'correction: The chart illustrates the amount of illiterate males and females in the world in 2020. Overall, there is a gap that divides countries with high level of literacy and illiterate countries. <br> To begin with, it is clearly seen that there are more illiterate females than males in every area. Though, the difference may be small, for example in Latin America or may be relatively large like in East Asia and the second type is more common. All regions can be divided in two groups. The first one is with high level of illiteracy and it includes South Asia, Arab States and Sub-Saharan Afrrica. Another consist of Developed countries, Latin America/Caribbean and East Asia/Oceania and shows low level of illiterate people (less than 20). <br> To sum up, the few facts should be emphasized. In 2020 there will be two different groups of areas, depending on level of illiteracy. Besides, the amount of illiterate females will be bigger than illiterate males in every area.', 'inputs': array([11698,    10,    37,  5059, 11485,     7,     8,   866,    13,\n",
            "           3,   173,  9842,   342,  5069,     7,    11,  3955,     7,\n",
            "          16,     8,   296,    16,  6503,     5,  9126,     6,   132,\n",
            "          19,     3,     9,  6813,    24, 14514,     7,  1440,    28,\n",
            "         306,   593,    13, 18298,    11,     3,   173,  9842,   342,\n",
            "        1440,     5,     3,     2,   115,    52,  3155,   304,  1731,\n",
            "          28,     6,    34,    19,  3133,   894,    24,   132,    33,\n",
            "          72,     3,   173,  9842,   342,  3955,     7,   145,  5069,\n",
            "           7,    16,   334,   616,     5,  4229,     6,     8,  1750,\n",
            "         164,    36,   422,     6,    21,   677,    16,  6271,  1371,\n",
            "          42,   164,    36,  4352,   508,   114,    16,  1932,  3826,\n",
            "          11,     8,   511,   686,    19,    72,  1017,     5,   432,\n",
            "        6266,    54,    36,  8807,    16,   192,  1637,     5,    37,\n",
            "         166,    80,    19,    28,   306,   593,    13,     3,   173,\n",
            "        9842,  4710,    11,    34,   963,  1013,  3826,     6,  9217,\n",
            "        1323,    11,  3325,    18,   134,     9, 14888,    29,    71,\n",
            "          89,    52,  2234,     9,     5,  2351,  5608,    13,     3,\n",
            "       31192,  1440,     6,  6271,  1371,    87,   254,     9,  6520,\n",
            "         346,   152,    11,  1932,  3826,    87,   667,   565, 11219,\n",
            "          11,  1267,   731,   593,    13,     3,   173,  9842,   342,\n",
            "         151,    41,   924,   145,   460,   137,     3,     2,   115,\n",
            "          52,  3155,   304,  4505,    95,     6,     8,   360,  6688,\n",
            "         225,    36,     3, 25472,     5,    86,  6503,   132,    56,\n",
            "          36,   192,   315,  1637,    13,   844,     6,  3345,    30,\n",
            "         593,    13,     3,   173,  9842,  4710,     5,     3,  8500,\n",
            "           6,     8,   866,    13,     3,   173,  9842,   342,  3955,\n",
            "           7,    56,    36,  4038,   145,     3,   173,  9842,   342,\n",
            "        5069,     7,    16,   334,   616,     5,     1]), 'targets_plaintext': b'The chart illustrates the amount of illiterate males and females in the world in 2020. Overall, there is a gap that divides countries with high level of literacy and countries with lower literacy. <br> To begin with, it is clearly seen that there are more illiterate females than males in every area. However, the difference may be small, as, for example, in Latin America, or relatively large , like in East Asia, and the second type is more common. All regions can be divided into two groups. The first one is with high level of illiteracy , and it includes South Asia, Arab States and Sub-Saharan Afrrica. The other consists of Developed countries, Latin America/Caribbean and East Asia/Oceania and shows low level of illiterate people (less than 20). <br> To sum up, a few facts should be emphasized. In 2020 there will be two different areas, according to level of their literacy. Besides, the number of illiterate females has been and will be bigger than illiterate males in every area.', 'targets': array([   37,  5059, 11485,     7,     8,   866,    13,     3,   173,\n",
            "        9842,   342,  5069,     7,    11,  3955,     7,    16,     8,\n",
            "         296,    16,  6503,     5,  9126,     6,   132,    19,     3,\n",
            "           9,  6813,    24, 14514,     7,  1440,    28,   306,   593,\n",
            "          13, 18298,    11,  1440,    28,  1364, 18298,     5,     3,\n",
            "           2,   115,    52,  3155,   304,  1731,    28,     6,    34,\n",
            "          19,  3133,   894,    24,   132,    33,    72,     3,   173,\n",
            "        9842,   342,  3955,     7,   145,  5069,     7,    16,   334,\n",
            "         616,     5,   611,     6,     8,  1750,   164,    36,   422,\n",
            "           6,    38,     6,    21,   677,     6,    16,  6271,  1371,\n",
            "           6,    42,  4352,   508,     3,     6,   114,    16,  1932,\n",
            "        3826,     6,    11,     8,   511,   686,    19,    72,  1017,\n",
            "           5,   432,  6266,    54,    36,  8807,   139,   192,  1637,\n",
            "           5,    37,   166,    80,    19,    28,   306,   593,    13,\n",
            "           3,   173,  9842,  4710,     3,     6,    11,    34,   963,\n",
            "        1013,  3826,     6,  9217,  1323,    11,  3325,    18,   134,\n",
            "           9, 14888,    29,    71,    89,    52,  2234,     9,     5,\n",
            "          37,   119,     3,  6848,    13,     3, 31192,  1440,     6,\n",
            "        6271,  1371,    87,   254,     9,  6520,   346,   152,    11,\n",
            "        1932,  3826,    87,   667,   565, 11219,    11,  1267,   731,\n",
            "         593,    13,     3,   173,  9842,   342,   151,    41,   924,\n",
            "         145,   460,   137,     3,     2,   115,    52,  3155,   304,\n",
            "        4505,    95,     6,     3,     9,   360,  6688,   225,    36,\n",
            "           3, 25472,     5,    86,  6503,   132,    56,    36,   192,\n",
            "         315,   844,     6,  1315,    12,   593,    13,    70, 18298,\n",
            "           5,     3,  8500,     6,     8,   381,    13,     3,   173,\n",
            "        9842,   342,  3955,     7,    65,   118,    11,    56,    36,\n",
            "        4038,   145,     3,   173,  9842,   342,  5069,     7,    16,\n",
            "         334,   616,     5,     1])}\n",
            "{'inputs_plaintext': b'correction: Next year, I plan to go to UK for spending one year. That is because my husband is going to go to study MBA in UK, so I will bring my baby to go there with him. However, I dont want to just spend time in UK as a housewife, I want to study English, and if possible, i even want to study L.L.M in UK. Within five years, after I had the experience in UK, I will go back to work from the childcare vacation.Although Im still a legal assistant in a big company, I intend to apply a job in a legal office.%%', 'inputs': array([11698,    10,  3021,   215,     6,    27,   515,    12,   281,\n",
            "          12,  1270,    21,  2887,    80,   215,     5,   466,    19,\n",
            "         250,    82,  2553,    19,   352,    12,   281,    12,   810,\n",
            "       15751,    16,  1270,     6,    78,    27,    56,   830,    82,\n",
            "        1871,    12,   281,   132,    28,   376,     5,   611,     6,\n",
            "          27,  2483,   241,    12,   131,  1492,    97,    16,  1270,\n",
            "          38,     3,     9,   629, 22106,     6,    27,   241,    12,\n",
            "         810,  1566,     6,    11,     3,    99,   487,     6,     3,\n",
            "          23,   237,   241,    12,   810,   301,     5,   434,     5,\n",
            "         329,    16,  1270,     5,  8381,   874,   203,     6,   227,\n",
            "          27,   141,     8,   351,    16,  1270,     6,    27,    56,\n",
            "         281,   223,    12,   161,    45,     8, 27862,  4257,     5,\n",
            "         188,    40, 11841,  1318,   341,     3,     9,  1281,  6165,\n",
            "          16,     3,     9,   600,   349,     6,    27,  8286,    12,\n",
            "        1581,     3,     9,   613,    16,     3,     9,  1281,   828,\n",
            "           5,  1454,  1454,     1]), 'targets_plaintext': b'Next year, I plan to go to the UK for a year. That is because my husband is going to study a MBA in the UK, so I will bring my baby as well. However, I dont want to just spend time in the UK as a housewife, I want to study English, and if possible, I even want to study L.L.M in the UK. Within five years, after I have experience in the UK, I will go back to work. Although Im still a legal assistant in a big company, I intend to apply for a job in a legal office.%%', 'targets': array([ 3021,   215,     6,    27,   515,    12,   281,    12,     8,\n",
            "        1270,    21,     3,     9,   215,     5,   466,    19,   250,\n",
            "          82,  2553,    19,   352,    12,   810,     3,     9, 15751,\n",
            "          16,     8,  1270,     6,    78,    27,    56,   830,    82,\n",
            "        1871,    38,   168,     5,   611,     6,    27,  2483,   241,\n",
            "          12,   131,  1492,    97,    16,     8,  1270,    38,     3,\n",
            "           9,   629, 22106,     6,    27,   241,    12,   810,  1566,\n",
            "           6,    11,     3,    99,   487,     6,    27,   237,   241,\n",
            "          12,   810,   301,     5,   434,     5,   329,    16,     8,\n",
            "        1270,     5,  8381,   874,   203,     6,   227,    27,    43,\n",
            "         351,    16,     8,  1270,     6,    27,    56,   281,   223,\n",
            "          12,   161,     5,  1875,  1318,   341,     3,     9,  1281,\n",
            "        6165,    16,     3,     9,   600,   349,     6,    27,  8286,\n",
            "          12,  1581,    21,     3,     9,   613,    16,     3,     9,\n",
            "        1281,   828,     5,  1454,  1454,     1])}\n",
            "{'inputs_plaintext': b'correction: Hello Luis Blanco, <br> I\\xe2\\x80\\x99m writing you about the staff recruitment for the \\xe2\\x80\\x9cNew Cloud Generation\\xe2\\x80\\x9d project. So we need your help with some questions about the process. <br> For your information, we are looking for people with three years of experience in similar projects, who are engineers and also have a high team work skill. We also value that they have specialised studies in Cloud technology, and hosting management. However it\\xe2\\x80\\x99s possible that we don\\xe2\\x80\\x99t find enough people with that profile so, we could accept people with one year of experience. <br> Is it possible to do the meeting this week? When and where do you prefer? (Remember that I work from 8 a.m to 6 p.m) I suggest we can do the meeting at your office, so you don\\xe2\\x80\\x99t have to move to another place. <br> Finally I need you to prepare some profiles that you think the staff of New Cloud Generation should have, and please tell me if you find another different request than I tell you before in that e-mail. <br> Please, answer me with the information as soon as possible. <br> \\xc3\\x8d\\xc3\\xb1igo Ojeda.', 'inputs': array([11698,    10,  8774,  2318,   159, 12824,    32,     6,     3,\n",
            "           2,   115,    52,  3155,    27,    22,    51,   913,    25,\n",
            "          81,     8,   871, 11615,    21,     8,   105,  6861,  5713,\n",
            "       11946,   153,   516,     5,   264,    62,   174,    39,   199,\n",
            "          28,   128,   746,    81,     8,   433,     5,     3,     2,\n",
            "         115,    52,  3155,   242,    39,   251,     6,    62,    33,\n",
            "         479,    21,   151,    28,   386,   203,    13,   351,    16,\n",
            "        1126,  1195,     6,   113,    33,  8702,    11,    92,    43,\n",
            "           3,     9,   306,   372,   161,  4359,     5,   101,    92,\n",
            "         701,    24,    79,    43,     3, 26725,  2116,    16,  5713,\n",
            "         748,     6,    11,  4434,   758,     5,   611,    34,    22,\n",
            "           7,   487,    24,    62,   278,    22,    17,   253,   631,\n",
            "         151,    28,    24,  3278,    78,     6,    62,   228,  1845,\n",
            "         151,    28,    80,   215,    13,   351,     5,     3,     2,\n",
            "         115,    52,  3155,    27,     7,    34,   487,    12,   103,\n",
            "           8,  1338,    48,   471,    58,   366,    11,   213,   103,\n",
            "          25,  2396,    58,    41,  1649, 12066,    24,    27,   161,\n",
            "          45,   505,     3,     9,     5,    51,    12,   431,     3,\n",
            "         102,     5,    51,    61,    27,  3130,    62,    54,   103,\n",
            "           8,  1338,    44,    39,   828,     6,    78,    25,   278,\n",
            "          22,    17,    43,    12,   888,    12,   430,   286,     5,\n",
            "           3,     2,   115,    52,  3155,  4213,    27,   174,    25,\n",
            "          12,  2967,   128, 10958,    24,    25,   317,     8,   871,\n",
            "          13,   368,  5713, 11946,   225,    43,     6,    11,   754,\n",
            "         817,   140,     3,    99,    25,   253,   430,   315,  1690,\n",
            "         145,    27,   817,    25,   274,    16,    24,     3,    15,\n",
            "          18,  1963,     5,     3,     2,   115,    52,  3155,   863,\n",
            "           6,  1525,   140,    28,     8,   251,    38,  1116,    38,\n",
            "         487,     5,     3,     2,   115,    52,  3155,     3,     2,\n",
            "          23,   839,   411,  1924,    26,     9,     5,     1]), 'targets_plaintext': b'Hello Luis Blanco, <br> I\\xe2\\x80\\x99m writing to you about the staff recruitment for the \\xe2\\x80\\x9cNew Cloud Generation\\xe2\\x80\\x9d project. So we need your help with some questions about the process. <br> For your information, we are looking for people with three years experience in similar projects, who are engineers and also have good teamworking skills. We also require that they have specialised studies in Cloud technology, and hosting management. However, it\\xe2\\x80\\x99s possible that we won\\xe2\\x80\\x99t find enough people with that profile, so, we could accept people with one years experience. <br> Is it possible to hold the meeting this week? When and where do you prefer? (Remember that I work from 8 a.m to 6 p.m) I suggest we could hold the meeting at your office, so you don\\xe2\\x80\\x99t have to travel to another place. <br> Finally, I need you to prepare some profiles of the qualities that you think the staff of New Cloud Generation should have, and please tell me if you find another different request than the one I told you before in that e-mail. <br> Please, answer me with the information as soon as possible. <br> \\xc3\\x8d\\xc3\\xb1igo Ojeda.', 'targets': array([ 8774,  2318,   159, 12824,    32,     6,     3,     2,   115,\n",
            "          52,  3155,    27,    22,    51,   913,    12,    25,    81,\n",
            "           8,   871, 11615,    21,     8,   105,  6861,  5713, 11946,\n",
            "         153,   516,     5,   264,    62,   174,    39,   199,    28,\n",
            "         128,   746,    81,     8,   433,     5,     3,     2,   115,\n",
            "          52,  3155,   242,    39,   251,     6,    62,    33,   479,\n",
            "          21,   151,    28,   386,   203,   351,    16,  1126,  1195,\n",
            "           6,   113,    33,  8702,    11,    92,    43,   207,   372,\n",
            "        9238,  1098,     5,   101,    92,  1457,    24,    79,    43,\n",
            "           3, 26725,  2116,    16,  5713,   748,     6,    11,  4434,\n",
            "         758,     5,   611,     6,    34,    22,     7,   487,    24,\n",
            "          62,   751,    22,    17,   253,   631,   151,    28,    24,\n",
            "        3278,     6,    78,     6,    62,   228,  1845,   151,    28,\n",
            "          80,   203,   351,     5,     3,     2,   115,    52,  3155,\n",
            "          27,     7,    34,   487,    12,  1520,     8,  1338,    48,\n",
            "         471,    58,   366,    11,   213,   103,    25,  2396,    58,\n",
            "          41,  1649, 12066,    24,    27,   161,    45,   505,     3,\n",
            "           9,     5,    51,    12,   431,     3,   102,     5,    51,\n",
            "          61,    27,  3130,    62,   228,  1520,     8,  1338,    44,\n",
            "          39,   828,     6,    78,    25,   278,    22,    17,    43,\n",
            "          12,  1111,    12,   430,   286,     5,     3,     2,   115,\n",
            "          52,  3155,  4213,     6,    27,   174,    25,    12,  2967,\n",
            "         128, 10958,    13,     8, 10596,    24,    25,   317,     8,\n",
            "         871,    13,   368,  5713, 11946,   225,    43,     6,    11,\n",
            "         754,   817,   140,     3,    99,    25,   253,   430,   315,\n",
            "        1690,   145,     8,    80,    27,  1219,    25,   274,    16,\n",
            "          24,     3,    15,    18,  1963,     5,     3,     2,   115,\n",
            "          52,  3155,   863,     6,  1525,   140,    28,     8,   251,\n",
            "          38,  1116,    38,   487,     5,     3,     2,   115,    52,\n",
            "        3155,     3,     2,    23,   839,   411,  1924,    26,     9,\n",
            "           5,     1])}\n",
            "{'inputs_plaintext': b'correction: I planning to my party on sunday in june at 11am we are inviting 20 of peoples coming in my party so we need to make a cake and brening the 30 peer and 10 cool drining <br> and also prening cd for music rock or dance and also we need make so chooclates for gairls which her coming our party and we thank all people coming my party', 'inputs': array([11698,    10,    27,  1459,    12,    82,  1088,    30,  1997,\n",
            "        1135,    16,     3,  6959,    15,    44,   850,   265,    62,\n",
            "          33, 14256,   460,    13,   151,     7,  1107,    16,    82,\n",
            "        1088,    78,    62,   174,    12,   143,     3,     9,  4340,\n",
            "          11,  6397,    35,    53,     8,   604, 11409,    11,   335,\n",
            "        1633,     3, 19161,    53,     3,     2,   115,    52,  3155,\n",
            "          11,    92,   554,    29,    53,     3,    75,    26,    21,\n",
            "         723,  2480,    42,  2595,    11,    92,    62,   174,   143,\n",
            "          78,     3,  3995,    32,  4651,  1422,    21,     3,   122,\n",
            "        2256,    40,     7,    84,   160,  1107,    69,  1088,    11,\n",
            "          62,  2763,    66,   151,  1107,    82,  1088,     1]), 'targets_plaintext': b'I am planning my party on Sunday in June at 11am. We are inviting 20 people coming to my party. So we need to make a cake, bring 30 beers and 10 cool drinks. <br> We should prepare a cd for rock or dance music and also we need make chocolates for the girls We thank all the people who are coming to my party.', 'targets': array([   27,   183,  1459,    82,  1088,    30,  1771,    16,  1515,\n",
            "          44,   850,   265,     5,   101,    33, 14256,   460,   151,\n",
            "        1107,    12,    82,  1088,     5,   264,    62,   174,    12,\n",
            "         143,     3,     9,  4340,     6,   830,   604,    36,   277,\n",
            "          11,   335,  1633,  6750,     5,     3,     2,   115,    52,\n",
            "        3155,   101,   225,  2967,     3,     9,     3,    75,    26,\n",
            "          21,  2480,    42,  2595,   723,    11,    92,    62,   174,\n",
            "         143,  3711,     7,    21,     8,  3567,   101,  2763,    66,\n",
            "           8,   151,   113,    33,  1107,    12,    82,  1088,     5,\n",
            "           1])}\n",
            "{'inputs_plaintext': b'correction: My Boos is very friendly, but very serious. He is very short and blacy har. He is always hardworking or studying. He never smiles.', 'inputs': array([11698,    10,   499,  1491,    32,     7,    19,   182,  2609,\n",
            "           6,    68,   182,  2261,     5,   216,    19,   182,   710,\n",
            "          11,     3,   115,    40,  4710,     3,  3272,     5,   216,\n",
            "          19,   373,   614,  9238,    42,  6908,     5,   216,   470,\n",
            "        3993,     7,     5,     1]), 'targets_plaintext': b'My boss is very friendly, but very serious. He is very short and has black hair. He is always hard-working or studying. He never smiles.', 'targets': array([ 499, 7930,   19,  182, 2609,    6,   68,  182, 2261,    5,  216,\n",
            "         19,  182,  710,   11,   65, 1001, 1268,    5,  216,   19,  373,\n",
            "        614,   18, 9238,   42, 6908,    5,  216,  470, 3993,    7,    5,\n",
            "          1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ailCy7DO4--y"
      },
      "source": [
        "### conll\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKb92NTv4--z"
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "conll_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"conll-train.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"conll-eval.tsv\")\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysZQWUSd4--5"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pykQy3i4--6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd66871-6b52-4862-ded8-b189b40a17b9"
      },
      "source": [
        "def conll_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(conll_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(conll_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'sentence: Keeping the Secret of Genetic Testing parsing: ROOT_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number prep_ADP compound_PROPN_NounType_Number pobj_PROPN_NounType_Number', 'corr_text': b'sentence: Keeping the Secret of Genetic Testing parsing: ROOT_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number prep_ADP compound_PROPN_NounType_Number pobj_PROPN_NounType_Number'}\n",
            "{'orig_text': b'sentence: What is genetic risk ? parsing: attr_NOUN_PronType ROOT_VERB_VerbForm_Tense_Number_Person amod_ADJ_Degree nsubj_NOUN_Number ?', 'corr_text': b'sentence: What is genetic risk ? parsing: attr_NOUN_PronType ROOT_VERB_VerbForm_Tense_Number_Person amod_ADJ_Degree nsubj_NOUN_Number ?'}\n",
            "{'orig_text': b'sentence: Genetic risk refers more to your chance of inheriting a disorder or disease . parsing: compound_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_ADV_Degree prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number .', 'corr_text': b'sentence: Genetic risk refers more to your chance of inheriting a disorder or disease . parsing: compound_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_ADV_Degree prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: People get certain disease because of genetic changes . parsing: nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense amod_ADJ_Degree dobj_NOUN_Number prep_ADP pcomp_ADP amod_ADJ_Degree pobj_NOUN_Number .', 'corr_text': b'sentence: People get certain disease because of genetic changes . parsing: nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense amod_ADJ_Degree dobj_NOUN_Number prep_ADP pcomp_ADP amod_ADJ_Degree pobj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: How much a genetic change tells us about your chance of developing a disorder is not always clear . parsing: advmod_ADV_PronType advmod_ADJ_Degree det_DET amod_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_PRON_PronType prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person neg_ADV_Degree advmod_ADV_Degree acomp_ADJ_Degree .', 'corr_text': b'sentence: How much a genetic change tells us about your chance of developing a disorder is not always clear . parsing: advmod_ADV_PronType advmod_ADJ_Degree det_DET amod_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_PRON_PronType prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person neg_ADV_Degree advmod_ADV_Degree acomp_ADJ_Degree .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfK0n9EM4-_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05294394-50cf-4aa8-89c7-b9ce47d07653"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/conll-train.tsv conll-train.tsv\n",
        "\n",
        "with open(\"conll-train.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  conll_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/conll-train.tsv...\n",
            "/ [1/1 files][ 12.0 MiB/ 12.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/12.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdBxpeUg4-_I"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTPrLkal4-_K"
      },
      "source": [
        "def conll_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"conll: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMPR6oam4-_Q"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBKerTO4-_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf584cf-e3d7-442e-a857-edaa9c6a9e7d"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"conll\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=conll_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[conll_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnAPQ5A34-_Y"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAIeE_fS4-_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebd460a-20f6-47cd-c4be-9fb864979a60"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"conll\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'conll: sentence: And so , he would have chosen not to undergo generic disorder testing and let the truth be mined forever . parsing: cc_CCONJ_ConjType advmod_ADV_Degree , nsubj_PRON_PronType aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect neg_ADV_Degree aux_PART_PartType_VerbForm xcomp_VERB_VerbForm amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType conj_VERB_VerbForm det_DET nsubjpass_NOUN_Number auxpass_VERB_VerbForm ccomp_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree .', 'inputs': array([  975,   195,    10,  7142,    10,   275,    78,     3,     6,\n",
            "           3,    88,   133,    43,  3934,    59,    12, 17601,  8165,\n",
            "        9311,  2505,    11,   752,     8,  2827,    36,  2000,    26,\n",
            "        6276,     3,     5,   260,     7,    53,    10,     3,    75,\n",
            "          75,   834,   254, 17752,   683,   834,  4302,   354, 25160,\n",
            "           3,     9,    26,   208,  7360,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,     3,     6,     3,    29,  7304,   354,\n",
            "         834,   345, 13044,   834,  3174,    29, 25160,   742,   834,\n",
            "       16174,   279,   834,  5000,   115, 25160,   742,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51, 10264,  6951,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   188,  5628, 14261,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,   742,   834, 19846,   834, 13725, 25160,\n",
            "         834,  5000,   115,  3809,    51,     3,   226,  7699,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,     3,     9,\n",
            "        7360,   834,  6762,   683,   834,  2962,  3584,    15, 12771,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   103,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "          75,    75,   834,   254, 17752,   683,   834,  4302,   354,\n",
            "       25160,   975,   354,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,    20,    17,   834,  5596,   382,     3,    29,\n",
            "        7304,   354,  3968,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,   742,  3968,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,    75,  7699,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628,     3,     9,    26,   208,  7360,   834,   188, 13529,\n",
            "         834,  2962,  3584,    15,     3,     5,     1]), 'targets_plaintext': b'sentence: And so , he would have chosen not to undergo generic disorder testing and let the truth be mined forever . parsing: cc_CCONJ_ConjType advmod_ADV_Degree , nsubj_PRON_PronType aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect neg_ADV_Degree aux_PART_PartType_VerbForm xcomp_VERB_VerbForm amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType conj_VERB_VerbForm det_DET nsubjpass_NOUN_Number auxpass_VERB_VerbForm ccomp_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree .', 'targets': array([ 7142,    10,   275,    78,     3,     6,     3,    88,   133,\n",
            "          43,  3934,    59,    12, 17601,  8165,  9311,  2505,    11,\n",
            "         752,     8,  2827,    36,  2000,    26,  6276,     3,     5,\n",
            "         260,     7,    53,    10,     3,    75,    75,   834,   254,\n",
            "       17752,   683,   834,  4302,   354, 25160,     3,     9,    26,\n",
            "         208,  7360,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "           3,     6,     3,    29,  7304,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,   742,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51, 10264,  6951,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628, 14261,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "         742,   834, 19846,   834, 13725, 25160,   834,  5000,   115,\n",
            "        3809,    51,     3,   226,  7699,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,     3,     9,  7360,   834,  6762,\n",
            "         683,   834,  2962,  3584,    15, 12771,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,   103,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    75,    75,   834,\n",
            "         254, 17752,   683,   834,  4302,   354, 25160,   975,   354,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,    20,\n",
            "          17,   834,  5596,   382,     3,    29,  7304,   354,  3968,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   742,  3968,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,     3,\n",
            "          75,  7699,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   188,  5628,     3,     9,\n",
            "          26,   208,  7360,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,     3,     5,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: May be we can trust our immediate family members but can we really trust all the other relatives ? parsing: aux_VERB_VerbType ROOT_VERB_VerbForm nsubj_PRON_PronType aux_VERB_VerbType ccomp_VERB_VerbForm poss_ADJ_PronType_Poss amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType aux_VERB_VerbType nsubj_PRON_PronType advmod_ADV_Degree conj_VERB_VerbForm predet_ADJ_AdjType_PronType det_DET amod_ADJ_Degree dobj_NOUN_Number ?', 'inputs': array([  975,   195,    10,  7142,    10,   932,    36,    62,    54,\n",
            "        2019,    69,  5299,   384,   724,    68,    54,    62,   310,\n",
            "        2019,    66,     8,   119, 12867,     3,    58,   260,     7,\n",
            "          53,    10,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160, 10264,  6951,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,    29,  7304,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,     3,    75,  7699,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,     3,  2748,     7,   834,\n",
            "        6762,   683,   834,  3174,    29, 25160,   834,   345,    32,\n",
            "           7,     7,     3,     9,  7360,   834,  6762,   683,   834,\n",
            "        2962,  3584,    15, 12771,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,   103,   115,   354,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,    75,    75,   834,   254, 17752,\n",
            "         683,   834,  4302,   354, 25160,   742,   834, 16174,   279,\n",
            "         834,  5000,   115, 25160,     3,    29,  7304,   354,   834,\n",
            "         345, 13044,   834,  3174,    29, 25160,     3,     9,    26,\n",
            "         208,  7360,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "         975,   354,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   554,   221,    17,   834,  6762,   683,   834,   188,\n",
            "          26,   354, 25160,   834,  3174,    29, 25160,    20,    17,\n",
            "         834,  5596,   382,     3,     9,  7360,   834,  6762,   683,\n",
            "         834,  2962,  3584,    15,   103,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    58,     1]), 'targets_plaintext': b'sentence: May be we can trust our immediate family members but can we really trust all the other relatives ? parsing: aux_VERB_VerbType ROOT_VERB_VerbForm nsubj_PRON_PronType aux_VERB_VerbType ccomp_VERB_VerbForm poss_ADJ_PronType_Poss amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType aux_VERB_VerbType nsubj_PRON_PronType advmod_ADV_Degree conj_VERB_VerbForm predet_ADJ_AdjType_PronType det_DET amod_ADJ_Degree dobj_NOUN_Number ?', 'targets': array([ 7142,    10,   932,    36,    62,    54,  2019,    69,  5299,\n",
            "         384,   724,    68,    54,    62,   310,  2019,    66,     8,\n",
            "         119, 12867,     3,    58,   260,     7,    53,    10,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160, 10264,  6951,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,     3,\n",
            "          29,  7304,   354,   834,   345, 13044,   834,  3174,    29,\n",
            "       25160,   742,   834, 16174,   279,   834,  5000,   115, 25160,\n",
            "           3,    75,  7699,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,  2748,     7,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,   834,   345,    32,     7,     7,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "       12771,   834,  7400,  7443,   834,   567,  5937,    49,   103,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160,     3,    29,  7304,   354,   834,   345, 13044,   834,\n",
            "        3174,    29, 25160,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,   975,   354,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   554,   221,\n",
            "          17,   834,  6762,   683,   834,   188,    26,   354, 25160,\n",
            "         834,  3174,    29, 25160,    20,    17,   834,  5596,   382,\n",
            "           3,     9,  7360,   834,  6762,   683,   834,  2962,  3584,\n",
            "          15,   103,   115,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,    58,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: I believe that some of the carriers will choose to not reveal their disease to the family members as they do not want to make their family members worry and feel uncomfortable or even feel ostracized by them . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense mark_ADP nsubj_DET prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType ccomp_VERB_VerbForm aux_PART_PartType_VerbForm neg_ADV_Degree xcomp_VERB_VerbForm poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number mark_ADP nsubj_PRON_PronType aux_VERB_VerbForm_Tense neg_ADV_Degree advcl_VERB_VerbForm aux_PART_PartType_VerbForm xcomp_VERB_VerbForm poss_ADJ_PronType_Poss compound_NOUN_Number nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense cc_CCONJ_ConjType conj_VERB_VerbForm_Tense acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_VERB_VerbForm xcomp_VERB_VerbForm_Tense_Aspect agent_ADP pobj_PRON_PronType .', 'inputs': array([  975,   195,    10,  7142,    10,    27,   857,    24,   128,\n",
            "          13,     8, 16642,    56,   854,    12,    59,  6731,    70,\n",
            "        1994,    12,     8,   384,   724,    38,    79,   103,    59,\n",
            "         241,    12,   143,    70,   384,   724,  3516,    11,   473,\n",
            "       14209,    42,   237,   473,     3,    32,     7,  6471,  1601,\n",
            "          57,   135,     3,     5,   260,     7,    53,    10,     3,\n",
            "          29,  7304,   354,   834,   345, 13044,   834,  3174,    29,\n",
            "       25160, 10264,  6951,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,  3946,   834,   188,  7410,\n",
            "           3,    29,  7304,   354,   834,  5596,   382, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,     3,    75,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         742,   834, 19846,   834, 13725, 25160,   834,  5000,   115,\n",
            "        3809,    51, 14261,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,     3,   226,  7699,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,     3,  2748,     7,   834,  6762,   683,\n",
            "         834,  3174,    29, 25160,   834,   345,    32,     7,     7,\n",
            "         103,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,    20,    17,   834,  5596,\n",
            "         382, 12771,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,  3946,   834,   188,  7410,     3,    29,  7304,   354,\n",
            "         834,   345, 13044,   834,  3174,    29, 25160,   742,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167, 14261,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "           3,     9,    26,   208,    75,    40,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   742,   834, 19846,   834,\n",
            "       13725, 25160,   834,  5000,   115,  3809,    51,     3,   226,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "           3,  2748,     7,   834,  6762,   683,   834,  3174,    29,\n",
            "       25160,   834,   345,    32,     7,     7, 12771,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    29,  7304,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,    75,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         834,   382,  5167,     3,    75,    75,   834,   254, 17752,\n",
            "         683,   834,  4302,   354, 25160,   975,   354,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   834,   382,  5167,\n",
            "           3,     9,  7699,   834,  6762,   683,   834,  2962,  3584,\n",
            "          15,     3,    75,    75,   834,   254, 17752,   683,   834,\n",
            "        4302,   354, 25160,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,   975,   354,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,     3,   226,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         834,   382,  5167,   834,   188,  5628,  3102,   834,   188,\n",
            "        7410,  1977,   115,   354,   834,   345, 13044,   834,  3174,\n",
            "          29, 25160,     3,     5,     1]), 'targets_plaintext': b'sentence: I believe that some of the carriers will choose to not reveal their disease to the family members as they do not want to make their family members worry and feel uncomfortable or even feel ostracized by them . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense mark_ADP nsubj_DET prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType ccomp_VERB_VerbForm aux_PART_PartType_VerbForm neg_ADV_Degree xcomp_VERB_VerbForm poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number mark_ADP nsubj_PRON_PronType aux_VERB_VerbForm_Tense neg_ADV_Degree advcl_VERB_VerbForm aux_PART_PartType_VerbForm xcomp_VERB_VerbForm poss_ADJ_PronType_Poss compound_NOUN_Number nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense cc_CCONJ_ConjType conj_VERB_VerbForm_Tense acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_VERB_VerbForm xcomp_VERB_VerbForm_Tense_Aspect agent_ADP pobj_PRON_PronType .', 'targets': array([ 7142,    10,    27,   857,    24,   128,    13,     8, 16642,\n",
            "          56,   854,    12,    59,  6731,    70,  1994,    12,     8,\n",
            "         384,   724,    38,    79,   103,    59,   241,    12,   143,\n",
            "          70,   384,   724,  3516,    11,   473, 14209,    42,   237,\n",
            "         473,     3,    32,     7,  6471,  1601,    57,   135,     3,\n",
            "           5,   260,     7,    53,    10,     3,    29,  7304,   354,\n",
            "         834,   345, 13044,   834,  3174,    29, 25160, 10264,  6951,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,   834,\n",
            "         382,  5167,  3946,   834,   188,  7410,     3,    29,  7304,\n",
            "         354,   834,  5596,   382, 13422,   834,   188,  7410,    20,\n",
            "          17,   834,  5596,   382,  1977,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,   742,   834, 16174,   279,\n",
            "         834,  5000,   115, 25160,     3,    75,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   742,   834, 19846,\n",
            "         834, 13725, 25160,   834,  5000,   115,  3809,    51, 14261,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,     3,   226,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "           3,  2748,     7,   834,  6762,   683,   834,  3174,    29,\n",
            "       25160,   834,   345,    32,     7,     7,   103,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382, 12771,   834,\n",
            "        7400,  7443,   834,   567,  5937,    49,  1977,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,  3946,   834,\n",
            "         188,  7410,     3,    29,  7304,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,   742,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167, 14261,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,     3,     9,    26,\n",
            "         208,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   742,   834, 19846,   834, 13725, 25160,   834,\n",
            "        5000,   115,  3809,    51,     3,   226,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,     3,  2748,     7,\n",
            "         834,  6762,   683,   834,  3174,    29, 25160,   834,   345,\n",
            "          32,     7,     7, 12771,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,    29,  7304,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,     3,    75,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   834,   382,  5167,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160,   975,   354,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,   834,   382,  5167,     3,     9,  7699,\n",
            "         834,  6762,   683,   834,  2962,  3584,    15,     3,    75,\n",
            "          75,   834,   254, 17752,   683,   834,  4302,   354, 25160,\n",
            "           3,     9,    26,   208,  7360,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,   975,   354,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,     3,   226,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   834,   382,  5167,\n",
            "         834,   188,  5628,  3102,   834,   188,  7410,  1977,   115,\n",
            "         354,   834,   345, 13044,   834,  3174,    29, 25160,     3,\n",
            "           5,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: To tell or not to tell parsing: aux_PART_PartType_VerbForm ROOT_VERB_VerbForm cc_CCONJ_ConjType neg_ADV_Degree aux_PART_PartType_VerbForm conj_VERB_VerbForm', 'inputs': array([  975,   195,    10,  7142,    10,   304,   817,    42,    59,\n",
            "          12,   817,   260,     7,    53,    10,   742,   834, 19846,\n",
            "         834, 13725, 25160,   834,  5000,   115,  3809,    51, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160, 14261,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,   742,   834, 19846,   834, 13725, 25160,   834,  5000,\n",
            "         115,  3809,    51,   975,   354,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,     1]), 'targets_plaintext': b'sentence: To tell or not to tell parsing: aux_PART_PartType_VerbForm ROOT_VERB_VerbForm cc_CCONJ_ConjType neg_ADV_Degree aux_PART_PartType_VerbForm conj_VERB_VerbForm', 'targets': array([ 7142,    10,   304,   817,    42,    59,    12,   817,   260,\n",
            "           7,    53,    10,   742,   834, 19846,   834, 13725, 25160,\n",
            "         834,  5000,   115,  3809,    51, 10264,  6951,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,     3,    75,    75,\n",
            "         834,   254, 17752,   683,   834,  4302,   354, 25160, 14261,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,   742,   834,\n",
            "       19846,   834, 13725, 25160,   834,  5000,   115,  3809,    51,\n",
            "         975,   354,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: Social media does have some other advantages beside their general benefits to human being . parsing: amod_ADJ_Degree nsubj_NOUN_Number aux_VERB_VerbForm_Tense_Number_Person ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss amod_ADJ_Degree pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number .', 'inputs': array([  975,   195,    10,  7142,    10,  2730,   783,   405,    43,\n",
            "         128,   119,  7648, 14898,    70,   879,  1393,    12,   936,\n",
            "         271,     3,     5,   260,     7,    53,    10,     3,     9,\n",
            "        7360,   834,  6762,   683,   834,  2962,  3584,    15,     3,\n",
            "          29,  7304,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,   742,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   567,  5937,    49,   834,\n",
            "         345, 13515, 10264,  6951,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,    20,    17,   834,  5596,   382,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "         103,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,     3,  2748,     7,   834,\n",
            "        6762,   683,   834,  3174,    29, 25160,   834,   345,    32,\n",
            "           7,     7,     3,     9,  7360,   834,  6762,   683,   834,\n",
            "        2962,  3584,    15,  1977,   115,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49, 13422,   834,   188,  7410,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,     3,     5,     1]), 'targets_plaintext': b'sentence: Social media does have some other advantages beside their general benefits to human being . parsing: amod_ADJ_Degree nsubj_NOUN_Number aux_VERB_VerbForm_Tense_Number_Person ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss amod_ADJ_Degree pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number .', 'targets': array([ 7142,    10,  2730,   783,   405,    43,   128,   119,  7648,\n",
            "       14898,    70,   879,  1393,    12,   936,   271,     3,     5,\n",
            "         260,     7,    53,    10,     3,     9,  7360,   834,  6762,\n",
            "         683,   834,  2962,  3584,    15,     3,    29,  7304,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   742,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   567,  5937,    49,   834,   345, 13515, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "          20,    17,   834,  5596,   382,     3,     9,  7360,   834,\n",
            "        6762,   683,   834,  2962,  3584,    15,   103,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,     3,  2748,     7,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,   834,   345,    32,     7,     7,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,     3,     9,  7360,   834,\n",
            "        6762,   683,   834,  2962,  3584,    15,  1977,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,     5,\n",
            "           1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJm0H4y5lgz"
      },
      "source": [
        "### jfleg\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYMaUrqR5lg9"
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "jfleg_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"jfleg-train.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"jfleg-eval.tsv\")\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV2WZkP65lhD"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAwPAnPR5lhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb42a31e-06ef-47a8-a1cc-d5bfb12400f9"
      },
      "source": [
        "def jfleg_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(jfleg_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(jfleg_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'New and new technology has been introduced to the society .', 'corr_text': b'New and new technology has been introduced to the society .'}\n",
            "{'orig_text': b'One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .', 'corr_text': b'One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .'}\n",
            "{'orig_text': b'Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .', 'corr_text': b'Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .'}\n",
            "{'orig_text': b'While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .', 'corr_text': b'While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .'}\n",
            "{'orig_text': b'Disadvantage is parking their car is very difficult .', 'corr_text': b'Disadvantage is parking their car is very difficult .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyO2lRv_5lhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4ad9b1-91b5-47f8-cdb3-f273caedba16"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/jfleg-train.tsv jfleg-train.tsv\n",
        "\n",
        "with open(\"jfleg-train.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  jfleg_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/jfleg-train.tsv...\n",
            "/ [1/1 files][570.1 KiB/570.1 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/570.1 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Yyu1XW5lhd"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKHVHAXA5lhe"
      },
      "source": [
        "def jfleg_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"jfleg: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm7SO2sc5lhg"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYYoQac55lhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b990492-2552-4ae9-a288-28470d333dd6"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"jfleg\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=jfleg_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[jfleg_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBKTsfxo5lhj"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU7QaX-W5lhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495b6ad2-c999-4048-c715-b4d993ac36b2"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"jfleg\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'jfleg: In her salary we cant buy some car because we are planing to finish our hause in Binangonan , Rizal and we will planing to finish my study in Boston .', 'inputs': array([   3,  354,   89, 5772,   10,   86,  160, 9090,   62,   54,   17,\n",
            "        805,  128,  443,  250,   62,   33,  515,   53,   12, 1992,   69,\n",
            "          3, 2989,   15,   16, 7617, 1468,  106,  152,    3,    6, 2403,\n",
            "        172,  138,   11,   62,   56,  515,   53,   12, 1992,   82,  810,\n",
            "         16, 5053,    3,    5,    1]), 'targets_plaintext': b'In her salary we cant buy some car because we are planing to finish our hause in Binangonan , Rizal and we will planing to finish my study in Boston .', 'targets': array([  86,  160, 9090,   62,   54,   17,  805,  128,  443,  250,   62,\n",
            "         33,  515,   53,   12, 1992,   69,    3, 2989,   15,   16, 7617,\n",
            "       1468,  106,  152,    3,    6, 2403,  172,  138,   11,   62,   56,\n",
            "        515,   53,   12, 1992,   82,  810,   16, 5053,    3,    5,    1])}\n",
            "{'inputs_plaintext': b'jfleg: In this movie father has handicapped .', 'inputs': array([    3,   354,    89,  5772,    10,    86,    48,  1974,  2353,\n",
            "          65, 14214,  3138,     3,     5,     1]), 'targets_plaintext': b'In this movie father has handicapped .', 'targets': array([   86,    48,  1974,  2353,    65, 14214,  3138,     3,     5,\n",
            "           1])}\n",
            "{'inputs_plaintext': b'jfleg: Can you ever visualize what a chaos our society would suffer if every individual considers himself/ herself an expert in every given field ?', 'inputs': array([    3,   354,    89,  5772,    10,  1072,    25,   664, 25086,\n",
            "         125,     3,     9, 16856,    69,  2710,   133,  5696,     3,\n",
            "          99,   334,   928,  1099,     7,  2448,    87,  6257,    46,\n",
            "        2205,    16,   334,   787,  1057,     3,    58,     1]), 'targets_plaintext': b'Can you ever visualize what a chaos our society would suffer if every individual considers himself/ herself an expert in every given field ?', 'targets': array([ 1072,    25,   664, 25086,   125,     3,     9, 16856,    69,\n",
            "        2710,   133,  5696,     3,    99,   334,   928,  1099,     7,\n",
            "        2448,    87,  6257,    46,  2205,    16,   334,   787,  1057,\n",
            "           3,    58,     1])}\n",
            "{'inputs_plaintext': b'jfleg: Many people whaching advertise .', 'inputs': array([    3,   354,    89,  5772,    10,  1404,   151, 14228, 12076,\n",
            "       17123,     3,     5,     1]), 'targets_plaintext': b'Many people whaching advertise .', 'targets': array([ 1404,   151, 14228, 12076, 17123,     3,     5,     1])}\n",
            "{'inputs_plaintext': b'jfleg: it is ture .', 'inputs': array([   3,  354,   89, 5772,   10,   34,   19,    3, 2693,    3,    5,\n",
            "          1]), 'targets_plaintext': b'it is ture .', 'targets': array([  34,   19,    3, 2693,    3,    5,    1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx8nxpt52wi"
      },
      "source": [
        "### bea\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s8_Ax8q52wm"
      },
      "source": [
        "bea_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"bea-train-strict.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"bea-eval.tsv\")\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T5Ox2Zw52ws"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otom1DRl52wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b36867a-a816-43a2-e531-8a2e40dc3633"
      },
      "source": [
        "def bea_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(bea_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(bea_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'sentence: Dear Sir , parsing: amod_ADJ_Degree ROOT_PROPN_NounType_Number ,', 'corr_text': b'sentence: Dear Sir , parsing: amod_ADJ_Degree ROOT_PROPN_NounType_Number ,'}\n",
            "{'orig_text': b'sentence: I have seen your advertisement for a job on the internet and I am writing to apply for a summer job as an instructor and keeper of children in your camp . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType nsubj_PRON_PronType aux_VERB_VerbForm_Tense conj_VERB_VerbForm_Tense_Aspect aux_PART_PartType_VerbForm advcl_VERB_VerbForm prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number .', 'corr_text': b'sentence: I have seen your advertisement for a job on the internet and I am writing to apply for a summer job as an instructor and keeper of children in your camp . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType nsubj_PRON_PronType aux_VERB_VerbForm_Tense conj_VERB_VerbForm_Tense_Aspect aux_PART_PartType_VerbForm advcl_VERB_VerbForm prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: I am working as a teacher in Spanish school with children aged between 8 and 14 . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number prep_ADP pobj_NOUN_Number acl_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NUM_NumType cc_CCONJ_ConjType conj_NUM_NumType .', 'corr_text': b'sentence: I am working as a teacher in Spanish school with children aged between 8 and 14 . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number prep_ADP pobj_NOUN_Number acl_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NUM_NumType cc_CCONJ_ConjType conj_NUM_NumType .'}\n",
            "{'orig_text': b'sentence: I am an easy going person with a lot of empathy for children . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense det_DET amod_ADJ_Degree compound_VERB_VerbForm_Tense_Aspect attr_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP pobj_NOUN_Number .', 'corr_text': b'sentence: I am an easy going person with a lot of empathy for children . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense det_DET amod_ADJ_Degree compound_VERB_VerbForm_Tense_Aspect attr_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP pobj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: On the other hand , in my leisure time , I usually do sports like jogging or swimming , adventures activities like trekking , climbing o espeleology too for 20 years ago . parsing: prep_ADP det_DET amod_ADJ_Degree pobj_NOUN_Number , prep_ADP poss_ADJ_PronType_Poss compound_NOUN_Number pobj_NOUN_Number , nsubj_PRON_PronType advmod_ADV_Degree ROOT_VERB_VerbForm_Tense dobj_NOUN_Number prep_ADP nmod_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number , compound_NOUN_Number conj_NOUN_Number intj_ADP advcl_VERB_VerbForm_Tense_Aspect , conj_VERB_VerbForm_Tense_Aspect prep_PART dobj_NOUN_Number advmod_ADV_Degree prep_ADP nummod_NUM_NumType npadvmod_NOUN_Number pcomp_ADV_Degree .', 'corr_text': b'sentence: On the other hand , in my leisure time , I usually do sports like jogging or swimming , adventures activities like trekking , climbing o espeleology too for 20 years ago . parsing: prep_ADP det_DET amod_ADJ_Degree pobj_NOUN_Number , prep_ADP poss_ADJ_PronType_Poss compound_NOUN_Number pobj_NOUN_Number , nsubj_PRON_PronType advmod_ADV_Degree ROOT_VERB_VerbForm_Tense dobj_NOUN_Number prep_ADP nmod_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number , compound_NOUN_Number conj_NOUN_Number intj_ADP advcl_VERB_VerbForm_Tense_Aspect , conj_VERB_VerbForm_Tense_Aspect prep_PART dobj_NOUN_Number advmod_ADV_Degree prep_ADP nummod_NUM_NumType npadvmod_NOUN_Number pcomp_ADV_Degree .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_lqXxXs52wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d150b40-1a9d-4584-ff24-b68d94068be8"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/bea-train-strict.tsv bea-train-strict.tsv\n",
        "\n",
        "with open(\"bea-train-strict.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  bea_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/bea-train-strict.tsv...\n",
            "\\ [1/1 files][ 16.6 MiB/ 16.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/16.6 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnZW6qsw52w2"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WPn29Vm52w2"
      },
      "source": [
        "def bea_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bea: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I9wCk9c52w6"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySkJr3iB52w7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9935552-2a99-4008-fdb9-ba8c9e7da4b6"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"bea\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=bea_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[bea_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-KuPfGm52w_"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_5atvEn52w_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abdfdb8-c3aa-4066-fdf6-4df003f9a8ae"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"bea\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'bea: sentence: The answer is pork , lamb and other meats . parsing: det_DET nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person attr_NOUN_Number , conj_NOUN_Number cc_CCONJ_ConjType amod_ADJ_Degree conj_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10,    37,  1525,    19, 13654,\n",
            "           3,     6, 17871,    11,   119,  3604,     7,     3,     5,\n",
            "         260,     7,    53,    10,    20,    17,   834,  5596,   382,\n",
            "           3,    29,  7304,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49, 10264,  6951,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,   834,   382,  5167,   834,   567,  5937,\n",
            "          49,   834,   345, 13515,    44,    17,    52,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     6,   975,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,    75,\n",
            "          75,   834,   254, 17752,   683,   834,  4302,   354, 25160,\n",
            "           3,     9,  7360,   834,  6762,   683,   834,  2962,  3584,\n",
            "          15,   975,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,     3,     5,     1]), 'targets_plaintext': b'sentence: The answer is pork , lamb and other meats . parsing: det_DET nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person attr_NOUN_Number , conj_NOUN_Number cc_CCONJ_ConjType amod_ADJ_Degree conj_NOUN_Number .', 'targets': array([ 7142,    10,    37,  1525,    19, 13654,     3,     6, 17871,\n",
            "          11,   119,  3604,     7,     3,     5,   260,     7,    53,\n",
            "          10,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         834,   382,  5167,   834,   567,  5937,    49,   834,   345,\n",
            "       13515,    44,    17,    52,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,     6,   975,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,     3,    75,    75,   834,   254,\n",
            "       17752,   683,   834,  4302,   354, 25160,     3,     9,  7360,\n",
            "         834,  6762,   683,   834,  2962,  3584,    15,   975,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,     5,\n",
            "           1])}\n",
            "{'inputs_plaintext': b'bea: sentence: After that the poles will have melt totally because the temperature of the sun will be become crazy and very hard so some people will have died for diseases , for example cancer . parsing: mark_ADP mark_ADP det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree mark_ADP det_DET nsubjpass_NOUN_Number prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm advcl_VERB_VerbForm_Tense_Aspect acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_ADV_Degree advmod_ADV_Degree det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm conj_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NOUN_Number , prep_ADP pobj_NOUN_Number appos_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10,   621,    24,     8, 11148,\n",
            "           7,    56,    43, 13297,  3536,   250,     8,  2912,    13,\n",
            "           8,  1997,    56,    36,   582,  6139,    11,   182,   614,\n",
            "          78,   128,   151,    56,    43,  3977,    21,  6716,     3,\n",
            "           6,    21,   677,  1874,     3,     5,   260,     7,    53,\n",
            "          10,  3946,   834,   188,  7410,  3946,   834,   188,  7410,\n",
            "          20,    17,   834,  5596,   382,     3,    29,  7304,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   742,   834,\n",
            "       16174,   279,   834,  5000,   115, 25160,   742,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51, 10264,  6951,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   188,  5628,     3,     9,    26,   208,  7360,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,  3946,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,     3,    29,\n",
            "        7304,   354,  3968,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,    20,    17,   834,  5596,\n",
            "         382,  1977,   115,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160,   742,  3968,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,     9,    26,   208,    75,    40,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   188,  5628,     3,     9,  7699,   834,  6762,\n",
            "         683,   834,  2962,  3584,    15,     3,    75,    75,   834,\n",
            "         254, 17752,   683,   834,  4302,   354, 25160,     3,     9,\n",
            "          26,   208,  7360,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,   975,   354,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,     3,     9,    26,   208,  7360,   834,   188, 13529,\n",
            "         834,  2962,  3584,    15,    20,    17,   834,  5596,   382,\n",
            "           3,    29,  7304,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160,   742,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   975,   354,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628, 13422,\n",
            "         834,   188,  7410,  1977,   115,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,     3,     6, 13422,   834,   188,\n",
            "        7410,  1977,   115,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,  1120,    32,     7,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,     5,     1]), 'targets_plaintext': b'sentence: After that the poles will have melt totally because the temperature of the sun will be become crazy and very hard so some people will have died for diseases , for example cancer . parsing: mark_ADP mark_ADP det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree mark_ADP det_DET nsubjpass_NOUN_Number prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm advcl_VERB_VerbForm_Tense_Aspect acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_ADV_Degree advmod_ADV_Degree det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm conj_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NOUN_Number , prep_ADP pobj_NOUN_Number appos_NOUN_Number .', 'targets': array([ 7142,    10,   621,    24,     8, 11148,     7,    56,    43,\n",
            "       13297,  3536,   250,     8,  2912,    13,     8,  1997,    56,\n",
            "          36,   582,  6139,    11,   182,   614,    78,   128,   151,\n",
            "          56,    43,  3977,    21,  6716,     3,     6,    21,   677,\n",
            "        1874,     3,     5,   260,     7,    53,    10,  3946,   834,\n",
            "         188,  7410,  3946,   834,   188,  7410,    20,    17,   834,\n",
            "        5596,   382,     3,    29,  7304,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,   742,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51, 10264,  6951,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628,     3,     9,    26,   208,  7360,   834,   188, 13529,\n",
            "         834,  2962,  3584,    15,  3946,   834,   188,  7410,    20,\n",
            "          17,   834,  5596,   382,     3,    29,  7304,   354,  3968,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,   742,  3968,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,     3,\n",
            "           9,    26,   208,    75,    40,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628,     3,     9,  7699,   834,  6762,   683,   834,  2962,\n",
            "        3584,    15,     3,    75,    75,   834,   254, 17752,   683,\n",
            "         834,  4302,   354, 25160,     3,     9,    26,   208,  7360,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,   975,   354,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,     3,     9,\n",
            "          26,   208,  7360,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,   742,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   975,   354,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,   834,\n",
            "         382,  5167,   834,   188,  5628, 13422,   834,   188,  7410,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,     3,     6, 13422,   834,   188,  7410,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,  1120,\n",
            "          32,     7,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,     5,     1])}\n",
            "{'inputs_plaintext': b'bea: \"sentence: A president of one country can be seen and heard in a country far away giving his views and ideas so that all can \"\" understand \"\" him . parsing: det_DET nsubjpass_NOUN_Number prep_ADP nummod_NUM_NumType pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect cc_CCONJ_ConjType conj_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number advmod_ADV_Degree advmod_ADV_Degree advcl_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number mark_ADP mark_ADP nsubj_DET aux_VERB_VerbType \"\" advcl_VERB_VerbForm \"\" dobj_PRON_PronType .\"', 'inputs': array([   36,     9,    10,    96,  5277,  1433,    10,    71,  2753,\n",
            "          13,    80,   684,    54,    36,   894,    11,  1943,    16,\n",
            "           3,     9,   684,   623,   550,  1517,   112,  2441,    11,\n",
            "         912,    78,    24,    66,    54,    96,   121,   734,    96,\n",
            "         121,   376,     3,     5,   260,     7,    53,    10,    20,\n",
            "          17,   834,  5596,   382,     3,    29,  7304,   354,  3968,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,     3,  5525,  7360,   834,   567,  6122,   834,\n",
            "         567,   440, 25160,  1977,   115,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,   742,  3968,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51, 10264,  6951,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   834,   382,  5167,   834,\n",
            "         188,  5628,     3,    75,    75,   834,   254, 17752,   683,\n",
            "         834,  4302,   354, 25160,   975,   354,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   834,   382,  5167,   834,\n",
            "         188,  5628, 13422,   834,   188,  7410,    20,    17,   834,\n",
            "        5596,   382,  1977,   115,   354,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,     3,     9,    26,\n",
            "         208,  7360,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "           3,     9,    26,   208,    75,    40,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   834,   382,  5167,   834,\n",
            "         188,  5628,     3,  2748,     7,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,   834,   345,    32,     7,     7,   103,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160,   975,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,  3946,   834,   188,  7410,  3946,   834,   188,\n",
            "        7410,     3,    29,  7304,   354,   834,  5596,   382,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,    96,   121,\n",
            "           3,     9,    26,   208,    75,    40,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,    96,   121,   103,   115,\n",
            "         354,   834,   345, 13044,   834,  3174,    29, 25160,     3,\n",
            "         535,     1]), 'targets_plaintext': b'\"sentence: A president of one country can be seen and heard in a country far away giving his views and ideas so that all can \"\" understand \"\" him . parsing: det_DET nsubjpass_NOUN_Number prep_ADP nummod_NUM_NumType pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect cc_CCONJ_ConjType conj_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number advmod_ADV_Degree advmod_ADV_Degree advcl_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number mark_ADP mark_ADP nsubj_DET aux_VERB_VerbType \"\" advcl_VERB_VerbForm \"\" dobj_PRON_PronType .\"', 'targets': array([   96,  5277,  1433,    10,    71,  2753,    13,    80,   684,\n",
            "          54,    36,   894,    11,  1943,    16,     3,     9,   684,\n",
            "         623,   550,  1517,   112,  2441,    11,   912,    78,    24,\n",
            "          66,    54,    96,   121,   734,    96,   121,   376,     3,\n",
            "           5,   260,     7,    53,    10,    20,    17,   834,  5596,\n",
            "         382,     3,    29,  7304,   354,  3968,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49, 13422,   834,   188,  7410,     3,\n",
            "        5525,  7360,   834,   567,  6122,   834,   567,   440, 25160,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,   742,   834, 16174,   279,   834,  5000,   115, 25160,\n",
            "         742,  3968,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51, 10264,  6951,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628,     3,\n",
            "          75,    75,   834,   254, 17752,   683,   834,  4302,   354,\n",
            "       25160,   975,   354,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628, 13422,\n",
            "         834,   188,  7410,    20,    17,   834,  5596,   382,  1977,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,     9,    26,   208,  7360,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,     3,     9,    26,\n",
            "         208,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628,     3,\n",
            "        2748,     7,   834,  6762,   683,   834,  3174,    29, 25160,\n",
            "         834,   345,    32,     7,     7,   103,   115,   354,   834,\n",
            "        7400,  7443,   834,   567,  5937,    49,     3,    75,    75,\n",
            "         834,   254, 17752,   683,   834,  4302,   354, 25160,   975,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,  3946,\n",
            "         834,   188,  7410,  3946,   834,   188,  7410,     3,    29,\n",
            "        7304,   354,   834,  5596,   382,   742,   834, 16174,   279,\n",
            "         834,  5000,   115, 25160,    96,   121,     3,     9,    26,\n",
            "         208,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,    96,   121,   103,   115,   354,   834,   345,\n",
            "       13044,   834,  3174,    29, 25160,     3,   535,     1])}\n",
            "{'inputs_plaintext': b'bea: sentence: Believe that the potential lies deep within you and that the sky is the limit . parsing: ROOT_VERB_VerbForm mark_ADP det_DET nsubj_ADJ_Degree ccomp_VERB_VerbForm_Tense_Number_Person advmod_ADV_Degree prep_ADP pobj_PRON_PronType cc_CCONJ_ConjType mark_ADP det_DET nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person det_DET attr_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10, 22070,    24,     8,  1055,\n",
            "        7797,  1659,   441,    25,    11,    24,     8,  5796,    19,\n",
            "           8,  2006,     3,     5,   260,     7,    53,    10, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "        3946,   834,   188,  7410,    20,    17,   834,  5596,   382,\n",
            "           3,    29,  7304,   354,   834,  6762,   683,   834,  2962,\n",
            "        3584,    15,     3,    75,  7699,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   567,\n",
            "        5937,    49,   834,   345, 13515,     3,     9,    26,   208,\n",
            "        7360,   834,   188, 13529,   834,  2962,  3584,    15, 13422,\n",
            "         834,   188,  7410,  1977,   115,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,     3,    75,    75,   834,   254,\n",
            "       17752,   683,   834,  4302,   354, 25160,  3946,   834,   188,\n",
            "        7410,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "          75,  7699,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   567,  5937,    49,   834,\n",
            "         345, 13515,    20,    17,   834,  5596,   382,    44,    17,\n",
            "          52,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "           5,     1]), 'targets_plaintext': b'sentence: Believe that the potential lies deep within you and that the sky is the limit . parsing: ROOT_VERB_VerbForm mark_ADP det_DET nsubj_ADJ_Degree ccomp_VERB_VerbForm_Tense_Number_Person advmod_ADV_Degree prep_ADP pobj_PRON_PronType cc_CCONJ_ConjType mark_ADP det_DET nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person det_DET attr_NOUN_Number .', 'targets': array([ 7142,    10, 22070,    24,     8,  1055,  7797,  1659,   441,\n",
            "          25,    11,    24,     8,  5796,    19,     8,  2006,     3,\n",
            "           5,   260,     7,    53,    10, 10264,  6951,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,  3946,   834,   188,\n",
            "        7410,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  6762,   683,   834,  2962,  3584,    15,     3,\n",
            "          75,  7699,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   567,  5937,    49,   834,\n",
            "         345, 13515,     3,     9,    26,   208,  7360,   834,   188,\n",
            "       13529,   834,  2962,  3584,    15, 13422,   834,   188,  7410,\n",
            "        1977,   115,   354,   834,   345, 13044,   834,  3174,    29,\n",
            "       25160,     3,    75,    75,   834,   254, 17752,   683,   834,\n",
            "        4302,   354, 25160,  3946,   834,   188,  7410,    20,    17,\n",
            "         834,  5596,   382,     3,    29,  7304,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    75,  7699,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   567,  5937,    49,   834,   345, 13515,    20,\n",
            "          17,   834,  5596,   382,    44,    17,    52,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     5,     1])}\n",
            "{'inputs_plaintext': b'bea: sentence: Consider a general election , in which three parties stand , X , Y , and Z , in a country of 5000 voters . parsing: ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number , prep_ADP pobj_ADJ_PronType nummod_NUM_NumType nsubj_NOUN_Number relcl_VERB_VerbForm_Tense , appos_PROPN_NounType_Number , conj_PROPN_NounType_Number , cc_CCONJ_ConjType appos_NOUN_Number , prep_ADP det_DET pobj_NOUN_Number prep_ADP compound_NUM_NumType pobj_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10,  9151,     3,     9,   879,\n",
            "        4356,     3,     6,    16,    84,   386,  2251,  1518,     3,\n",
            "           6,     3,     4,     3,     6,     3,   476,     3,     6,\n",
            "          11,  1027,     3,     6,    16,     3,     9,   684,    13,\n",
            "           3, 12814, 10861,     3,     5,   260,     7,    53,    10,\n",
            "       10264,  6951,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,    20,    17,   834,  5596,   382,     3,     9,  7360,\n",
            "         834,  6762,   683,   834,  2962,  3584,    15,   103,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "           6, 13422,   834,   188,  7410,  1977,   115,   354,   834,\n",
            "        6762,   683,   834,  3174,    29, 25160,     3,  5525,  7360,\n",
            "         834,   567,  6122,   834,   567,   440, 25160,     3,    29,\n",
            "        7304,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "        8318,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,     3,     6,  1120,    32,\n",
            "           7,   834, 17618, 15420,   834,  4168,   202, 25160,   834,\n",
            "         567,  5937,    49,     3,     6,   975,   354,   834, 17618,\n",
            "       15420,   834,  4168,   202, 25160,   834,   567,  5937,    49,\n",
            "           3,     6,     3,    75,    75,   834,   254, 17752,   683,\n",
            "         834,  4302,   354, 25160,  1120,    32,     7,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     6, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49, 13422,\n",
            "         834,   188,  7410, 12771,   834,   567,  6122,   834,   567,\n",
            "         440, 25160,  1977,   115,   354,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,     5,     1]), 'targets_plaintext': b'sentence: Consider a general election , in which three parties stand , X , Y , and Z , in a country of 5000 voters . parsing: ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number , prep_ADP pobj_ADJ_PronType nummod_NUM_NumType nsubj_NOUN_Number relcl_VERB_VerbForm_Tense , appos_PROPN_NounType_Number , conj_PROPN_NounType_Number , cc_CCONJ_ConjType appos_NOUN_Number , prep_ADP det_DET pobj_NOUN_Number prep_ADP compound_NUM_NumType pobj_NOUN_Number .', 'targets': array([ 7142,    10,  9151,     3,     9,   879,  4356,     3,     6,\n",
            "          16,    84,   386,  2251,  1518,     3,     6,     3,     4,\n",
            "           3,     6,     3,   476,     3,     6,    11,  1027,     3,\n",
            "           6,    16,     3,     9,   684,    13,     3, 12814, 10861,\n",
            "           3,     5,   260,     7,    53,    10, 10264,  6951,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,    20,    17,\n",
            "         834,  5596,   382,     3,     9,  7360,   834,  6762,   683,\n",
            "         834,  2962,  3584,    15,   103,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     6, 13422,   834,\n",
            "         188,  7410,  1977,   115,   354,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,     3,  5525,  7360,   834,   567,  6122,\n",
            "         834,   567,   440, 25160,     3,    29,  7304,   354,   834,\n",
            "        7400,  7443,   834,   567,  5937,    49,  8318,    75,    40,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,   834,\n",
            "         382,  5167,     3,     6,  1120,    32,     7,   834, 17618,\n",
            "       15420,   834,  4168,   202, 25160,   834,   567,  5937,    49,\n",
            "           3,     6,   975,   354,   834, 17618, 15420,   834,  4168,\n",
            "         202, 25160,   834,   567,  5937,    49,     3,     6,     3,\n",
            "          75,    75,   834,   254, 17752,   683,   834,  4302,   354,\n",
            "       25160,  1120,    32,     7,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,     6, 13422,   834,   188,  7410,    20,\n",
            "          17,   834,  5596,   382,  1977,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49, 13422,   834,   188,  7410,\n",
            "       12771,   834,   567,  6122,   834,   567,   440, 25160,  1977,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,     5,     1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlghm_3rAd-M"
      },
      "source": [
        "## Dataset Mixture\n",
        "\n",
        "We now create a `Mixture` from the above `Tasks`, which we will fine-tune on.\n",
        "\n",
        "There are different ways to automatically set the rate (for example, based on the number of examples using `rate_num_examples`), but we will just hardcode an equal mixture for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWe_XOtY9VPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fc05a4e5-bfd3-47b4-fcc6-d3a0d6fd0a78"
      },
      "source": [
        "print(correct_len)\n",
        "print(conll_len)\n",
        "print(jfleg_len)\n",
        "print(bea_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32668\n",
            "28346\n",
            "3016\n",
            "34304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfmiY5cT9XWE"
      },
      "source": [
        "tasks_and_weights = [\n",
        "  ('correct', float(correct_len)),\n",
        "  ('conll', float(conll_len)),\n",
        "  ('jfleg', float(jfleg_len)),\n",
        "  ('bea', float(bea_len))\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgs-s3eDAU37"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"correctit_all\")\n",
        "t5.data.MixtureRegistry.add(\"correctit_all\", tasks_and_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUkorodCENGw"
      },
      "source": [
        "# Transferring to new Tasks\n",
        "\n",
        "We are now ready to fine-tune one of the pre-trained T5 models on our new mixture of closed-book QA tasks.\n",
        "\n",
        "First, we'll instantiate a `Model` object using the model size of your choice. Note that larger models are slower to train and use but will likely achieve higher accuracy. You also may be able to increase accuracy by training longer with more `FINETUNE_STEPS` below.\n",
        "\n",
        "\n",
        "## Caveats\n",
        "\n",
        "* Due to its memory requirements, you will not be able to train the `11B` parameter model on the TPU provided by Colab. Instead, you will need to fine-tune inside of a GCP instance (see [README](https://github.com/google-research/text-to-text-transfer-transformer/)).\n",
        "* Due to the checkpoint size, you will not be able use the 5GB GCS free tier for the `3B` parameter models. You will need at least 25GB of space, which you can purchase with your $300 of initial credit on GCP.\n",
        "* While `large` can achieve decent results, it is recommended that you fine-tune at least the `3B` parameter model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syte5n0nnMOC"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pesK1uu6QAqx"
      },
      "source": [
        "run = \"total\"  # @param {\"type\": \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGQ-zpgy3raf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7164a478-3dbb-4759-b636-7faa5bab7c46"
      },
      "source": [
        "MODEL_SIZE = \"3B\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n",
        "# Public GCS path for T5 pre-trained model checkpoints\n",
        "BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "if run not in [None, \"\"]:\n",
        "    MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE+\"-\"+run)\n",
        "else:\n",
        "    MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n",
        "\n",
        "if ON_CLOUD and MODEL_SIZE == \"3B\":\n",
        "  tf.logging.warn(\n",
        "      \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n",
        "      \"Make sure you have at least 25GB on GCS before continuing.\"\n",
        "  )\n",
        "elif ON_CLOUD and MODEL_SIZE == \"11B\":\n",
        "  raise ValueError(\n",
        "      \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n",
        "      \"provided by Colab. Please comment out this Error if you're running \"\n",
        "      \"on a larger TPU.\"\n",
        "  )\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 128, 16),\n",
        "    \"base\": (2, 64, 8),\n",
        "    \"large\": (8, 32, 4),\n",
        "    \"3B\": (8, 8, 1),\n",
        "    \"11B\": (8, 8, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `3B` model is too large to use with the 5GB GCS free tier. Make sure you have at least 25GB on GCS before continuing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYeciUZ_D7T2"
      },
      "source": [
        "## Train and evaluate\n",
        "\n",
        "We now evaluate on the validation sets of the tasks in our mixture. Accuracy results will be logged and added to the TensorBoard above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDGYBcDKSSfy"
      },
      "source": [
        "from mosestokenizer import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYpQCcENUN9g"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\", disable=[\"tagger\", \"parser\", 'ner', 'textcat', 'lemmatizer'])\n",
        "tokenify = lambda snt: \" \".join(str(x) for x in nlp(snt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0FBl-hNT0y6"
      },
      "source": [
        "from contextlib import contextmanager\n",
        "import logging\n",
        "\n",
        "@contextmanager\n",
        "def all_logging_disabled(highest_level=logging.CRITICAL):\n",
        "    \"\"\"\n",
        "    A context manager that will prevent any logging messages\n",
        "    triggered during the body from being processed.\n",
        "    :param highest_level: the maximum logging level in use.\n",
        "      This would only need to be changed if a custom level greater than CRITICAL\n",
        "      is defined.\n",
        "    \"\"\"\n",
        "    # two kind-of hacks here:\n",
        "    #    * can't get the highest logging level in effect => delegate to the user\n",
        "    #    * can't get the current module-level override => use an undocumented\n",
        "    #       (but non-private!) interface\n",
        "\n",
        "    previous_level = logging.root.manager.disable\n",
        "\n",
        "    logging.disable(highest_level)\n",
        "\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        logging.disable(previous_level)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sY09GFvCRI_"
      },
      "source": [
        "from nltk import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVDH7iZBuVU"
      },
      "source": [
        "def heal(insent, raw=True):\n",
        "  insent = insent.replace(chr(8263) + \" \", \"<\")\n",
        "  if not raw:\n",
        "    tlist = tokenify(insent).split(\" \")\n",
        "    with MosesDetokenizer('en') as detokenize:\n",
        "      ss = detokenize(tlist)\n",
        "    # outsent = \" \".join(s.capitalize() for s in sent_tokenize(ss, \"english\"))\n",
        "    outsent = outsent.replace(\" - \", \"-\").replace(\" 've\", \"\")\n",
        "    # outsent = re.sub(r'(.*? < br > )(.)(.*?)', lambda m: r'{}'.format(m.group(1)+m.group(2).upper()+m.group(3)), outsent)\n",
        "  outsent = outsent.replace(\" <br> \", \"\\n\").replace(\" < br > \", \"\\n\")\n",
        "  return outsent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RgVhOk0VzX8"
      },
      "source": [
        "import warnings\n",
        "from math import ceil\n",
        "\n",
        "from nltk.translate.gleu_score import corpus_gleu\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw_Pj_Mre5LM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6a64377b-75ae-4ffc-fa67-eb3cbefb10cb"
      },
      "source": [
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/correct-target.tsv correct-target.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/correct-target.tsv...\n",
            "- [1/1 files][ 13.4 MiB/ 13.4 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/13.4 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqkvMe8A1Hks"
      },
      "source": [
        "import signal\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class TimeoutException(Exception): pass\n",
        "\n",
        "@contextmanager\n",
        "def time_limit(seconds):\n",
        "    def signal_handler(signum, frame):\n",
        "        raise TimeoutException(\"Timed out!\")\n",
        "    signal.signal(signal.SIGALRM, signal_handler)\n",
        "    signal.alarm(seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.alarm(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm0S1f5Ee136"
      },
      "source": [
        "ft_steps = 6400\n",
        "n_ckpts = 4\n",
        "steparr = [int(n) for n in np.linspace(0, ft_steps, n_ckpts)[1:]]\n",
        "\n",
        "for i in range(n_ckpts):\n",
        "  STEP = steparr[i]\n",
        "\n",
        "  print(\"**** Training checkpoint %s ****\" % str(STEP))\n",
        "  # The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "  model = t5.models.MtfModel(\n",
        "      model_dir=MODEL_DIR,\n",
        "      tpu=TPU_ADDRESS,\n",
        "      tpu_topology=TPU_TOPOLOGY,\n",
        "      model_parallelism=model_parallelism,\n",
        "      batch_size=train_batch_size,\n",
        "      sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "      learning_rate_schedule=0.0025,\n",
        "      save_checkpoints_steps=STEP + 1000,\n",
        "      keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "      iterations_per_loop=100,\n",
        "  )\n",
        "\n",
        "  FINETUNE_STEPS = STEP\n",
        "  print(\"Finetuning for\", FINETUNE_STEPS, \"steps\")\n",
        "\n",
        "  model.finetune(\n",
        "      mixture_or_task_name=\"correctit_all\",\n",
        "      pretrained_model_dir=PRETRAINED_DIR,\n",
        "      finetune_steps=FINETUNE_STEPS\n",
        "  )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"\")\n",
        "\n",
        "  epoch = 1000000 + STEP\n",
        "  print(\"**** Evaluating checkpoint %s ****\" % str(epoch))\n",
        "\n",
        "  with all_logging_disabled():\n",
        "    model.batch_size = train_batch_size * 4\n",
        "    model.eval(\n",
        "        mixture_or_task_name=\"correctit_all\",\n",
        "        checkpoint_steps=[epoch]\n",
        "    )\n",
        "\n",
        "    for task in tasks:\n",
        "      !gsutil -m cp -r $MODEL_DIR/validation_eval/{task}_{str(epoch)}_predictions {task}_{str(epoch)}.bin\n",
        "      with open(task + \"_\" + str(epoch) + \".bin\", \"rb\") as inf:\n",
        "        exec(task + ' = inf.read().decode().split(\"\\\\n\")[:-1]')\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on raw outputs **\")\n",
        "\n",
        "  refs = list(test[\"targets\"])\n",
        "  tknzd = [tokenify(sent) for sent in refs]\n",
        "\n",
        "  cor_df = pd.DataFrame({\n",
        "      \"predictions\": correct,\n",
        "      \"references\": refs,\n",
        "      \"tokenized\": tknzd\n",
        "  })\n",
        "\n",
        "  picklename = \"corr_test_tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch) + \".pickle\"\n",
        "  cor_df.to_pickle(picklename)\n",
        "\n",
        "  !cp ./{picklename} /content/gdrive/My\\ Drive/heptabot/output\n",
        "\n",
        "  t_preds = [tokenify(sent).split(\" \") for sent in correct]\n",
        "  t_refs = [ref.split(\" \") for ref in tknzd]\n",
        "  gleu = corpus_gleu([[ref] for ref in t_refs], t_preds)\n",
        "  print(\"GLEU:\", gleu)\n",
        "\n",
        "  try:\n",
        "    with time_limit(300):\n",
        "      rouge_scores = rouge.get_scores(correct, refs, avg=True, ignore_empty=False)\n",
        "      print(\"ROUGE-L:\", rouge_scores[\"rouge-l\"])\n",
        "  except TimeoutException as e:\n",
        "    print(\"ROUGE function timed out\")\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on BEA-2019 **\")\n",
        "  outstr = \"\"\n",
        "  for line in bea:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"ABCN.bea19.test.corr\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "\n",
        "  !zip bea-test-{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.zip ABCN.bea19.test.corr\n",
        "  !cp ./bea-test-{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.zip /content/gdrive/My\\ Drive/heptabot/output\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on JFLEG **\")\n",
        "  %cd jfleg\n",
        "  outstr = \"\"\n",
        "\n",
        "  for line in jfleg:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"test.nospc.res\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "  \n",
        "  !cp ./test.nospc.res /content/gdrive/My\\ Drive/heptabot/output/test.nospc.{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.res\n",
        "\n",
        "  !python ./eval/gleu.py -r ./test/test.ref[0-3] -s ./test/test.src --hyp test.nospc.res\n",
        "\n",
        "  %cd ../\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on CoNLL-2014 **\")\n",
        "  outstr = \"\"\n",
        "\n",
        "  for line in conll:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"conll14_nospc.txt\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "  !cp ./conll14_nospc.txt /content/gdrive/My\\ Drive/heptabot/output/conll14_nospc_{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.txt\n",
        "\n",
        "  try:\n",
        "    with time_limit(300):\n",
        "      !python2 ./m2scorer/scripts/m2scorer.py ./conll14_nospc.txt ./conll14st-test-data/noalt/official-2014.combined.m2\n",
        "  except TimeoutException as e:\n",
        "    print(\"M2 scorer timed out\")\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "w9Ny7g-m2Umh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "b305393e-fba3-4656-8ade-6a9b21cb7cd8"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR\n",
        "\n",
        "ckpts = !gsutil ls $MODEL_DIR\n",
        "ckptset = set(int(re.search(r\"ckpt-([0-9]+?)\\.data\", s).group(1)) for s in ckpts\n",
        "              if re.search(r\"ckpt-([0-9]+?)\\.data\", s))\n",
        "ckptlist = sorted(list(ckptset))\n",
        "\n",
        "tasks = [\"correct\", \"jfleg\", \"conll\", \"bea\"]\n",
        "test = pd.read_csv(\"correct-target.tsv\", sep=\"\\t\", header=None)\n",
        "test.columns = [\"sources\", \"targets\"]\n",
        "\n",
        "\n",
        "for epoch in ckptlist:\n",
        "  print(\"**** Evaluating checkpoint %s ****\" % str(epoch))\n",
        "\n",
        "  with all_logging_disabled():\n",
        "    model.batch_size = train_batch_size * 4\n",
        "    model.eval(\n",
        "        mixture_or_task_name=\"correctit_all\",\n",
        "        checkpoint_steps=[epoch]\n",
        "    )\n",
        "\n",
        "    for task in tasks:\n",
        "      !gsutil -m cp -r $MODEL_DIR/validation_eval/{task}_{str(epoch)}_predictions {task}_{str(epoch)}.bin\n",
        "      with open(task + \"_\" + str(epoch) + \".bin\", \"rb\") as inf:\n",
        "        exec(task + ' = inf.read().decode().split(\"\\\\n\")[:-1]')\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on raw outputs **\")\n",
        "\n",
        "  refs = list(test[\"targets\"])\n",
        "  tknzd = [tokenify(sent) for sent in refs]\n",
        "\n",
        "  cor_df = pd.DataFrame({\n",
        "      \"predictions\": correct,\n",
        "      \"references\": refs,\n",
        "      \"tokenized\": tknzd\n",
        "  })\n",
        "\n",
        "  picklename = \"corr_test_tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch) + \".pickle\"\n",
        "  cor_df.to_pickle(picklename)\n",
        "\n",
        "  !cp ./{picklename} /content/gdrive/My\\ Drive/heptabot/output\n",
        "\n",
        "  t_preds = [tokenify(sent).split(\" \") for sent in correct]\n",
        "  t_refs = [ref.split(\" \") for ref in tknzd]\n",
        "  gleu = corpus_gleu([[ref] for ref in t_refs], t_preds)\n",
        "  print(\"GLEU:\", gleu)\n",
        "\n",
        "  try:\n",
        "    with time_limit(300):\n",
        "      rouge_scores = rouge.get_scores(correct, refs, avg=True, ignore_empty=False)\n",
        "      print(\"ROUGE-L:\", rouge_scores[\"rouge-l\"])\n",
        "  except TimeoutException as e:\n",
        "    print(\"ROUGE function timed out\")\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on BEA-2019 **\")\n",
        "  outstr = \"\"\n",
        "  for line in bea:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"ABCN.bea19.test.corr\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "\n",
        "  !zip bea-test-{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.zip ABCN.bea19.test.corr\n",
        "  !cp ./bea-test-{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.zip /content/gdrive/My\\ Drive/heptabot/output\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on JFLEG **\")\n",
        "  %cd jfleg\n",
        "  outstr = \"\"\n",
        "\n",
        "  for line in jfleg:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"test.nospc.res\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "  \n",
        "  !cp ./test.nospc.res /content/gdrive/My\\ Drive/heptabot/output/test.nospc.{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.res\n",
        "\n",
        "  !python ./eval/gleu.py -r ./test/test.ref[0-3] -s ./test/test.src --hyp test.nospc.res\n",
        "\n",
        "  %cd ../\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on CoNLL-2014 **\")\n",
        "  outstr = \"\"\n",
        "\n",
        "  for line in conll:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"conll14_nospc.txt\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "  !cp ./conll14_nospc.txt /content/gdrive/My\\ Drive/heptabot/output/conll14_nospc_{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.txt\n",
        "\n",
        "  try:\n",
        "    with time_limit(300):\n",
        "      !python2 ./m2scorer/scripts/m2scorer.py ./conll14_nospc.txt ./conll14st-test-data/noalt/official-2014.combined.m2\n",
        "  except TimeoutException as e:\n",
        "    print(\"M2 scorer timed out\")\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Evaluating checkpoint 1006400 ****\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/models/3B-total/validation_eval/correct_1006400_predictions...\n",
            "- [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/3B-total/validation_eval/jfleg_1006400_predictions...\n",
            "/ [1/1 files][ 71.6 KiB/ 71.6 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.6 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/3B-total/validation_eval/conll_1006400_predictions...\n",
            "/ [1/1 files][158.5 KiB/158.5 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.5 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/3B-total/validation_eval/bea_1006400_predictions...\n",
            "/ [1/1 files][430.4 KiB/430.4 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/430.4 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7928478747125757\n",
            "ROUGE-L: {'f': 0.9014139075783819, 'p': 0.9033984605242797, 'r': 0.9005185228217442}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.637448', '0.007886', '(0.622,0.653)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6595\n",
            "Recall      : 0.5392\n",
            "F_0.5       : 0.6313\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}