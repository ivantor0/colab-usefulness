{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Train_current_heptabot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3vVHx6AVGrtM",
        "dMoJ-G9mqDqa",
        "152zECujzPMk",
        "ailCy7DO4--y",
        "bpJm0H4y5lgz",
        "1Tx8nxpt52wi"
      ]
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YONnGjpAYUdU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/ivantor0/colab-usefulness/blob/master/Train_current_heptabot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrtR2urJV3ST",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 The T5 Authors\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWdCSqJ6WHBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 The T5 Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSeyoqE7WMwu",
        "colab_type": "text"
      },
      "source": [
        "# Fine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\n",
        "## _Or: What does T5 know?_\n",
        "\n",
        "*The following tutorial guides you through the process of fine-tuning a pre-trained T5 model, evaluating its accuracy, and using it for prediction,\n",
        "all on a free Google Cloud TPU <a href=\"https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>.*\n",
        "\n",
        "### Background\n",
        "\n",
        "T5 was introduced in the paper [_Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer_](https://arxiv.org/abs/1910.10683). In that paper, we provided a comprehensive picture of how we pre-trained a standard text-to-text Transformer model on a large text corpus, achieving state-of-the-art results on many NLP tasks after fine-tuning.\n",
        "\n",
        "We pre-trained T5 on a mixture of supervised and unsupervised tasks with the majoriy of data coming from an unlabeled dataset we developed called [C4](https://www.tensorflow.org/datasets/catalog/c4). C4 is based on a massive scrape of the web produced by [Common Crawl](https://commoncrawl.org). Loosely speaking, pre-training on C4 ideally gives T5 an understanding of natural language in addition to general world knowledge.\n",
        "\n",
        "### How can we assess what T5 knows?\n",
        "\n",
        "As the name implies, T5 is a text-to-text model, which enables us to train it on arbitrary tasks involving a textual input and output. As we showed in our paper, a huge variety of NLP tasks can be cast in this format, including translation, summarization, and even classification and regression tasks.\n",
        "\n",
        "One way to use this text-to-text framework is on reading comprehension problems, where the model is fed some context along with a orig_text and is trained to predict the orig_text's corr_text. For example, we might feed the model the text from the Wikipedia article about [Hurrican Connie](https://en.wikipedia.org/wiki/Hurricane_Connie) along with the orig_text \"On what date did Hurricane Connie occur?\" and train the model to predict the corr_text \"August 3rd, 1955\".\n",
        "A related task is open-domain orig_text answering (QA) where the model is not provided with this oracle context. Typically, open-domain QA systems include a mechanism to look up information in an external knowledge source. This setting is similar to an \"open-book\" exam.\n",
        "\n",
        "In this notebook, we'll be training T5 on a variant of this task which we call **closed-book orig_text answering**. In closed-book QA, we feed the model a orig_text *without any context or access to external knowledge* and train it to predict the corr_text. Since the model doesn't receive any context, the primary way it can learn to corr_text these orig_texts is based on the \"knowledge\" it obtained during pre-training. We don't expect T5 to contain super specific information, so we will be focusing on two orig_text-answering datasets which largely include trivia orig_texts (i.e. facts about well-known subjects). [Similar](https://arxiv.org/abs/1909.01066) [investigations](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) have recently been done to test the knowledge stored by BERT and GPT-2.\n",
        "\n",
        "T5 was not pre-trained on closed-book QA, so in this notebook we'll first create two new tasks and then use the [`t5`](https://github.com/google-research/text-to-text-transfer-transformer) library to fine-tune, evaluate, and obtain predictions from T5. In the end, T5's performance on closed-book QA can give us a sense of what kind (and how much) information T5 managed to learn during pre-training.\n",
        "\n",
        "## State-of-the-art Results\n",
        "We published a [more in-depth investigation](https://arxiv.org/abs/2002.08910) of closed-book QA with T5 where we achieved SOTA on open-domain variants of WebQuestions and TriviaQA in addition to surpisingly strong results on Natural Questions. The code in this notebook is a simplified version of those experiments but still produces good results.\n",
        "\n",
        "For code to reproduce our best results, please see the [t5_closed_book_qa](https://github.com/google-research/google-research/tree/master/t5_closed_book_qa) repo.\n",
        "\n",
        "\n",
        "### Caveats\n",
        "\n",
        "* While we provide instructions for running on a [Cloud TPU](https://cloud.google.com/tpu/) via Colab for free, a [Google Cloud Storage (GCS)](http://console.cloud.google.com/storage) bucket is required for storing model parameters and data. The [GCS free tier](https://cloud.google.com/free/) provides 5 GB of storage, which should be enough to train the `large` model and smaller but not the `3B` or `11B` parameter models. You can use part of your initial $300 credit to get more space.\n",
        "* The Cloud TPU provided by Colab (a `v2-8`) does not have enough memory to fine-tune the `11B` parameter model. For this model, you will need to fine-tune inside of a GCP instance (see [README](https://github.com/google-research/text-to-text-transfer-transformer/)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAb_APDrefs6",
        "colab_type": "text"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDeE_yVuHMYg",
        "colab_type": "text"
      },
      "source": [
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   1. Create a Cloud Storage bucket for your data and model checkpoints at http://console.cloud.google.com/storage, and fill in the `BASE_DIR` parameter in the following form. There is a [free tier](https://cloud.google.com/free/) if you do not yet have an account.\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   1. Run the following cell and follow instructions to:\n",
        "    *  Set up a Colab TPU running environment\n",
        "    *   Verify that you are connected to a TPU device\n",
        "    *   Upload your credentials to TPU to access your GCS bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h86a4THsRv29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7549f228-87b2-4430-ef7b-ec53418752ab"
      },
      "source": [
        "!pip install spacy==1.9.0\n",
        "!python -m spacy download -d en_core_web_sm-1.2.0\n",
        "!python -m spacy link en_core_web_sm en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/ce/afee53c365617e5f3e58825d71421bce14949a15f7150742d2a7b8859c53/spacy-1.9.0.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (1.18.5)\n",
            "Collecting murmurhash<0.27,>=0.26\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/53/1f428861e59c2382e22b8839d03cc315e1a7633a827497b3d389b8d8772d/murmurhash-0.26.4.tar.gz\n",
            "Collecting cymem<1.32,>=1.30\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/0f/d29aa68c55db37844c77e7e96143bd96651fd0f4453c9f6ee043ac846b77/cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/88/57a818051f3d71e800bfb7ba4df56d3ea5793482ef11f1d2109b726f3bac/preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n",
            "\u001b[?25hCollecting thinc<6.6.0,>=6.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/9b/78fab962e0c8b55e3a745ebf2458708dfc7922c55eca3a9bff0233b25294/thinc-6.5.2.tar.gz (926kB)\n",
            "\u001b[K     |████████████████████████████████| 931kB 38.3MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting pip<10.0.0,>=9.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (1.15.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (1.0.1)\n",
            "Collecting ujson>=1.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/1a/36ead6ae1bc3b82ea864ef87f8c8e1e06bec3a745dc65915f47b46f241d0/ujson-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (175kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 54.8MB/s \n",
            "\u001b[?25hCollecting dill<0.3,>=0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==1.9.0) (2.23.0)\n",
            "Collecting regex<2017.12.1,>=2017.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/fb/e894cf877cc3ad788892899c23e2a73e3e54524c4ec6e5d3826fe153af80/regex-2017.11.09.tar.gz (608kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 51.0MB/s \n",
            "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy==1.9.0) (1.12.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy==1.9.0) (4.41.1)\n",
            "Collecting cytoolz<0.9,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/e6/ccc124714dcc1bd511e64ddafb4d5d20ada2533b92e3173a4cf09e0d0831/cytoolz-0.8.2.tar.gz (386kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy==1.9.0) (1.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==1.9.0) (2.10)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (0.2.5)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy==1.9.0) (0.10.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy==1.9.0) (0.5.1)\n",
            "Building wheels for collected packages: spacy, murmurhash, thinc, dill, regex, ftfy, cytoolz\n",
            "  Building wheel for spacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-1.9.0-cp36-cp36m-linux_x86_64.whl size=7776419 sha256=b8e090870801c889dc94e57e1dfe5597c977ef2a21bf0cb062a6ec4866693e3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/b1/94/5e66dac91b157627f0dfc81b3af926e16919e7c0ef9f7e0616\n",
            "  Building wheel for murmurhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for murmurhash: filename=murmurhash-0.26.4-cp36-cp36m-linux_x86_64.whl size=41067 sha256=713561be2bbfefbadea5fd9af7df1a6b30595edc5e2116d2e97b5b1e08c51464\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/af/51/9efd49862c6dcb6439baaa235714fc4de5cecf3e01613b2fef\n",
            "  Building wheel for thinc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thinc: filename=thinc-6.5.2-cp36-cp36m-linux_x86_64.whl size=2313237 sha256=757119405b086d67315f128e45d7274b7b9c3aa32acf557fc2899433188bbfbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/36/8b/306d475aa414ff9fcec211da3da5d5e59582219d57f7ea9fa1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.2.9-cp36-none-any.whl size=77404 sha256=38bbb5663d6a424851273c0db2f7d1ad6af995547f6204b9fb345ae7cedd0732\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.11.9-cp36-cp36m-linux_x86_64.whl size=538835 sha256=89e7a3c759c285b8d8c0a7cf9438fda3f07b33603a45eee4b702741e21f8db10\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/ae/8f/cab02cdf653ac0a2e9d9ec302721ae3c20052a36951addd111\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-4.4.3-cp36-none-any.whl size=41068 sha256=0272490cb4dfad0a47d1c424cae2ac46d7d5861caa02891b5dfb78a9b1fd20a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.8.2-cp36-cp36m-linux_x86_64.whl size=1093017 sha256=51e232cca1c0b528e12e3820a9e13008c00c89cf5941e1d2969cdab61740e50c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b1/86/c92e4d36b690208fff8471711b85eaa6bc6d19860a86199a09\n",
            "Successfully built spacy murmurhash thinc dill regex ftfy cytoolz\n",
            "\u001b[31mERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement spacy>=2.0.18; python_version < \"3.8\", but you'll have spacy 1.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 1.9.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: murmurhash, cymem, preshed, cytoolz, plac, dill, thinc, pip, ujson, regex, ftfy, spacy\n",
            "  Found existing installation: murmurhash 1.0.2\n",
            "    Uninstalling murmurhash-1.0.2:\n",
            "      Successfully uninstalled murmurhash-1.0.2\n",
            "  Found existing installation: cymem 2.0.3\n",
            "    Uninstalling cymem-2.0.3:\n",
            "      Successfully uninstalled cymem-2.0.3\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: dill 0.3.2\n",
            "    Uninstalling dill-0.3.2:\n",
            "      Successfully uninstalled dill-0.3.2\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed cymem-1.31.2 cytoolz-0.8.2 dill-0.2.9 ftfy-4.4.3 murmurhash-0.26.4 pip-9.0.3 plac-0.9.6 preshed-1.0.1 regex-2017.11.9 spacy-1.9.0 thinc-6.5.2 ujson-3.1.0\n",
            "\n",
            "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
            "\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 52.2MB 77.9MB/s \n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "\u001b[?25hRequirement already satisfied: spacy<2.0.0,>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: murmurhash<0.27,>=0.26 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-1.2.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
            "    --> /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en').\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA4ygKpsSQz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "4962f061-7de8-4255-9ed2-751dc885bdab"
      },
      "source": [
        "!pip install mosestokenizer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mosestokenizer\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/b3/c0af235b16c4f44a2828ef017f7947d1262b2646e440f85c6a2ff26a8c6f/mosestokenizer-1.1.0.tar.gz\n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from mosestokenizer)\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e6/805db6867faacb488b44ba8e0829ef4de151dd0499f3c5da5f4ad11698a7/openfile-0.0.7-py3-none-any.whl\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/04/cb/70ed842d9a43460eedaa11f7503b4ab6537b43b63f0d854d59d8e150fac1/uctools-1.3.0.tar.gz\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/41/7b/34bf8fb69426d8a18bcc61081e9d126f4fcd41c3c832072bef39af1602cd/toolwrapper-2.1.0.tar.gz\n",
            "Building wheels for collected packages: mosestokenizer, uctools, toolwrapper\n",
            "  Running setup.py bdist_wheel for mosestokenizer ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a2/e7/48/48d5e0f9c0cd5def2dfd7cb8543945f906448ed1313de24a29\n",
            "  Running setup.py bdist_wheel for uctools ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/06/b6/8f/935d5bf5bca85d47c6f5ec31641879bba057d336ab36b1e773\n",
            "  Running setup.py bdist_wheel for toolwrapper ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/84/ea/29/e02f3b855bf19344972092873a1091b329309bbc3d3d0cbaef\n",
            "Successfully built mosestokenizer uctools toolwrapper\n",
            "Installing collected packages: openfile, uctools, toolwrapper, mosestokenizer\n",
            "Successfully installed mosestokenizer-1.1.0 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQQH8r2VXnDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9319a668-9318-47ec-9a97-efc119cdffc3"
      },
      "source": [
        "!wget https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\n",
        "!tar -xzf m2scorer.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-09 06:54:28--  https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\n",
            "Resolving www.comp.nus.edu.sg (www.comp.nus.edu.sg)... 45.60.31.225\n",
            "Connecting to www.comp.nus.edu.sg (www.comp.nus.edu.sg)|45.60.31.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22836 (22K) [application/x-gzip]\n",
            "Saving to: ‘m2scorer.tar.gz’\n",
            "\n",
            "m2scorer.tar.gz     100%[===================>]  22.30K  82.6KB/s    in 0.3s    \n",
            "\n",
            "2020-08-09 06:54:30 (82.6 KB/s) - ‘m2scorer.tar.gz’ saved [22836/22836]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNaiuRTRx5ws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "72dbf523-4289-4756-ddc2-f13d80bd9805"
      },
      "source": [
        "!git clone https://github.com/keisks/jfleg"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'jfleg'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (170/170), 777.12 KiB | 5.76 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfTs7kzNyxmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5f9c5825-d75b-462b-c80d-4efdb7f72094"
      },
      "source": [
        "!wget https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
        "!tar -xzf conll14st-test-data.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-09 06:54:35--  https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
            "Resolving www.comp.nus.edu.sg (www.comp.nus.edu.sg)... 45.60.31.225\n",
            "Connecting to www.comp.nus.edu.sg (www.comp.nus.edu.sg)|45.60.31.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 643482 (628K) [application/x-gzip]\n",
            "Saving to: ‘conll14st-test-data.tar.gz’\n",
            "\n",
            "conll14st-test-data 100%[===================>] 628.40K   320KB/s    in 2.0s    \n",
            "\n",
            "2020-08-09 06:54:39 (320 KB/s) - ‘conll14st-test-data.tar.gz’ saved [643482/643482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM1P8KYJqbSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3fd3c970-f775-4fb4-a8e5-818213893970"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o68EireWQAp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "5c00742c-9615-4951-fbf6-11500b6509d9"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5==0.6.0\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "BASE_DIR = \"gs://ml-bucket-isikus/t5-base-model\" #@param { type: \"string\" }\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 4.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 307kB 2.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 1.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 2.6MB 458kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 778kB 1.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 12.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 368kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 890kB 2.0MB/s \n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
            "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 396kB/s \n",
            "\u001b[?25h  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
            "\u001b[?25hSetting up GCS access...\n",
            "Running on TPU: grpc://10.42.24.138:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7HvbeSKi_b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2bc45802-737a-4fac-c34b-adc118267cca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAN3xsIX78eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.utils.set_global_cache_dirs([BASE_DIR, os.getcwd()])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XH9lh-gQAp_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3b53da89-8416-4e37-cdb7-c9ab7501c08c"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'better-record'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://ml-bucket-isikus/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao9KKRSzDX8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0adb0814-3ef3-40e4-b8f8-bb3a1c35e87f"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vVHx6AVGrtM",
        "colab_type": "text"
      },
      "source": [
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3c-wp6n_8O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMoJ-G9mqDqa",
        "colab_type": "text"
      },
      "source": [
        "# Creating new Tasks and Mixture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoLPQhE6bef",
        "colab_type": "text"
      },
      "source": [
        "Two core components of the T5 library are `Task` and `Mixture` objects.\n",
        "\n",
        "A `Task` is a dataset along with preprocessing functions and evaluation metrics. A `Mixture` is a collection of `Task` objects along with a mixing rate or a function defining how to compute a mixing rate based on the properties of the constituent `Tasks`.\n",
        "\n",
        "For this example, we will fine-tune the model to do closed-book orig_text answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152zECujzPMk",
        "colab_type": "text"
      },
      "source": [
        "### correct\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjEonhK3zNRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "corr_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"correct-train.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"correct-target.tsv\")\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Ja8akCX1dR",
        "colab_type": "text"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPOteeqctpzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ac704671-501e-45a8-fc39-19be326194e1"
      },
      "source": [
        "def corr_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(corr_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(corr_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'Armageddon is my favourite science fiction movie. The plot is about how to survive in the bad situation. It presented the cooperation from everybody such as new technology, joining between american and russian astronauts and the private people who had great experience in digging. I felt of the bravehearts who make a sacrifice. The soundtrack was pretty good. I felt sad when the character played by Bruce Willis called his daughter on the earth, prior to exploding a main meteor. Armageddon was directed by Michael Bay.', 'corr_text': b'Armageddon is my favourite science fiction movie. The plot is about how to survive in a bad situation. It presented the cooperation from everybody such as new technology, cooperation between American and Russian astronauts and the private people who had great experience in digging. I felt the bravehearts who make a sacrifice. The soundtrack was pretty good. I felt sad when the character played by Bruce Willis called his daughter on the earth, prior to exploding a main meteor. Armageddon was directed by Michael Bay.'}\n",
            "{'orig_text': b'This apartment, that is my home, is too large for a single retired person and so its renting. Its located in the prime Pacific Eights Area and it has a breathtaking view. It was completely remodeled five years ago and so every fixtures are in good order. Walls was repainted last year. Its quite a roomy house. In fact it has a large entrance hall, tree bedrooms, a dining room with an island kitchen, a living room, two bathrooms. There is a walk-in closet near to every bedroom and a great all-wall wood bookshelf in the entrance hall. It has quite high ceilings and marble floors. Furthermore, all rooms have a balcony and the living room has a veranda with a barbeque. This veranda is the very nicest particular of the apartment, because its quite large and supplied with rustic design furniture, so that you can have there your dining on summer. The rent is very affordable at only $1850 per month. Not for nothing, but its really a business. See and believe!', 'corr_text': b'This apartment, that is my home, is too large for a single retired person and so its renting. Its located in the prime Pacific Heights Area and it has a breathtaking view. It was completely remodeled five years ago and so all fixtures are in good order. Walls were repainted last year. Its quite a roomy house. In fact it has a large entrance hall, three bedrooms, a dining room with an island kitchen, a living room and two bathrooms. There is a walk-in closet near to every bedroom and a great all-wall wood bookshelf in the entrance hall. It has quite high ceilings and marble floors. Furthermore, all rooms have a balcony and the living room has a veranda with a barbeque. This veranda is the very nicest part of the apartment, because its quite large and supplied with rustic design furniture, so that you can have your dinner there in summer. The rent is very affordable at only $1850 per month. Not for nothing, but its really a business. See and believe!'}\n",
            "{'orig_text': b\"I am a doctor. I work at a hospital. I work long hours and take short breaks. I take care of my patients. When I finish my work, I feel tired. However, I love my job because I think it's rewarding.I feel very proud to be a doctor.\", 'corr_text': b\"I am a doctor. I work in a hospital. I work long hours and take short breaks. I take care of my patients. When I finish my work, I feel tired. However, I love my job because I think it's rewarding.I feel very proud to be a doctor.\"}\n",
            "{'orig_text': b\"My father is short and fat.He wears just a brown pants.He has a short gray hair.He has a good sense of humour,so he's funny.My mother wears purple singlet, dark blue pants and glasses,because she's very clever.My sister wears red T-shirt and blue pants.She's funny and sociable,but she's naughty.She has a long,straight red hair,like and mother.I love them.\", 'corr_text': b\"My father is short and fat.He wears just a brown pants.He has a short gray hair.He has a good sense of humour,so he's funny.My mother wears purple singlet, dark blue pants and glasses,because she's very clever.My sister wears red T-shirt and blue pants.She's funny and sociable,but she's naughty.She has a long,straight red hair,like and mother.I love them.\"}\n",
            "{'orig_text': b\"Jane,the bus stop is on Gold street,walk along Gold st. then turn left at London Road,turn right Green avenue. My home is next to the supermarket and next to the restaurant.It's opposite to the park. The park is on the corner of Green avenue and Liverpool Road.\", 'corr_text': b\"Jane,the bus stop is on Gold street,walk along Gold st. then turn left at London Road,turn right Green avenue. My home is next to the supermarket and next to the restaurant.It's opposite to the park. The park is on the corner of Green avenue and Liverpool Road.\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imbEYc-tQ2gB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "618aeda3-b4f9-456d-84f4-cb342fc7a026"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/correct-train.tsv correct_train.tsv\n",
        "\n",
        "with open(\"correct_train.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  correct_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/correct-train.tsv...\n",
            "- [1/1 files][ 53.0 MiB/ 53.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/53.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUYT7JmX9Tj",
        "colab_type": "text"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8tNn6HMYLMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correction_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"correction: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm1Pm2aRZ9Ow",
        "colab_type": "text"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJyRavOpZ7UW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5c5a20d8-869d-44f7-f04d-d07f642e9d93"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"correct\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=corr_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[correction_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # metric_fns=[t5.evaluation.metrics.accuracy]\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe4o_0jFbP-p",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I64TqHGxbOJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1a6fddac-c776-43fb-ef2b-4570764a20f0"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"correct\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'correction: The chart illustrates the amount of illiterate males and females in the world in 2020. Overall, there is a gap that divides countries with high level of literacy and illiterate countries. <br> To begin with, it is clearly seen that there are more illiterate females than males in every area. Though, the difference may be small, for example in Latin America or may be relatively large like in East Asia and the second type is more common. All regions can be divided in two groups. The first one is with high level of illiteracy and it includes South Asia, Arab States and Sub-Saharan Afrrica. Another consist of Developed countries, Latin America/Caribbean and East Asia/Oceania and shows low level of illiterate people (less than 20). <br> To sum up, the few facts should be emphasized. In 2020 there will be two different groups of areas, depending on level of illiteracy. Besides, the amount of illiterate females will be bigger than illiterate males in every area.', 'inputs': array([11698,    10,    37,  5059, 11485,     7,     8,   866,    13,\n",
            "           3,   173,  9842,   342,  5069,     7,    11,  3955,     7,\n",
            "          16,     8,   296,    16,  6503,     5,  9126,     6,   132,\n",
            "          19,     3,     9,  6813,    24, 14514,     7,  1440,    28,\n",
            "         306,   593,    13, 18298,    11,     3,   173,  9842,   342,\n",
            "        1440,     5,     3,     2,   115,    52,  3155,   304,  1731,\n",
            "          28,     6,    34,    19,  3133,   894,    24,   132,    33,\n",
            "          72,     3,   173,  9842,   342,  3955,     7,   145,  5069,\n",
            "           7,    16,   334,   616,     5,  4229,     6,     8,  1750,\n",
            "         164,    36,   422,     6,    21,   677,    16,  6271,  1371,\n",
            "          42,   164,    36,  4352,   508,   114,    16,  1932,  3826,\n",
            "          11,     8,   511,   686,    19,    72,  1017,     5,   432,\n",
            "        6266,    54,    36,  8807,    16,   192,  1637,     5,    37,\n",
            "         166,    80,    19,    28,   306,   593,    13,     3,   173,\n",
            "        9842,  4710,    11,    34,   963,  1013,  3826,     6,  9217,\n",
            "        1323,    11,  3325,    18,   134,     9, 14888,    29,    71,\n",
            "          89,    52,  2234,     9,     5,  2351,  5608,    13,     3,\n",
            "       31192,  1440,     6,  6271,  1371,    87,   254,     9,  6520,\n",
            "         346,   152,    11,  1932,  3826,    87,   667,   565, 11219,\n",
            "          11,  1267,   731,   593,    13,     3,   173,  9842,   342,\n",
            "         151,    41,   924,   145,   460,   137,     3,     2,   115,\n",
            "          52,  3155,   304,  4505,    95,     6,     8,   360,  6688,\n",
            "         225,    36,     3, 25472,     5,    86,  6503,   132,    56,\n",
            "          36,   192,   315,  1637,    13,   844,     6,  3345,    30,\n",
            "         593,    13,     3,   173,  9842,  4710,     5,     3,  8500,\n",
            "           6,     8,   866,    13,     3,   173,  9842,   342,  3955,\n",
            "           7,    56,    36,  4038,   145,     3,   173,  9842,   342,\n",
            "        5069,     7,    16,   334,   616,     5,     1]), 'targets_plaintext': b'The chart illustrates the amount of illiterate males and females in the world in 2020. Overall, there is a gap that divides countries with high level of literacy and countries with lower literacy. <br> To begin with, it is clearly seen that there are more illiterate females than males in every area. However, the difference may be small, as, for example, in Latin America, or relatively large , like in East Asia, and the second type is more common. All regions can be divided into two groups. The first one is with high level of illiteracy , and it includes South Asia, Arab States and Sub-Saharan Afrrica. The other consists of Developed countries, Latin America/Caribbean and East Asia/Oceania and shows low level of illiterate people (less than 20). <br> To sum up, a few facts should be emphasized. In 2020 there will be two different areas, according to level of their literacy. Besides, the number of illiterate females has been and will be bigger than illiterate males in every area.', 'targets': array([   37,  5059, 11485,     7,     8,   866,    13,     3,   173,\n",
            "        9842,   342,  5069,     7,    11,  3955,     7,    16,     8,\n",
            "         296,    16,  6503,     5,  9126,     6,   132,    19,     3,\n",
            "           9,  6813,    24, 14514,     7,  1440,    28,   306,   593,\n",
            "          13, 18298,    11,  1440,    28,  1364, 18298,     5,     3,\n",
            "           2,   115,    52,  3155,   304,  1731,    28,     6,    34,\n",
            "          19,  3133,   894,    24,   132,    33,    72,     3,   173,\n",
            "        9842,   342,  3955,     7,   145,  5069,     7,    16,   334,\n",
            "         616,     5,   611,     6,     8,  1750,   164,    36,   422,\n",
            "           6,    38,     6,    21,   677,     6,    16,  6271,  1371,\n",
            "           6,    42,  4352,   508,     3,     6,   114,    16,  1932,\n",
            "        3826,     6,    11,     8,   511,   686,    19,    72,  1017,\n",
            "           5,   432,  6266,    54,    36,  8807,   139,   192,  1637,\n",
            "           5,    37,   166,    80,    19,    28,   306,   593,    13,\n",
            "           3,   173,  9842,  4710,     3,     6,    11,    34,   963,\n",
            "        1013,  3826,     6,  9217,  1323,    11,  3325,    18,   134,\n",
            "           9, 14888,    29,    71,    89,    52,  2234,     9,     5,\n",
            "          37,   119,     3,  6848,    13,     3, 31192,  1440,     6,\n",
            "        6271,  1371,    87,   254,     9,  6520,   346,   152,    11,\n",
            "        1932,  3826,    87,   667,   565, 11219,    11,  1267,   731,\n",
            "         593,    13,     3,   173,  9842,   342,   151,    41,   924,\n",
            "         145,   460,   137,     3,     2,   115,    52,  3155,   304,\n",
            "        4505,    95,     6,     3,     9,   360,  6688,   225,    36,\n",
            "           3, 25472,     5,    86,  6503,   132,    56,    36,   192,\n",
            "         315,   844,     6,  1315,    12,   593,    13,    70, 18298,\n",
            "           5,     3,  8500,     6,     8,   381,    13,     3,   173,\n",
            "        9842,   342,  3955,     7,    65,   118,    11,    56,    36,\n",
            "        4038,   145,     3,   173,  9842,   342,  5069,     7,    16,\n",
            "         334,   616,     5,     1])}\n",
            "{'inputs_plaintext': b'correction: Next year, I plan to go to UK for spending one year. That is because my husband is going to go to study MBA in UK, so I will bring my baby to go there with him. However, I dont want to just spend time in UK as a housewife, I want to study English, and if possible, i even want to study L.L.M in UK. Within five years, after I had the experience in UK, I will go back to work from the childcare vacation.Although Im still a legal assistant in a big company, I intend to apply a job in a legal office.%%', 'inputs': array([11698,    10,  3021,   215,     6,    27,   515,    12,   281,\n",
            "          12,  1270,    21,  2887,    80,   215,     5,   466,    19,\n",
            "         250,    82,  2553,    19,   352,    12,   281,    12,   810,\n",
            "       15751,    16,  1270,     6,    78,    27,    56,   830,    82,\n",
            "        1871,    12,   281,   132,    28,   376,     5,   611,     6,\n",
            "          27,  2483,   241,    12,   131,  1492,    97,    16,  1270,\n",
            "          38,     3,     9,   629, 22106,     6,    27,   241,    12,\n",
            "         810,  1566,     6,    11,     3,    99,   487,     6,     3,\n",
            "          23,   237,   241,    12,   810,   301,     5,   434,     5,\n",
            "         329,    16,  1270,     5,  8381,   874,   203,     6,   227,\n",
            "          27,   141,     8,   351,    16,  1270,     6,    27,    56,\n",
            "         281,   223,    12,   161,    45,     8, 27862,  4257,     5,\n",
            "         188,    40, 11841,  1318,   341,     3,     9,  1281,  6165,\n",
            "          16,     3,     9,   600,   349,     6,    27,  8286,    12,\n",
            "        1581,     3,     9,   613,    16,     3,     9,  1281,   828,\n",
            "           5,  1454,  1454,     1]), 'targets_plaintext': b'Next year, I plan to go to the UK for a year. That is because my husband is going to study a MBA in the UK, so I will bring my baby as well. However, I dont want to just spend time in the UK as a housewife, I want to study English, and if possible, I even want to study L.L.M in the UK. Within five years, after I have experience in the UK, I will go back to work. Although Im still a legal assistant in a big company, I intend to apply for a job in a legal office.%%', 'targets': array([ 3021,   215,     6,    27,   515,    12,   281,    12,     8,\n",
            "        1270,    21,     3,     9,   215,     5,   466,    19,   250,\n",
            "          82,  2553,    19,   352,    12,   810,     3,     9, 15751,\n",
            "          16,     8,  1270,     6,    78,    27,    56,   830,    82,\n",
            "        1871,    38,   168,     5,   611,     6,    27,  2483,   241,\n",
            "          12,   131,  1492,    97,    16,     8,  1270,    38,     3,\n",
            "           9,   629, 22106,     6,    27,   241,    12,   810,  1566,\n",
            "           6,    11,     3,    99,   487,     6,    27,   237,   241,\n",
            "          12,   810,   301,     5,   434,     5,   329,    16,     8,\n",
            "        1270,     5,  8381,   874,   203,     6,   227,    27,    43,\n",
            "         351,    16,     8,  1270,     6,    27,    56,   281,   223,\n",
            "          12,   161,     5,  1875,  1318,   341,     3,     9,  1281,\n",
            "        6165,    16,     3,     9,   600,   349,     6,    27,  8286,\n",
            "          12,  1581,    21,     3,     9,   613,    16,     3,     9,\n",
            "        1281,   828,     5,  1454,  1454,     1])}\n",
            "{'inputs_plaintext': b'correction: Hello Luis Blanco, <br> I\\xe2\\x80\\x99m writing you about the staff recruitment for the \\xe2\\x80\\x9cNew Cloud Generation\\xe2\\x80\\x9d project. So we need your help with some questions about the process. <br> For your information, we are looking for people with three years of experience in similar projects, who are engineers and also have a high team work skill. We also value that they have specialised studies in Cloud technology, and hosting management. However it\\xe2\\x80\\x99s possible that we don\\xe2\\x80\\x99t find enough people with that profile so, we could accept people with one year of experience. <br> Is it possible to do the meeting this week? When and where do you prefer? (Remember that I work from 8 a.m to 6 p.m) I suggest we can do the meeting at your office, so you don\\xe2\\x80\\x99t have to move to another place. <br> Finally I need you to prepare some profiles that you think the staff of New Cloud Generation should have, and please tell me if you find another different request than I tell you before in that e-mail. <br> Please, answer me with the information as soon as possible. <br> \\xc3\\x8d\\xc3\\xb1igo Ojeda.', 'inputs': array([11698,    10,  8774,  2318,   159, 12824,    32,     6,     3,\n",
            "           2,   115,    52,  3155,    27,    22,    51,   913,    25,\n",
            "          81,     8,   871, 11615,    21,     8,   105,  6861,  5713,\n",
            "       11946,   153,   516,     5,   264,    62,   174,    39,   199,\n",
            "          28,   128,   746,    81,     8,   433,     5,     3,     2,\n",
            "         115,    52,  3155,   242,    39,   251,     6,    62,    33,\n",
            "         479,    21,   151,    28,   386,   203,    13,   351,    16,\n",
            "        1126,  1195,     6,   113,    33,  8702,    11,    92,    43,\n",
            "           3,     9,   306,   372,   161,  4359,     5,   101,    92,\n",
            "         701,    24,    79,    43,     3, 26725,  2116,    16,  5713,\n",
            "         748,     6,    11,  4434,   758,     5,   611,    34,    22,\n",
            "           7,   487,    24,    62,   278,    22,    17,   253,   631,\n",
            "         151,    28,    24,  3278,    78,     6,    62,   228,  1845,\n",
            "         151,    28,    80,   215,    13,   351,     5,     3,     2,\n",
            "         115,    52,  3155,    27,     7,    34,   487,    12,   103,\n",
            "           8,  1338,    48,   471,    58,   366,    11,   213,   103,\n",
            "          25,  2396,    58,    41,  1649, 12066,    24,    27,   161,\n",
            "          45,   505,     3,     9,     5,    51,    12,   431,     3,\n",
            "         102,     5,    51,    61,    27,  3130,    62,    54,   103,\n",
            "           8,  1338,    44,    39,   828,     6,    78,    25,   278,\n",
            "          22,    17,    43,    12,   888,    12,   430,   286,     5,\n",
            "           3,     2,   115,    52,  3155,  4213,    27,   174,    25,\n",
            "          12,  2967,   128, 10958,    24,    25,   317,     8,   871,\n",
            "          13,   368,  5713, 11946,   225,    43,     6,    11,   754,\n",
            "         817,   140,     3,    99,    25,   253,   430,   315,  1690,\n",
            "         145,    27,   817,    25,   274,    16,    24,     3,    15,\n",
            "          18,  1963,     5,     3,     2,   115,    52,  3155,   863,\n",
            "           6,  1525,   140,    28,     8,   251,    38,  1116,    38,\n",
            "         487,     5,     3,     2,   115,    52,  3155,     3,     2,\n",
            "          23,   839,   411,  1924,    26,     9,     5,     1]), 'targets_plaintext': b'Hello Luis Blanco, <br> I\\xe2\\x80\\x99m writing to you about the staff recruitment for the \\xe2\\x80\\x9cNew Cloud Generation\\xe2\\x80\\x9d project. So we need your help with some questions about the process. <br> For your information, we are looking for people with three years experience in similar projects, who are engineers and also have good teamworking skills. We also require that they have specialised studies in Cloud technology, and hosting management. However, it\\xe2\\x80\\x99s possible that we won\\xe2\\x80\\x99t find enough people with that profile, so, we could accept people with one years experience. <br> Is it possible to hold the meeting this week? When and where do you prefer? (Remember that I work from 8 a.m to 6 p.m) I suggest we could hold the meeting at your office, so you don\\xe2\\x80\\x99t have to travel to another place. <br> Finally, I need you to prepare some profiles of the qualities that you think the staff of New Cloud Generation should have, and please tell me if you find another different request than the one I told you before in that e-mail. <br> Please, answer me with the information as soon as possible. <br> \\xc3\\x8d\\xc3\\xb1igo Ojeda.', 'targets': array([ 8774,  2318,   159, 12824,    32,     6,     3,     2,   115,\n",
            "          52,  3155,    27,    22,    51,   913,    12,    25,    81,\n",
            "           8,   871, 11615,    21,     8,   105,  6861,  5713, 11946,\n",
            "         153,   516,     5,   264,    62,   174,    39,   199,    28,\n",
            "         128,   746,    81,     8,   433,     5,     3,     2,   115,\n",
            "          52,  3155,   242,    39,   251,     6,    62,    33,   479,\n",
            "          21,   151,    28,   386,   203,   351,    16,  1126,  1195,\n",
            "           6,   113,    33,  8702,    11,    92,    43,   207,   372,\n",
            "        9238,  1098,     5,   101,    92,  1457,    24,    79,    43,\n",
            "           3, 26725,  2116,    16,  5713,   748,     6,    11,  4434,\n",
            "         758,     5,   611,     6,    34,    22,     7,   487,    24,\n",
            "          62,   751,    22,    17,   253,   631,   151,    28,    24,\n",
            "        3278,     6,    78,     6,    62,   228,  1845,   151,    28,\n",
            "          80,   203,   351,     5,     3,     2,   115,    52,  3155,\n",
            "          27,     7,    34,   487,    12,  1520,     8,  1338,    48,\n",
            "         471,    58,   366,    11,   213,   103,    25,  2396,    58,\n",
            "          41,  1649, 12066,    24,    27,   161,    45,   505,     3,\n",
            "           9,     5,    51,    12,   431,     3,   102,     5,    51,\n",
            "          61,    27,  3130,    62,   228,  1520,     8,  1338,    44,\n",
            "          39,   828,     6,    78,    25,   278,    22,    17,    43,\n",
            "          12,  1111,    12,   430,   286,     5,     3,     2,   115,\n",
            "          52,  3155,  4213,     6,    27,   174,    25,    12,  2967,\n",
            "         128, 10958,    13,     8, 10596,    24,    25,   317,     8,\n",
            "         871,    13,   368,  5713, 11946,   225,    43,     6,    11,\n",
            "         754,   817,   140,     3,    99,    25,   253,   430,   315,\n",
            "        1690,   145,     8,    80,    27,  1219,    25,   274,    16,\n",
            "          24,     3,    15,    18,  1963,     5,     3,     2,   115,\n",
            "          52,  3155,   863,     6,  1525,   140,    28,     8,   251,\n",
            "          38,  1116,    38,   487,     5,     3,     2,   115,    52,\n",
            "        3155,     3,     2,    23,   839,   411,  1924,    26,     9,\n",
            "           5,     1])}\n",
            "{'inputs_plaintext': b'correction: I planning to my party on sunday in june at 11am we are inviting 20 of peoples coming in my party so we need to make a cake and brening the 30 peer and 10 cool drining <br> and also prening cd for music rock or dance and also we need make so chooclates for gairls which her coming our party and we thank all people coming my party', 'inputs': array([11698,    10,    27,  1459,    12,    82,  1088,    30,  1997,\n",
            "        1135,    16,     3,  6959,    15,    44,   850,   265,    62,\n",
            "          33, 14256,   460,    13,   151,     7,  1107,    16,    82,\n",
            "        1088,    78,    62,   174,    12,   143,     3,     9,  4340,\n",
            "          11,  6397,    35,    53,     8,   604, 11409,    11,   335,\n",
            "        1633,     3, 19161,    53,     3,     2,   115,    52,  3155,\n",
            "          11,    92,   554,    29,    53,     3,    75,    26,    21,\n",
            "         723,  2480,    42,  2595,    11,    92,    62,   174,   143,\n",
            "          78,     3,  3995,    32,  4651,  1422,    21,     3,   122,\n",
            "        2256,    40,     7,    84,   160,  1107,    69,  1088,    11,\n",
            "          62,  2763,    66,   151,  1107,    82,  1088,     1]), 'targets_plaintext': b'I am planning my party on Sunday in June at 11am. We are inviting 20 people coming to my party. So we need to make a cake, bring 30 beers and 10 cool drinks. <br> We should prepare a cd for rock or dance music and also we need make chocolates for the girls We thank all the people who are coming to my party.', 'targets': array([   27,   183,  1459,    82,  1088,    30,  1771,    16,  1515,\n",
            "          44,   850,   265,     5,   101,    33, 14256,   460,   151,\n",
            "        1107,    12,    82,  1088,     5,   264,    62,   174,    12,\n",
            "         143,     3,     9,  4340,     6,   830,   604,    36,   277,\n",
            "          11,   335,  1633,  6750,     5,     3,     2,   115,    52,\n",
            "        3155,   101,   225,  2967,     3,     9,     3,    75,    26,\n",
            "          21,  2480,    42,  2595,   723,    11,    92,    62,   174,\n",
            "         143,  3711,     7,    21,     8,  3567,   101,  2763,    66,\n",
            "           8,   151,   113,    33,  1107,    12,    82,  1088,     5,\n",
            "           1])}\n",
            "{'inputs_plaintext': b'correction: My Boos is very friendly, but very serious. He is very short and blacy har. He is always hardworking or studying. He never smiles.', 'inputs': array([11698,    10,   499,  1491,    32,     7,    19,   182,  2609,\n",
            "           6,    68,   182,  2261,     5,   216,    19,   182,   710,\n",
            "          11,     3,   115,    40,  4710,     3,  3272,     5,   216,\n",
            "          19,   373,   614,  9238,    42,  6908,     5,   216,   470,\n",
            "        3993,     7,     5,     1]), 'targets_plaintext': b'My boss is very friendly, but very serious. He is very short and has black hair. He is always hard-working or studying. He never smiles.', 'targets': array([ 499, 7930,   19,  182, 2609,    6,   68,  182, 2261,    5,  216,\n",
            "         19,  182,  710,   11,   65, 1001, 1268,    5,  216,   19,  373,\n",
            "        614,   18, 9238,   42, 6908,    5,  216,  470, 3993,    7,    5,\n",
            "          1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ailCy7DO4--y"
      },
      "source": [
        "### conll\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SKb92NTv4--z",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "conll_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"conll-train.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"conll-eval.tsv\")\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ysZQWUSd4--5"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3pykQy3i4--6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2eb5d42f-e937-4282-fa4c-1a3c3bce5804"
      },
      "source": [
        "def conll_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(conll_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(conll_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'sentence: Keeping the Secret of Genetic Testing parsing: ROOT_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number prep_ADP compound_PROPN_NounType_Number pobj_PROPN_NounType_Number', 'corr_text': b'sentence: Keeping the Secret of Genetic Testing parsing: ROOT_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number prep_ADP compound_PROPN_NounType_Number pobj_PROPN_NounType_Number'}\n",
            "{'orig_text': b'sentence: What is genetic risk ? parsing: attr_NOUN_PronType ROOT_VERB_VerbForm_Tense_Number_Person amod_ADJ_Degree nsubj_NOUN_Number ?', 'corr_text': b'sentence: What is genetic risk ? parsing: attr_NOUN_PronType ROOT_VERB_VerbForm_Tense_Number_Person amod_ADJ_Degree nsubj_NOUN_Number ?'}\n",
            "{'orig_text': b'sentence: Genetic risk refers more to your chance of inheriting a disorder or disease . parsing: compound_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_ADV_Degree prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number .', 'corr_text': b'sentence: Genetic risk refers more to your chance of inheriting a disorder or disease . parsing: compound_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_ADV_Degree prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: People get certain disease because of genetic changes . parsing: nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense amod_ADJ_Degree dobj_NOUN_Number prep_ADP pcomp_ADP amod_ADJ_Degree pobj_NOUN_Number .', 'corr_text': b'sentence: People get certain disease because of genetic changes . parsing: nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense amod_ADJ_Degree dobj_NOUN_Number prep_ADP pcomp_ADP amod_ADJ_Degree pobj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: How much a genetic change tells us about your chance of developing a disorder is not always clear . parsing: advmod_ADV_PronType advmod_ADJ_Degree det_DET amod_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_PRON_PronType prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person neg_ADV_Degree advmod_ADV_Degree acomp_ADJ_Degree .', 'corr_text': b'sentence: How much a genetic change tells us about your chance of developing a disorder is not always clear . parsing: advmod_ADV_PronType advmod_ADJ_Degree det_DET amod_ADJ_Degree nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person dobj_PRON_PronType prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number prep_ADP pcomp_VERB_VerbForm_Tense_Aspect det_DET dobj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person neg_ADV_Degree advmod_ADV_Degree acomp_ADJ_Degree .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tfK0n9EM4-_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "94edcb00-7efa-45ee-feb0-64e25b06cd26"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/conll-train.tsv conll-train.tsv\n",
        "\n",
        "with open(\"conll-train.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  conll_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/conll-train.tsv...\n",
            "/ [1/1 files][ 12.0 MiB/ 12.0 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/12.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdBxpeUg4-_I"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JTPrLkal4-_K",
        "colab": {}
      },
      "source": [
        "def conll_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"conll: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMPR6oam4-_Q"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScBKerTO4-_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "365a3c51-b949-49c7-c669-2a6aefa3f6c0"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"conll\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=conll_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[conll_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rnAPQ5A34-_Y"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FAIeE_fS4-_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a6cb9948-6b2a-4c05-ae11-015f19122ccc"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"conll\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'conll: sentence: And so , he would have chosen not to undergo generic disorder testing and let the truth be mined forever . parsing: cc_CCONJ_ConjType advmod_ADV_Degree , nsubj_PRON_PronType aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect neg_ADV_Degree aux_PART_PartType_VerbForm xcomp_VERB_VerbForm amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType conj_VERB_VerbForm det_DET nsubjpass_NOUN_Number auxpass_VERB_VerbForm ccomp_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree .', 'inputs': array([  975,   195,    10,  7142,    10,   275,    78,     3,     6,\n",
            "           3,    88,   133,    43,  3934,    59,    12, 17601,  8165,\n",
            "        9311,  2505,    11,   752,     8,  2827,    36,  2000,    26,\n",
            "        6276,     3,     5,   260,     7,    53,    10,     3,    75,\n",
            "          75,   834,   254, 17752,   683,   834,  4302,   354, 25160,\n",
            "           3,     9,    26,   208,  7360,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,     3,     6,     3,    29,  7304,   354,\n",
            "         834,   345, 13044,   834,  3174,    29, 25160,   742,   834,\n",
            "       16174,   279,   834,  5000,   115, 25160,   742,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51, 10264,  6951,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   188,  5628, 14261,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,   742,   834, 19846,   834, 13725, 25160,\n",
            "         834,  5000,   115,  3809,    51,     3,   226,  7699,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,     3,     9,\n",
            "        7360,   834,  6762,   683,   834,  2962,  3584,    15, 12771,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   103,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "          75,    75,   834,   254, 17752,   683,   834,  4302,   354,\n",
            "       25160,   975,   354,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,    20,    17,   834,  5596,   382,     3,    29,\n",
            "        7304,   354,  3968,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,   742,  3968,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,    75,  7699,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628,     3,     9,    26,   208,  7360,   834,   188, 13529,\n",
            "         834,  2962,  3584,    15,     3,     5,     1]), 'targets_plaintext': b'sentence: And so , he would have chosen not to undergo generic disorder testing and let the truth be mined forever . parsing: cc_CCONJ_ConjType advmod_ADV_Degree , nsubj_PRON_PronType aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect neg_ADV_Degree aux_PART_PartType_VerbForm xcomp_VERB_VerbForm amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType conj_VERB_VerbForm det_DET nsubjpass_NOUN_Number auxpass_VERB_VerbForm ccomp_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree .', 'targets': array([ 7142,    10,   275,    78,     3,     6,     3,    88,   133,\n",
            "          43,  3934,    59,    12, 17601,  8165,  9311,  2505,    11,\n",
            "         752,     8,  2827,    36,  2000,    26,  6276,     3,     5,\n",
            "         260,     7,    53,    10,     3,    75,    75,   834,   254,\n",
            "       17752,   683,   834,  4302,   354, 25160,     3,     9,    26,\n",
            "         208,  7360,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "           3,     6,     3,    29,  7304,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,   742,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51, 10264,  6951,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628, 14261,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "         742,   834, 19846,   834, 13725, 25160,   834,  5000,   115,\n",
            "        3809,    51,     3,   226,  7699,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,     3,     9,  7360,   834,  6762,\n",
            "         683,   834,  2962,  3584,    15, 12771,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,   103,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    75,    75,   834,\n",
            "         254, 17752,   683,   834,  4302,   354, 25160,   975,   354,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,    20,\n",
            "          17,   834,  5596,   382,     3,    29,  7304,   354,  3968,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   742,  3968,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,     3,\n",
            "          75,  7699,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   188,  5628,     3,     9,\n",
            "          26,   208,  7360,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,     3,     5,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: May be we can trust our immediate family members but can we really trust all the other relatives ? parsing: aux_VERB_VerbType ROOT_VERB_VerbForm nsubj_PRON_PronType aux_VERB_VerbType ccomp_VERB_VerbForm poss_ADJ_PronType_Poss amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType aux_VERB_VerbType nsubj_PRON_PronType advmod_ADV_Degree conj_VERB_VerbForm predet_ADJ_AdjType_PronType det_DET amod_ADJ_Degree dobj_NOUN_Number ?', 'inputs': array([  975,   195,    10,  7142,    10,   932,    36,    62,    54,\n",
            "        2019,    69,  5299,   384,   724,    68,    54,    62,   310,\n",
            "        2019,    66,     8,   119, 12867,     3,    58,   260,     7,\n",
            "          53,    10,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160, 10264,  6951,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,    29,  7304,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,     3,    75,  7699,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,     3,  2748,     7,   834,\n",
            "        6762,   683,   834,  3174,    29, 25160,   834,   345,    32,\n",
            "           7,     7,     3,     9,  7360,   834,  6762,   683,   834,\n",
            "        2962,  3584,    15, 12771,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,   103,   115,   354,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,    75,    75,   834,   254, 17752,\n",
            "         683,   834,  4302,   354, 25160,   742,   834, 16174,   279,\n",
            "         834,  5000,   115, 25160,     3,    29,  7304,   354,   834,\n",
            "         345, 13044,   834,  3174,    29, 25160,     3,     9,    26,\n",
            "         208,  7360,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "         975,   354,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   554,   221,    17,   834,  6762,   683,   834,   188,\n",
            "          26,   354, 25160,   834,  3174,    29, 25160,    20,    17,\n",
            "         834,  5596,   382,     3,     9,  7360,   834,  6762,   683,\n",
            "         834,  2962,  3584,    15,   103,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    58,     1]), 'targets_plaintext': b'sentence: May be we can trust our immediate family members but can we really trust all the other relatives ? parsing: aux_VERB_VerbType ROOT_VERB_VerbForm nsubj_PRON_PronType aux_VERB_VerbType ccomp_VERB_VerbForm poss_ADJ_PronType_Poss amod_ADJ_Degree compound_NOUN_Number dobj_NOUN_Number cc_CCONJ_ConjType aux_VERB_VerbType nsubj_PRON_PronType advmod_ADV_Degree conj_VERB_VerbForm predet_ADJ_AdjType_PronType det_DET amod_ADJ_Degree dobj_NOUN_Number ?', 'targets': array([ 7142,    10,   932,    36,    62,    54,  2019,    69,  5299,\n",
            "         384,   724,    68,    54,    62,   310,  2019,    66,     8,\n",
            "         119, 12867,     3,    58,   260,     7,    53,    10,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160, 10264,  6951,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,     3,\n",
            "          29,  7304,   354,   834,   345, 13044,   834,  3174,    29,\n",
            "       25160,   742,   834, 16174,   279,   834,  5000,   115, 25160,\n",
            "           3,    75,  7699,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,  2748,     7,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,   834,   345,    32,     7,     7,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "       12771,   834,  7400,  7443,   834,   567,  5937,    49,   103,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160,     3,    29,  7304,   354,   834,   345, 13044,   834,\n",
            "        3174,    29, 25160,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,   975,   354,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   554,   221,\n",
            "          17,   834,  6762,   683,   834,   188,    26,   354, 25160,\n",
            "         834,  3174,    29, 25160,    20,    17,   834,  5596,   382,\n",
            "           3,     9,  7360,   834,  6762,   683,   834,  2962,  3584,\n",
            "          15,   103,   115,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,    58,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: I believe that some of the carriers will choose to not reveal their disease to the family members as they do not want to make their family members worry and feel uncomfortable or even feel ostracized by them . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense mark_ADP nsubj_DET prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType ccomp_VERB_VerbForm aux_PART_PartType_VerbForm neg_ADV_Degree xcomp_VERB_VerbForm poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number mark_ADP nsubj_PRON_PronType aux_VERB_VerbForm_Tense neg_ADV_Degree advcl_VERB_VerbForm aux_PART_PartType_VerbForm xcomp_VERB_VerbForm poss_ADJ_PronType_Poss compound_NOUN_Number nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense cc_CCONJ_ConjType conj_VERB_VerbForm_Tense acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_VERB_VerbForm xcomp_VERB_VerbForm_Tense_Aspect agent_ADP pobj_PRON_PronType .', 'inputs': array([  975,   195,    10,  7142,    10,    27,   857,    24,   128,\n",
            "          13,     8, 16642,    56,   854,    12,    59,  6731,    70,\n",
            "        1994,    12,     8,   384,   724,    38,    79,   103,    59,\n",
            "         241,    12,   143,    70,   384,   724,  3516,    11,   473,\n",
            "       14209,    42,   237,   473,     3,    32,     7,  6471,  1601,\n",
            "          57,   135,     3,     5,   260,     7,    53,    10,     3,\n",
            "          29,  7304,   354,   834,   345, 13044,   834,  3174,    29,\n",
            "       25160, 10264,  6951,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,  3946,   834,   188,  7410,\n",
            "           3,    29,  7304,   354,   834,  5596,   382, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,     3,    75,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         742,   834, 19846,   834, 13725, 25160,   834,  5000,   115,\n",
            "        3809,    51, 14261,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,     3,   226,  7699,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,     3,  2748,     7,   834,  6762,   683,\n",
            "         834,  3174,    29, 25160,   834,   345,    32,     7,     7,\n",
            "         103,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,    20,    17,   834,  5596,\n",
            "         382, 12771,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,  3946,   834,   188,  7410,     3,    29,  7304,   354,\n",
            "         834,   345, 13044,   834,  3174,    29, 25160,   742,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167, 14261,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "           3,     9,    26,   208,    75,    40,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   742,   834, 19846,   834,\n",
            "       13725, 25160,   834,  5000,   115,  3809,    51,     3,   226,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "           3,  2748,     7,   834,  6762,   683,   834,  3174,    29,\n",
            "       25160,   834,   345,    32,     7,     7, 12771,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    29,  7304,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,    75,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         834,   382,  5167,     3,    75,    75,   834,   254, 17752,\n",
            "         683,   834,  4302,   354, 25160,   975,   354,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   834,   382,  5167,\n",
            "           3,     9,  7699,   834,  6762,   683,   834,  2962,  3584,\n",
            "          15,     3,    75,    75,   834,   254, 17752,   683,   834,\n",
            "        4302,   354, 25160,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,   975,   354,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,     3,   226,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         834,   382,  5167,   834,   188,  5628,  3102,   834,   188,\n",
            "        7410,  1977,   115,   354,   834,   345, 13044,   834,  3174,\n",
            "          29, 25160,     3,     5,     1]), 'targets_plaintext': b'sentence: I believe that some of the carriers will choose to not reveal their disease to the family members as they do not want to make their family members worry and feel uncomfortable or even feel ostracized by them . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense mark_ADP nsubj_DET prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType ccomp_VERB_VerbForm aux_PART_PartType_VerbForm neg_ADV_Degree xcomp_VERB_VerbForm poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number mark_ADP nsubj_PRON_PronType aux_VERB_VerbForm_Tense neg_ADV_Degree advcl_VERB_VerbForm aux_PART_PartType_VerbForm xcomp_VERB_VerbForm poss_ADJ_PronType_Poss compound_NOUN_Number nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense cc_CCONJ_ConjType conj_VERB_VerbForm_Tense acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_VERB_VerbForm xcomp_VERB_VerbForm_Tense_Aspect agent_ADP pobj_PRON_PronType .', 'targets': array([ 7142,    10,    27,   857,    24,   128,    13,     8, 16642,\n",
            "          56,   854,    12,    59,  6731,    70,  1994,    12,     8,\n",
            "         384,   724,    38,    79,   103,    59,   241,    12,   143,\n",
            "          70,   384,   724,  3516,    11,   473, 14209,    42,   237,\n",
            "         473,     3,    32,     7,  6471,  1601,    57,   135,     3,\n",
            "           5,   260,     7,    53,    10,     3,    29,  7304,   354,\n",
            "         834,   345, 13044,   834,  3174,    29, 25160, 10264,  6951,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,   834,\n",
            "         382,  5167,  3946,   834,   188,  7410,     3,    29,  7304,\n",
            "         354,   834,  5596,   382, 13422,   834,   188,  7410,    20,\n",
            "          17,   834,  5596,   382,  1977,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,   742,   834, 16174,   279,\n",
            "         834,  5000,   115, 25160,     3,    75,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   742,   834, 19846,\n",
            "         834, 13725, 25160,   834,  5000,   115,  3809,    51, 14261,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,     3,   226,\n",
            "        7699,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "           3,  2748,     7,   834,  6762,   683,   834,  3174,    29,\n",
            "       25160,   834,   345,    32,     7,     7,   103,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382, 12771,   834,\n",
            "        7400,  7443,   834,   567,  5937,    49,  1977,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,  3946,   834,\n",
            "         188,  7410,     3,    29,  7304,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,   742,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167, 14261,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,     3,     9,    26,\n",
            "         208,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   742,   834, 19846,   834, 13725, 25160,   834,\n",
            "        5000,   115,  3809,    51,     3,   226,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,     3,  2748,     7,\n",
            "         834,  6762,   683,   834,  3174,    29, 25160,   834,   345,\n",
            "          32,     7,     7, 12771,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,    29,  7304,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,     3,    75,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   834,   382,  5167,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160,   975,   354,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,   834,   382,  5167,     3,     9,  7699,\n",
            "         834,  6762,   683,   834,  2962,  3584,    15,     3,    75,\n",
            "          75,   834,   254, 17752,   683,   834,  4302,   354, 25160,\n",
            "           3,     9,    26,   208,  7360,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,   975,   354,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,     3,   226,  7699,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,   834,   382,  5167,\n",
            "         834,   188,  5628,  3102,   834,   188,  7410,  1977,   115,\n",
            "         354,   834,   345, 13044,   834,  3174,    29, 25160,     3,\n",
            "           5,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: To tell or not to tell parsing: aux_PART_PartType_VerbForm ROOT_VERB_VerbForm cc_CCONJ_ConjType neg_ADV_Degree aux_PART_PartType_VerbForm conj_VERB_VerbForm', 'inputs': array([  975,   195,    10,  7142,    10,   304,   817,    42,    59,\n",
            "          12,   817,   260,     7,    53,    10,   742,   834, 19846,\n",
            "         834, 13725, 25160,   834,  5000,   115,  3809,    51, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160, 14261,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,   742,   834, 19846,   834, 13725, 25160,   834,  5000,\n",
            "         115,  3809,    51,   975,   354,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,     1]), 'targets_plaintext': b'sentence: To tell or not to tell parsing: aux_PART_PartType_VerbForm ROOT_VERB_VerbForm cc_CCONJ_ConjType neg_ADV_Degree aux_PART_PartType_VerbForm conj_VERB_VerbForm', 'targets': array([ 7142,    10,   304,   817,    42,    59,    12,   817,   260,\n",
            "           7,    53,    10,   742,   834, 19846,   834, 13725, 25160,\n",
            "         834,  5000,   115,  3809,    51, 10264,  6951,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,     3,    75,    75,\n",
            "         834,   254, 17752,   683,   834,  4302,   354, 25160, 14261,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,   742,   834,\n",
            "       19846,   834, 13725, 25160,   834,  5000,   115,  3809,    51,\n",
            "         975,   354,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,     1])}\n",
            "{'inputs_plaintext': b'conll: sentence: Social media does have some other advantages beside their general benefits to human being . parsing: amod_ADJ_Degree nsubj_NOUN_Number aux_VERB_VerbForm_Tense_Number_Person ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss amod_ADJ_Degree pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number .', 'inputs': array([  975,   195,    10,  7142,    10,  2730,   783,   405,    43,\n",
            "         128,   119,  7648, 14898,    70,   879,  1393,    12,   936,\n",
            "         271,     3,     5,   260,     7,    53,    10,     3,     9,\n",
            "        7360,   834,  6762,   683,   834,  2962,  3584,    15,     3,\n",
            "          29,  7304,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,   742,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   567,  5937,    49,   834,\n",
            "         345, 13515, 10264,  6951,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,    20,    17,   834,  5596,   382,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "         103,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,     3,  2748,     7,   834,\n",
            "        6762,   683,   834,  3174,    29, 25160,   834,   345,    32,\n",
            "           7,     7,     3,     9,  7360,   834,  6762,   683,   834,\n",
            "        2962,  3584,    15,  1977,   115,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49, 13422,   834,   188,  7410,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,     3,     5,     1]), 'targets_plaintext': b'sentence: Social media does have some other advantages beside their general benefits to human being . parsing: amod_ADJ_Degree nsubj_NOUN_Number aux_VERB_VerbForm_Tense_Number_Person ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss amod_ADJ_Degree pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number .', 'targets': array([ 7142,    10,  2730,   783,   405,    43,   128,   119,  7648,\n",
            "       14898,    70,   879,  1393,    12,   936,   271,     3,     5,\n",
            "         260,     7,    53,    10,     3,     9,  7360,   834,  6762,\n",
            "         683,   834,  2962,  3584,    15,     3,    29,  7304,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   742,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   567,  5937,    49,   834,   345, 13515, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "          20,    17,   834,  5596,   382,     3,     9,  7360,   834,\n",
            "        6762,   683,   834,  2962,  3584,    15,   103,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,     3,  2748,     7,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,   834,   345,    32,     7,     7,     3,\n",
            "           9,  7360,   834,  6762,   683,   834,  2962,  3584,    15,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,     3,     9,  7360,   834,\n",
            "        6762,   683,   834,  2962,  3584,    15,  1977,   115,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,     5,\n",
            "           1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bpJm0H4y5lgz"
      },
      "source": [
        "### jfleg\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hYMaUrqR5lg9",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "jfleg_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"jfleg-train.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"jfleg-eval.tsv\")\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KV2WZkP65lhD"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAwPAnPR5lhE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c467c920-b3b4-4b65-c20b-e88539c72d6d"
      },
      "source": [
        "def jfleg_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(jfleg_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(jfleg_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'New and new technology has been introduced to the society .', 'corr_text': b'New and new technology has been introduced to the society .'}\n",
            "{'orig_text': b'One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .', 'corr_text': b'One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .'}\n",
            "{'orig_text': b'Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .', 'corr_text': b'Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .'}\n",
            "{'orig_text': b'While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .', 'corr_text': b'While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .'}\n",
            "{'orig_text': b'Disadvantage is parking their car is very difficult .', 'corr_text': b'Disadvantage is parking their car is very difficult .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UyO2lRv_5lhY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9fd31e5c-f3ef-4f77-9feb-2f6d6b66defa"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/jfleg-train.tsv jfleg-train.tsv\n",
        "\n",
        "with open(\"jfleg-train.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  jfleg_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/jfleg-train.tsv...\n",
            "/ [1/1 files][570.1 KiB/570.1 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/570.1 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2Yyu1XW5lhd"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKHVHAXA5lhe",
        "colab": {}
      },
      "source": [
        "def jfleg_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"jfleg: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nm7SO2sc5lhg"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sYYoQac55lhh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "06d08264-7c50-44a0-9b90-7e57e888daa4"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"jfleg\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=jfleg_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[jfleg_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dBKTsfxo5lhj"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DU7QaX-W5lhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "76f299aa-ff41-4547-bb8c-807a4874ca1d"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"jfleg\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'jfleg: In her salary we cant buy some car because we are planing to finish our hause in Binangonan , Rizal and we will planing to finish my study in Boston .', 'inputs': array([   3,  354,   89, 5772,   10,   86,  160, 9090,   62,   54,   17,\n",
            "        805,  128,  443,  250,   62,   33,  515,   53,   12, 1992,   69,\n",
            "          3, 2989,   15,   16, 7617, 1468,  106,  152,    3,    6, 2403,\n",
            "        172,  138,   11,   62,   56,  515,   53,   12, 1992,   82,  810,\n",
            "         16, 5053,    3,    5,    1]), 'targets_plaintext': b'In her salary we cant buy some car because we are planing to finish our hause in Binangonan , Rizal and we will planing to finish my study in Boston .', 'targets': array([  86,  160, 9090,   62,   54,   17,  805,  128,  443,  250,   62,\n",
            "         33,  515,   53,   12, 1992,   69,    3, 2989,   15,   16, 7617,\n",
            "       1468,  106,  152,    3,    6, 2403,  172,  138,   11,   62,   56,\n",
            "        515,   53,   12, 1992,   82,  810,   16, 5053,    3,    5,    1])}\n",
            "{'inputs_plaintext': b'jfleg: In this movie father has handicapped .', 'inputs': array([    3,   354,    89,  5772,    10,    86,    48,  1974,  2353,\n",
            "          65, 14214,  3138,     3,     5,     1]), 'targets_plaintext': b'In this movie father has handicapped .', 'targets': array([   86,    48,  1974,  2353,    65, 14214,  3138,     3,     5,\n",
            "           1])}\n",
            "{'inputs_plaintext': b'jfleg: Can you ever visualize what a chaos our society would suffer if every individual considers himself/ herself an expert in every given field ?', 'inputs': array([    3,   354,    89,  5772,    10,  1072,    25,   664, 25086,\n",
            "         125,     3,     9, 16856,    69,  2710,   133,  5696,     3,\n",
            "          99,   334,   928,  1099,     7,  2448,    87,  6257,    46,\n",
            "        2205,    16,   334,   787,  1057,     3,    58,     1]), 'targets_plaintext': b'Can you ever visualize what a chaos our society would suffer if every individual considers himself/ herself an expert in every given field ?', 'targets': array([ 1072,    25,   664, 25086,   125,     3,     9, 16856,    69,\n",
            "        2710,   133,  5696,     3,    99,   334,   928,  1099,     7,\n",
            "        2448,    87,  6257,    46,  2205,    16,   334,   787,  1057,\n",
            "           3,    58,     1])}\n",
            "{'inputs_plaintext': b'jfleg: Many people whaching advertise .', 'inputs': array([    3,   354,    89,  5772,    10,  1404,   151, 14228, 12076,\n",
            "       17123,     3,     5,     1]), 'targets_plaintext': b'Many people whaching advertise .', 'targets': array([ 1404,   151, 14228, 12076, 17123,     3,     5,     1])}\n",
            "{'inputs_plaintext': b'jfleg: it is ture .', 'inputs': array([   3,  354,   89, 5772,   10,   34,   19,    3, 2693,    3,    5,\n",
            "          1]), 'targets_plaintext': b'it is ture .', 'targets': array([  34,   19,    3, 2693,    3,    5,    1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Tx8nxpt52wi"
      },
      "source": [
        "### bea\n",
        "\n",
        "[Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) is a challenging corpus for open-domain QA. Each example includes a orig_text along with an entire Wikipedia article that may or may not contain its corr_text. The goal is to produce the correct corr_text given this context. In our case, we will be ignoring the provided context in hopes that the model will learn to find the corr_texts from the world knowledge it has acquired during pre-training.\n",
        "\n",
        "Since the raw data splits are stored as JSONL files, we will first need to convert them to TSV format to make them parseable in TensorFlow. We will also take the opportunity to drop information we will not be using, remove orig_texts with multiple corr_texts, and to do a bit of cleaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0s8_Ax8q52wm",
        "colab": {}
      },
      "source": [
        "bea_tsv_path = {\n",
        "    \"train\": os.path.join(DATA_DIR, \"bea-train-strict.tsv\"),\n",
        "    \"validation\": os.path.join(DATA_DIR, \"bea-eval.tsv\")\n",
        "}"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0T5Ox2Zw52ws"
      },
      "source": [
        "Next, we define a function to load the TSV data as a `tf.data.Dataset` in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Otom1DRl52wt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9fa71108-db21-4756-ba82-7b95fcc6833b"
      },
      "source": [
        "def bea_dataset_fn(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(bea_tsv_path[split])\n",
        "  # Split each \"<orig_text>\\t<corr_text>\" example into (orig_text, corr_text) tuple.\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  # Map each tuple to a {\"orig_text\": ... \"corr_text\": ...} dict.\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"orig_text\", \"corr_text\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(bea_dataset_fn(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'orig_text': b'sentence: Dear Sir , parsing: amod_ADJ_Degree ROOT_PROPN_NounType_Number ,', 'corr_text': b'sentence: Dear Sir , parsing: amod_ADJ_Degree ROOT_PROPN_NounType_Number ,'}\n",
            "{'orig_text': b'sentence: I have seen your advertisement for a job on the internet and I am writing to apply for a summer job as an instructor and keeper of children in your camp . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType nsubj_PRON_PronType aux_VERB_VerbForm_Tense conj_VERB_VerbForm_Tense_Aspect aux_PART_PartType_VerbForm advcl_VERB_VerbForm prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number .', 'corr_text': b'sentence: I have seen your advertisement for a job on the internet and I am writing to apply for a summer job as an instructor and keeper of children in your camp . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType nsubj_PRON_PronType aux_VERB_VerbForm_Tense conj_VERB_VerbForm_Tense_Aspect aux_PART_PartType_VerbForm advcl_VERB_VerbForm prep_ADP det_DET compound_NOUN_Number pobj_NOUN_Number prep_ADP det_DET pobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP poss_ADJ_PronType_Poss pobj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: I am working as a teacher in Spanish school with children aged between 8 and 14 . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number prep_ADP pobj_NOUN_Number acl_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NUM_NumType cc_CCONJ_ConjType conj_NUM_NumType .', 'corr_text': b'sentence: I am working as a teacher in Spanish school with children aged between 8 and 14 . parsing: nsubj_PRON_PronType aux_VERB_VerbForm_Tense ROOT_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number prep_ADP amod_ADJ_Degree pobj_NOUN_Number prep_ADP pobj_NOUN_Number acl_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NUM_NumType cc_CCONJ_ConjType conj_NUM_NumType .'}\n",
            "{'orig_text': b'sentence: I am an easy going person with a lot of empathy for children . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense det_DET amod_ADJ_Degree compound_VERB_VerbForm_Tense_Aspect attr_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP pobj_NOUN_Number .', 'corr_text': b'sentence: I am an easy going person with a lot of empathy for children . parsing: nsubj_PRON_PronType ROOT_VERB_VerbForm_Tense det_DET amod_ADJ_Degree compound_VERB_VerbForm_Tense_Aspect attr_NOUN_Number prep_ADP det_DET pobj_NOUN_Number prep_ADP pobj_NOUN_Number prep_ADP pobj_NOUN_Number .'}\n",
            "{'orig_text': b'sentence: On the other hand , in my leisure time , I usually do sports like jogging or swimming , adventures activities like trekking , climbing o espeleology too for 20 years ago . parsing: prep_ADP det_DET amod_ADJ_Degree pobj_NOUN_Number , prep_ADP poss_ADJ_PronType_Poss compound_NOUN_Number pobj_NOUN_Number , nsubj_PRON_PronType advmod_ADV_Degree ROOT_VERB_VerbForm_Tense dobj_NOUN_Number prep_ADP nmod_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number , compound_NOUN_Number conj_NOUN_Number intj_ADP advcl_VERB_VerbForm_Tense_Aspect , conj_VERB_VerbForm_Tense_Aspect prep_PART dobj_NOUN_Number advmod_ADV_Degree prep_ADP nummod_NUM_NumType npadvmod_NOUN_Number pcomp_ADV_Degree .', 'corr_text': b'sentence: On the other hand , in my leisure time , I usually do sports like jogging or swimming , adventures activities like trekking , climbing o espeleology too for 20 years ago . parsing: prep_ADP det_DET amod_ADJ_Degree pobj_NOUN_Number , prep_ADP poss_ADJ_PronType_Poss compound_NOUN_Number pobj_NOUN_Number , nsubj_PRON_PronType advmod_ADV_Degree ROOT_VERB_VerbForm_Tense dobj_NOUN_Number prep_ADP nmod_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number , compound_NOUN_Number conj_NOUN_Number intj_ADP advcl_VERB_VerbForm_Tense_Aspect , conj_VERB_VerbForm_Tense_Aspect prep_PART dobj_NOUN_Number advmod_ADV_Degree prep_ADP nummod_NUM_NumType npadvmod_NOUN_Number pcomp_ADV_Degree .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x_lqXxXs52wx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "19750edd-da60-4be5-fa6e-15319eb9410d"
      },
      "source": [
        "bucket_name = 'ml-bucket-isikus'\n",
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/bea-train-strict.tsv bea-train-strict.tsv\n",
        "\n",
        "with open(\"bea-train-strict.tsv\", \"r\", encoding=\"utf-8\") as inf:\n",
        "  bea_len = len([l for l in inf.read().split(\"\\n\") if l])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/bea-train-strict.tsv...\n",
            "/ [1/1 files][ 16.6 MiB/ 16.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/16.6 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HnZW6qsw52w2"
      },
      "source": [
        "Now, we write a preprocess function to convert the examples in the `tf.data.Dataset` into a text-to-text format, with both `inputs` and `targets` fields. The preprocessor also normalizes the text by lowercasing it and removing quotes since the corr_texts are sometimes formatted in odd ways. Finally, we prepend 'trivia orig_text:' to the inputs so that the model knows what task it's trying to solve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7WPn29Vm52w2",
        "colab": {}
      },
      "source": [
        "def bea_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Remove quotes from a TensorFlow string.\"\"\"\n",
        "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"orig_text\": ..., \"corr_text\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bea: \", normalize_text(ex[\"orig_text\"])]),\n",
        "        \"targets\": normalize_text(ex[\"corr_text\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_I9wCk9c52w6"
      },
      "source": [
        "Finally, we put everything together to create a `Task`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySkJr3iB52w7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4510161c-cf4b-4527-95c1-57a45910a0c5"
      },
      "source": [
        "t5.data.TaskRegistry.add(\n",
        "    \"bea\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=bea_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[bea_preprocessor],\n",
        "    # Use the same vocabulary that we used for pre-training.\n",
        "    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy]\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`sentencepiece_model_path` is deprecated and is ignored. Please update your code as this will cause a failure in future versions.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2-KuPfGm52w_"
      },
      "source": [
        "Let's look at a few pre-processed examples from the validation set. Note they contain both the tokenized (integer) and plain-text inputs and targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T_5atvEn52w_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9baec6ec-c5a9-4791-802e-3a1e03fc1baa"
      },
      "source": [
        "corr_task = t5.data.TaskRegistry.get(\"bea\")\n",
        "ds = corr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_plaintext': b'bea: sentence: The answer is pork , lamb and other meats . parsing: det_DET nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person attr_NOUN_Number , conj_NOUN_Number cc_CCONJ_ConjType amod_ADJ_Degree conj_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10,    37,  1525,    19, 13654,\n",
            "           3,     6, 17871,    11,   119,  3604,     7,     3,     5,\n",
            "         260,     7,    53,    10,    20,    17,   834,  5596,   382,\n",
            "           3,    29,  7304,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49, 10264,  6951,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51,   834,   382,  5167,   834,   567,  5937,\n",
            "          49,   834,   345, 13515,    44,    17,    52,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     6,   975,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,    75,\n",
            "          75,   834,   254, 17752,   683,   834,  4302,   354, 25160,\n",
            "           3,     9,  7360,   834,  6762,   683,   834,  2962,  3584,\n",
            "          15,   975,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,     3,     5,     1]), 'targets_plaintext': b'sentence: The answer is pork , lamb and other meats . parsing: det_DET nsubj_NOUN_Number ROOT_VERB_VerbForm_Tense_Number_Person attr_NOUN_Number , conj_NOUN_Number cc_CCONJ_ConjType amod_ADJ_Degree conj_NOUN_Number .', 'targets': array([ 7142,    10,    37,  1525,    19, 13654,     3,     6, 17871,\n",
            "          11,   119,  3604,     7,     3,     5,   260,     7,    53,\n",
            "          10,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "         834,   382,  5167,   834,   567,  5937,    49,   834,   345,\n",
            "       13515,    44,    17,    52,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,     6,   975,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,     3,    75,    75,   834,   254,\n",
            "       17752,   683,   834,  4302,   354, 25160,     3,     9,  7360,\n",
            "         834,  6762,   683,   834,  2962,  3584,    15,   975,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,     3,     5,\n",
            "           1])}\n",
            "{'inputs_plaintext': b'bea: sentence: After that the poles will have melt totally because the temperature of the sun will be become crazy and very hard so some people will have died for diseases , for example cancer . parsing: mark_ADP mark_ADP det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree mark_ADP det_DET nsubjpass_NOUN_Number prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm advcl_VERB_VerbForm_Tense_Aspect acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_ADV_Degree advmod_ADV_Degree det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm conj_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NOUN_Number , prep_ADP pobj_NOUN_Number appos_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10,   621,    24,     8, 11148,\n",
            "           7,    56,    43, 13297,  3536,   250,     8,  2912,    13,\n",
            "           8,  1997,    56,    36,   582,  6139,    11,   182,   614,\n",
            "          78,   128,   151,    56,    43,  3977,    21,  6716,     3,\n",
            "           6,    21,   677,  1874,     3,     5,   260,     7,    53,\n",
            "          10,  3946,   834,   188,  7410,  3946,   834,   188,  7410,\n",
            "          20,    17,   834,  5596,   382,     3,    29,  7304,   354,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49,   742,   834,\n",
            "       16174,   279,   834,  5000,   115, 25160,   742,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51, 10264,  6951,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   188,  5628,     3,     9,    26,   208,  7360,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,  3946,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,     3,    29,\n",
            "        7304,   354,  3968,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49, 13422,   834,   188,  7410,    20,    17,   834,  5596,\n",
            "         382,  1977,   115,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160,   742,  3968,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,     3,     9,    26,   208,    75,    40,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   188,  5628,     3,     9,  7699,   834,  6762,\n",
            "         683,   834,  2962,  3584,    15,     3,    75,    75,   834,\n",
            "         254, 17752,   683,   834,  4302,   354, 25160,     3,     9,\n",
            "          26,   208,  7360,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,   975,   354,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,     3,     9,    26,   208,  7360,   834,   188, 13529,\n",
            "         834,  2962,  3584,    15,    20,    17,   834,  5596,   382,\n",
            "           3,    29,  7304,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,   742,   834, 16174,   279,   834,  5000,   115,\n",
            "       25160,   742,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   975,   354,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628, 13422,\n",
            "         834,   188,  7410,  1977,   115,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,     3,     6, 13422,   834,   188,\n",
            "        7410,  1977,   115,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,  1120,    32,     7,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,     5,     1]), 'targets_plaintext': b'sentence: After that the poles will have melt totally because the temperature of the sun will be become crazy and very hard so some people will have died for diseases , for example cancer . parsing: mark_ADP mark_ADP det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect advmod_ADV_Degree mark_ADP det_DET nsubjpass_NOUN_Number prep_ADP det_DET pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm advcl_VERB_VerbForm_Tense_Aspect acomp_ADJ_Degree cc_CCONJ_ConjType advmod_ADV_Degree conj_ADV_Degree advmod_ADV_Degree det_DET nsubj_NOUN_Number aux_VERB_VerbType aux_VERB_VerbForm conj_VERB_VerbForm_Tense_Aspect prep_ADP pobj_NOUN_Number , prep_ADP pobj_NOUN_Number appos_NOUN_Number .', 'targets': array([ 7142,    10,   621,    24,     8, 11148,     7,    56,    43,\n",
            "       13297,  3536,   250,     8,  2912,    13,     8,  1997,    56,\n",
            "          36,   582,  6139,    11,   182,   614,    78,   128,   151,\n",
            "          56,    43,  3977,    21,  6716,     3,     6,    21,   677,\n",
            "        1874,     3,     5,   260,     7,    53,    10,  3946,   834,\n",
            "         188,  7410,  3946,   834,   188,  7410,    20,    17,   834,\n",
            "        5596,   382,     3,    29,  7304,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,   742,   834, 16174,   279,   834,  5000,\n",
            "         115,  3809,    51, 10264,  6951,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628,     3,     9,    26,   208,  7360,   834,   188, 13529,\n",
            "         834,  2962,  3584,    15,  3946,   834,   188,  7410,    20,\n",
            "          17,   834,  5596,   382,     3,    29,  7304,   354,  3968,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,   742,  3968,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,     3,\n",
            "           9,    26,   208,    75,    40,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   188,\n",
            "        5628,     3,     9,  7699,   834,  6762,   683,   834,  2962,\n",
            "        3584,    15,     3,    75,    75,   834,   254, 17752,   683,\n",
            "         834,  4302,   354, 25160,     3,     9,    26,   208,  7360,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,   975,   354,\n",
            "         834,   188, 13529,   834,  2962,  3584,    15,     3,     9,\n",
            "          26,   208,  7360,   834,   188, 13529,   834,  2962,  3584,\n",
            "          15,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,   742,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   975,   354,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,   834,\n",
            "         382,  5167,   834,   188,  5628, 13422,   834,   188,  7410,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,     3,     6, 13422,   834,   188,  7410,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,  1120,\n",
            "          32,     7,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,     5,     1])}\n",
            "{'inputs_plaintext': b'bea: \"sentence: A president of one country can be seen and heard in a country far away giving his views and ideas so that all can \"\" understand \"\" him . parsing: det_DET nsubjpass_NOUN_Number prep_ADP nummod_NUM_NumType pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect cc_CCONJ_ConjType conj_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number advmod_ADV_Degree advmod_ADV_Degree advcl_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number mark_ADP mark_ADP nsubj_DET aux_VERB_VerbType \"\" advcl_VERB_VerbForm \"\" dobj_PRON_PronType .\"', 'inputs': array([   36,     9,    10,    96,  5277,  1433,    10,    71,  2753,\n",
            "          13,    80,   684,    54,    36,   894,    11,  1943,    16,\n",
            "           3,     9,   684,   623,   550,  1517,   112,  2441,    11,\n",
            "         912,    78,    24,    66,    54,    96,   121,   734,    96,\n",
            "         121,   376,     3,     5,   260,     7,    53,    10,    20,\n",
            "          17,   834,  5596,   382,     3,    29,  7304,   354,  3968,\n",
            "         834,  7400,  7443,   834,   567,  5937,    49, 13422,   834,\n",
            "         188,  7410,     3,  5525,  7360,   834,   567,  6122,   834,\n",
            "         567,   440, 25160,  1977,   115,   354,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49,   742,   834, 16174,   279,   834,\n",
            "        5000,   115, 25160,   742,  3968,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51, 10264,  6951,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   834,   382,  5167,   834,\n",
            "         188,  5628,     3,    75,    75,   834,   254, 17752,   683,\n",
            "         834,  4302,   354, 25160,   975,   354,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   834,   382,  5167,   834,\n",
            "         188,  5628, 13422,   834,   188,  7410,    20,    17,   834,\n",
            "        5596,   382,  1977,   115,   354,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,     3,     9,    26,\n",
            "         208,  7360,   834,   188, 13529,   834,  2962,  3584,    15,\n",
            "           3,     9,    26,   208,    75,    40,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,   834,   382,  5167,   834,\n",
            "         188,  5628,     3,  2748,     7,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,   834,   345,    32,     7,     7,   103,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,    75,    75,   834,   254, 17752,   683,   834,  4302,\n",
            "         354, 25160,   975,   354,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,  3946,   834,   188,  7410,  3946,   834,   188,\n",
            "        7410,     3,    29,  7304,   354,   834,  5596,   382,   742,\n",
            "         834, 16174,   279,   834,  5000,   115, 25160,    96,   121,\n",
            "           3,     9,    26,   208,    75,    40,   834, 16174,   279,\n",
            "         834,  5000,   115,  3809,    51,    96,   121,   103,   115,\n",
            "         354,   834,   345, 13044,   834,  3174,    29, 25160,     3,\n",
            "         535,     1]), 'targets_plaintext': b'\"sentence: A president of one country can be seen and heard in a country far away giving his views and ideas so that all can \"\" understand \"\" him . parsing: det_DET nsubjpass_NOUN_Number prep_ADP nummod_NUM_NumType pobj_NOUN_Number aux_VERB_VerbType auxpass_VERB_VerbForm ROOT_VERB_VerbForm_Tense_Aspect cc_CCONJ_ConjType conj_VERB_VerbForm_Tense_Aspect prep_ADP det_DET pobj_NOUN_Number advmod_ADV_Degree advmod_ADV_Degree advcl_VERB_VerbForm_Tense_Aspect poss_ADJ_PronType_Poss dobj_NOUN_Number cc_CCONJ_ConjType conj_NOUN_Number mark_ADP mark_ADP nsubj_DET aux_VERB_VerbType \"\" advcl_VERB_VerbForm \"\" dobj_PRON_PronType .\"', 'targets': array([   96,  5277,  1433,    10,    71,  2753,    13,    80,   684,\n",
            "          54,    36,   894,    11,  1943,    16,     3,     9,   684,\n",
            "         623,   550,  1517,   112,  2441,    11,   912,    78,    24,\n",
            "          66,    54,    96,   121,   734,    96,   121,   376,     3,\n",
            "           5,   260,     7,    53,    10,    20,    17,   834,  5596,\n",
            "         382,     3,    29,  7304,   354,  3968,   834,  7400,  7443,\n",
            "         834,   567,  5937,    49, 13422,   834,   188,  7410,     3,\n",
            "        5525,  7360,   834,   567,  6122,   834,   567,   440, 25160,\n",
            "        1977,   115,   354,   834,  7400,  7443,   834,   567,  5937,\n",
            "          49,   742,   834, 16174,   279,   834,  5000,   115, 25160,\n",
            "         742,  3968,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51, 10264,  6951,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628,     3,\n",
            "          75,    75,   834,   254, 17752,   683,   834,  4302,   354,\n",
            "       25160,   975,   354,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628, 13422,\n",
            "         834,   188,  7410,    20,    17,   834,  5596,   382,  1977,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,     9,    26,   208,  7360,   834,   188, 13529,   834,\n",
            "        2962,  3584,    15,     3,     9,    26,   208,  7360,   834,\n",
            "         188, 13529,   834,  2962,  3584,    15,     3,     9,    26,\n",
            "         208,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,   834,   188,  5628,     3,\n",
            "        2748,     7,   834,  6762,   683,   834,  3174,    29, 25160,\n",
            "         834,   345,    32,     7,     7,   103,   115,   354,   834,\n",
            "        7400,  7443,   834,   567,  5937,    49,     3,    75,    75,\n",
            "         834,   254, 17752,   683,   834,  4302,   354, 25160,   975,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,  3946,\n",
            "         834,   188,  7410,  3946,   834,   188,  7410,     3,    29,\n",
            "        7304,   354,   834,  5596,   382,   742,   834, 16174,   279,\n",
            "         834,  5000,   115, 25160,    96,   121,     3,     9,    26,\n",
            "         208,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,    96,   121,   103,   115,   354,   834,   345,\n",
            "       13044,   834,  3174,    29, 25160,     3,   535,     1])}\n",
            "{'inputs_plaintext': b'bea: sentence: Believe that the potential lies deep within you and that the sky is the limit . parsing: ROOT_VERB_VerbForm mark_ADP det_DET nsubj_ADJ_Degree ccomp_VERB_VerbForm_Tense_Number_Person advmod_ADV_Degree prep_ADP pobj_PRON_PronType cc_CCONJ_ConjType mark_ADP det_DET nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person det_DET attr_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10, 22070,    24,     8,  1055,\n",
            "        7797,  1659,   441,    25,    11,    24,     8,  5796,    19,\n",
            "           8,  2006,     3,     5,   260,     7,    53,    10, 10264,\n",
            "        6951,   834, 16174,   279,   834,  5000,   115,  3809,    51,\n",
            "        3946,   834,   188,  7410,    20,    17,   834,  5596,   382,\n",
            "           3,    29,  7304,   354,   834,  6762,   683,   834,  2962,\n",
            "        3584,    15,     3,    75,  7699,   834, 16174,   279,   834,\n",
            "        5000,   115,  3809,    51,   834,   382,  5167,   834,   567,\n",
            "        5937,    49,   834,   345, 13515,     3,     9,    26,   208,\n",
            "        7360,   834,   188, 13529,   834,  2962,  3584,    15, 13422,\n",
            "         834,   188,  7410,  1977,   115,   354,   834,   345, 13044,\n",
            "         834,  3174,    29, 25160,     3,    75,    75,   834,   254,\n",
            "       17752,   683,   834,  4302,   354, 25160,  3946,   834,   188,\n",
            "        7410,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "          75,  7699,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   567,  5937,    49,   834,\n",
            "         345, 13515,    20,    17,   834,  5596,   382,    44,    17,\n",
            "          52,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "           5,     1]), 'targets_plaintext': b'sentence: Believe that the potential lies deep within you and that the sky is the limit . parsing: ROOT_VERB_VerbForm mark_ADP det_DET nsubj_ADJ_Degree ccomp_VERB_VerbForm_Tense_Number_Person advmod_ADV_Degree prep_ADP pobj_PRON_PronType cc_CCONJ_ConjType mark_ADP det_DET nsubj_NOUN_Number ccomp_VERB_VerbForm_Tense_Number_Person det_DET attr_NOUN_Number .', 'targets': array([ 7142,    10, 22070,    24,     8,  1055,  7797,  1659,   441,\n",
            "          25,    11,    24,     8,  5796,    19,     8,  2006,     3,\n",
            "           5,   260,     7,    53,    10, 10264,  6951,   834, 16174,\n",
            "         279,   834,  5000,   115,  3809,    51,  3946,   834,   188,\n",
            "        7410,    20,    17,   834,  5596,   382,     3,    29,  7304,\n",
            "         354,   834,  6762,   683,   834,  2962,  3584,    15,     3,\n",
            "          75,  7699,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,   834,   382,  5167,   834,   567,  5937,    49,   834,\n",
            "         345, 13515,     3,     9,    26,   208,  7360,   834,   188,\n",
            "       13529,   834,  2962,  3584,    15, 13422,   834,   188,  7410,\n",
            "        1977,   115,   354,   834,   345, 13044,   834,  3174,    29,\n",
            "       25160,     3,    75,    75,   834,   254, 17752,   683,   834,\n",
            "        4302,   354, 25160,  3946,   834,   188,  7410,    20,    17,\n",
            "         834,  5596,   382,     3,    29,  7304,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,    75,  7699,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,   834,   382,\n",
            "        5167,   834,   567,  5937,    49,   834,   345, 13515,    20,\n",
            "          17,   834,  5596,   382,    44,    17,    52,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     5,     1])}\n",
            "{'inputs_plaintext': b'bea: sentence: Consider a general election , in which three parties stand , X , Y , and Z , in a country of 5000 voters . parsing: ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number , prep_ADP pobj_ADJ_PronType nummod_NUM_NumType nsubj_NOUN_Number relcl_VERB_VerbForm_Tense , appos_PROPN_NounType_Number , conj_PROPN_NounType_Number , cc_CCONJ_ConjType appos_NOUN_Number , prep_ADP det_DET pobj_NOUN_Number prep_ADP compound_NUM_NumType pobj_NOUN_Number .', 'inputs': array([   36,     9,    10,  7142,    10,  9151,     3,     9,   879,\n",
            "        4356,     3,     6,    16,    84,   386,  2251,  1518,     3,\n",
            "           6,     3,     4,     3,     6,     3,   476,     3,     6,\n",
            "          11,  1027,     3,     6,    16,     3,     9,   684,    13,\n",
            "           3, 12814, 10861,     3,     5,   260,     7,    53,    10,\n",
            "       10264,  6951,   834, 16174,   279,   834,  5000,   115,  3809,\n",
            "          51,    20,    17,   834,  5596,   382,     3,     9,  7360,\n",
            "         834,  6762,   683,   834,  2962,  3584,    15,   103,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49,     3,\n",
            "           6, 13422,   834,   188,  7410,  1977,   115,   354,   834,\n",
            "        6762,   683,   834,  3174,    29, 25160,     3,  5525,  7360,\n",
            "         834,   567,  6122,   834,   567,   440, 25160,     3,    29,\n",
            "        7304,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "        8318,    75,    40,   834, 16174,   279,   834,  5000,   115,\n",
            "        3809,    51,   834,   382,  5167,     3,     6,  1120,    32,\n",
            "           7,   834, 17618, 15420,   834,  4168,   202, 25160,   834,\n",
            "         567,  5937,    49,     3,     6,   975,   354,   834, 17618,\n",
            "       15420,   834,  4168,   202, 25160,   834,   567,  5937,    49,\n",
            "           3,     6,     3,    75,    75,   834,   254, 17752,   683,\n",
            "         834,  4302,   354, 25160,  1120,    32,     7,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     6, 13422,   834,\n",
            "         188,  7410,    20,    17,   834,  5596,   382,  1977,   115,\n",
            "         354,   834,  7400,  7443,   834,   567,  5937,    49, 13422,\n",
            "         834,   188,  7410, 12771,   834,   567,  6122,   834,   567,\n",
            "         440, 25160,  1977,   115,   354,   834,  7400,  7443,   834,\n",
            "         567,  5937,    49,     3,     5,     1]), 'targets_plaintext': b'sentence: Consider a general election , in which three parties stand , X , Y , and Z , in a country of 5000 voters . parsing: ROOT_VERB_VerbForm det_DET amod_ADJ_Degree dobj_NOUN_Number , prep_ADP pobj_ADJ_PronType nummod_NUM_NumType nsubj_NOUN_Number relcl_VERB_VerbForm_Tense , appos_PROPN_NounType_Number , conj_PROPN_NounType_Number , cc_CCONJ_ConjType appos_NOUN_Number , prep_ADP det_DET pobj_NOUN_Number prep_ADP compound_NUM_NumType pobj_NOUN_Number .', 'targets': array([ 7142,    10,  9151,     3,     9,   879,  4356,     3,     6,\n",
            "          16,    84,   386,  2251,  1518,     3,     6,     3,     4,\n",
            "           3,     6,     3,   476,     3,     6,    11,  1027,     3,\n",
            "           6,    16,     3,     9,   684,    13,     3, 12814, 10861,\n",
            "           3,     5,   260,     7,    53,    10, 10264,  6951,   834,\n",
            "       16174,   279,   834,  5000,   115,  3809,    51,    20,    17,\n",
            "         834,  5596,   382,     3,     9,  7360,   834,  6762,   683,\n",
            "         834,  2962,  3584,    15,   103,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49,     3,     6, 13422,   834,\n",
            "         188,  7410,  1977,   115,   354,   834,  6762,   683,   834,\n",
            "        3174,    29, 25160,     3,  5525,  7360,   834,   567,  6122,\n",
            "         834,   567,   440, 25160,     3,    29,  7304,   354,   834,\n",
            "        7400,  7443,   834,   567,  5937,    49,  8318,    75,    40,\n",
            "         834, 16174,   279,   834,  5000,   115,  3809,    51,   834,\n",
            "         382,  5167,     3,     6,  1120,    32,     7,   834, 17618,\n",
            "       15420,   834,  4168,   202, 25160,   834,   567,  5937,    49,\n",
            "           3,     6,   975,   354,   834, 17618, 15420,   834,  4168,\n",
            "         202, 25160,   834,   567,  5937,    49,     3,     6,     3,\n",
            "          75,    75,   834,   254, 17752,   683,   834,  4302,   354,\n",
            "       25160,  1120,    32,     7,   834,  7400,  7443,   834,   567,\n",
            "        5937,    49,     3,     6, 13422,   834,   188,  7410,    20,\n",
            "          17,   834,  5596,   382,  1977,   115,   354,   834,  7400,\n",
            "        7443,   834,   567,  5937,    49, 13422,   834,   188,  7410,\n",
            "       12771,   834,   567,  6122,   834,   567,   440, 25160,  1977,\n",
            "         115,   354,   834,  7400,  7443,   834,   567,  5937,    49,\n",
            "           3,     5,     1])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlghm_3rAd-M",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Mixture\n",
        "\n",
        "We now create a `Mixture` from the above `Tasks`, which we will fine-tune on.\n",
        "\n",
        "There are different ways to automatically set the rate (for example, based on the number of examples using `rate_num_examples`), but we will just hardcode an equal mixture for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWe_XOtY9VPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "083cf460-482d-4149-8143-60b3808edbe9"
      },
      "source": [
        "print(correct_len)\n",
        "print(conll_len)\n",
        "print(jfleg_len)\n",
        "print(bea_len)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32668\n",
            "28346\n",
            "3016\n",
            "34304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfmiY5cT9XWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tasks_and_weights = [\n",
        "  ('correct', float(correct_len)),\n",
        "  ('conll', float(conll_len)),\n",
        "  ('jfleg', float(jfleg_len)),\n",
        "  ('bea', float(bea_len))\n",
        "]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgs-s3eDAU37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"correctit_all\")\n",
        "t5.data.MixtureRegistry.add(\"correctit_all\", tasks_and_weights)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUkorodCENGw",
        "colab_type": "text"
      },
      "source": [
        "# Transferring to new Tasks\n",
        "\n",
        "We are now ready to fine-tune one of the pre-trained T5 models on our new mixture of closed-book QA tasks.\n",
        "\n",
        "First, we'll instantiate a `Model` object using the model size of your choice. Note that larger models are slower to train and use but will likely achieve higher accuracy. You also may be able to increase accuracy by training longer with more `FINETUNE_STEPS` below.\n",
        "\n",
        "\n",
        "## Caveats\n",
        "\n",
        "* Due to its memory requirements, you will not be able to train the `11B` parameter model on the TPU provided by Colab. Instead, you will need to fine-tune inside of a GCP instance (see [README](https://github.com/google-research/text-to-text-transfer-transformer/)).\n",
        "* Due to the checkpoint size, you will not be able use the 5GB GCS free tier for the `3B` parameter models. You will need at least 25GB of space, which you can purchase with your $300 of initial credit on GCP.\n",
        "* While `large` can achieve decent results, it is recommended that you fine-tune at least the `3B` parameter model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syte5n0nnMOC",
        "colab_type": "text"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pesK1uu6QAqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run = \"basedrei\"  # @param {\"type\": \"string\"}"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "yGQ-zpgy3raf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_SIZE = \"base\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n",
        "# Public GCS path for T5 pre-trained model checkpoints\n",
        "BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "if run not in [None, \"\"]:\n",
        "    MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE+\"-\"+run)\n",
        "else:\n",
        "    MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n",
        "\n",
        "if ON_CLOUD and MODEL_SIZE == \"3B\":\n",
        "  tf.logging.warn(\n",
        "      \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n",
        "      \"Make sure you have at least 25GB on GCS before continuing.\"\n",
        "  )\n",
        "elif ON_CLOUD and MODEL_SIZE == \"11B\":\n",
        "  raise ValueError(\n",
        "      \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n",
        "      \"provided by Colab. Please comment out this Error if you're running \"\n",
        "      \"on a larger TPU.\"\n",
        "  )\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 128, 16),\n",
        "    \"base\": (2, 64, 8),\n",
        "    \"large\": (8, 32, 4),\n",
        "    \"3B\": (8, 8, 1),\n",
        "    \"11B\": (8, 8, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    learning_rate_schedule=0.0025,\n",
        "    save_checkpoints_steps=600,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZhAd0U_4B_o",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tune\n",
        "\n",
        "We are now ready to fine-tune our model. This will take a while (~2 hours with default settings), so please be patient! The larger the model and more `FINETUNE_STEPS` you use, the longer it will take.\n",
        "\n",
        "Don't worry, you can always come back later and increase the number of steps, and it will automatically pick up where you left off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7t7a25LBTj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5269b4fb-5d90-44d9-b4e3-b4308c80ad55"
      },
      "source": [
        "FINETUNE_STEPS = 3900\n",
        "print(\"Finetuning for\", FINETUNE_STEPS, \"steps\")\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"correctit_all\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finetuning for 3900 steps\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://ml-bucket-isikus/t5-base-model/models/base-basedrei', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.42.24.138:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.42.24.138:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.42.24.138:8470', '_evaluation_master': 'grpc://10.42.24.138:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc709ddb860>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.42.24.138:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.42.24.138:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -2693549616424924369)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -6814107139779829524)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -7472621283288117479)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -1863837777876742348)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2916098661972990486)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -2689094701565191669)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -5458833458910966820)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2329314034622360886)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7132857999896116674)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 2675992341010654233)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5334792912721686931)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[4, 2] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[2] physical_shape=[1, 1, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1)]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=4, model=2] LayoutRules{('ensemble', 'ensemble'), ('batch', 'batch'), ('vocab', 'model'), ('d_ff', 'model'), ('experts', 'batch'), ('heads', 'model')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fc709578898>\n",
            "INFO:tensorflow:serialize_num_microbatches: tokens_per_microbatch_per_replica=8192 batch_dim=Dimension(name='batch', size=64) sequence_length={'inputs': 512, 'targets': 512} batch_per_replica=16 num_microbatches=1\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 24674304     slice_size 12337152     Shape[vocab=32128, d_model=768]                             \n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 768          slice_size 384          Shape[stacked=2, heads=12, buckets=32]                      \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 47616        slice_size 47616        Shape[stacked=62, d_model=768]                              \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_006/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_006/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_007/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_007/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_008/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_008/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_009/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_009/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_010/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_010/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_011/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_011/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/final_layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_006/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_006/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_006/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_007/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_007/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_007/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_008/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_008/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_008/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_009/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_009/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_009/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_010/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_010/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_010/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_011/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_011/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_011/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/final_layer_norm/scale\n",
            "INFO:tensorflow:Trainable Variables            count: 195     Total size: 222903552        Total slice_size: 111475584      \n",
            "INFO:tensorflow:All Variables                  count: 203     Total size: 223390336        Total slice_size: 111816896      \n",
            "INFO:tensorflow:Counters:\n",
            "allreduce: 7.44e+09\n",
            " allreduce/[0]: 1.19e+09\n",
            "  allreduce/[0]/einsum_op: 8.92e+08\n",
            "  allreduce/[0]/reduce_op: 3.02e+08\n",
            " allreduce/[1]: 6.24e+09\n",
            "  allreduce/[1]/einsum_op: 6.24e+09\n",
            "  allreduce/[1]/reduce_op: 1.45e+06\n",
            "einsum: 2.97e+13\n",
            "einsum_unique: 2.96e+13\n",
            "output: 2.52e+11\n",
            " output/AddOperation: 5.05e+10\n",
            " output/BinaryOpWithBroadcasting: 3.75e+09\n",
            " output/BroadcastOperation: 1.25e+10\n",
            " output/Constant: 8\n",
            " output/EinsumOperation: 1.17e+11\n",
            " output/ImportOperation: 1.58e+06\n",
            " output/MinMaxOperation: 7.55e+07\n",
            " output/OneHotOperation: 4.77e+09\n",
            " output/RandomOperation: 2.52e+07\n",
            " output/RangeOperation: 8.19e+03\n",
            " output/ReduceOperation: 6.41e+09\n",
            " output/ReshapeOperation: 1.16e+10\n",
            " output/ScalarAddOperation: 9.97e+08\n",
            " output/ScalarMultiplyOperation: 4.68e+09\n",
            " output/ShiftOperation: 6.55e+04\n",
            " output/SlicewiseOperation: 3.09e+10\n",
            " output/StackOperation: 2.99e+06\n",
            " output/StackedVariable: 2.99e+06\n",
            " output/StopGradient: 8.3e+09\n",
            " output/UnstackOperation: 2.99e+06\n",
            " output/Variable: 8.92e+08\n",
            "output_unique: 2.05e+11\n",
            " output_unique/AddOperation: 4.36e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 3.06e+09\n",
            " output_unique/BroadcastOperation: 1.09e+10\n",
            " output_unique/Constant: 1\n",
            " output_unique/EinsumOperation: 9.25e+10\n",
            " output_unique/ImportOperation: 1.97e+05\n",
            " output_unique/MinMaxOperation: 9.44e+06\n",
            " output_unique/OneHotOperation: 3.36e+09\n",
            " output_unique/RandomOperation: 2.2e+07\n",
            " output_unique/RangeOperation: 1.02e+03\n",
            " output_unique/ReduceOperation: 5.57e+09\n",
            " output_unique/ReshapeOperation: 1.03e+10\n",
            " output_unique/ScalarAddOperation: 2.38e+08\n",
            " output_unique/ScalarMultiplyOperation: 2.27e+09\n",
            " output_unique/ShiftOperation: 3.28e+04\n",
            " output_unique/SlicewiseOperation: 2.44e+10\n",
            " output_unique/StackOperation: 5.03e+05\n",
            " output_unique/StackedVariable: 5.03e+05\n",
            " output_unique/StopGradient: 8.3e+09\n",
            " output_unique/UnstackOperation: 5.03e+05\n",
            " output_unique/Variable: 2.23e+08\n",
            "variables: 2.23e+08\n",
            " variables/trainable: 2.23e+08\n",
            " variables/untrainable: 4.87e+05\n",
            "INFO:tensorflow:Initializing variables from gs://t5-data/pretrained_models/base/model.ckpt-999900:\n",
            "INFO:tensorflow:Variables in gs://t5-data/pretrained_models/base/model.ckpt-999900 but not in graph:\n",
            "INFO:tensorflow:\n",
            "INFO:tensorflow:Variables in graph but not in gs://t5-data/pretrained_models/base/model.ckpt-999900:\n",
            "INFO:tensorflow:\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Starting the session.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:767: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Initialized dataset iterators in 1 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Before copy master to slices.\n",
            "INFO:tensorflow:Done with copy master to slices.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 999900...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 999900 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 999900...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:loss = 0.014282227, step = 1000000\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (1, 9)\n",
            "INFO:tensorflow:loss = 0.01184082, step = 1000100 (54.112 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.84813\n",
            "INFO:tensorflow:examples/sec: 118.28\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (2, 35)\n",
            "INFO:tensorflow:loss = 0.017333984, step = 1000200 (47.760 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09377\n",
            "INFO:tensorflow:examples/sec: 134.001\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (3, 61)\n",
            "INFO:tensorflow:loss = 0.0138549805, step = 1000300 (49.142 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03485\n",
            "INFO:tensorflow:examples/sec: 130.231\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (4, 84)\n",
            "INFO:tensorflow:loss = 0.015014648, step = 1000400 (47.755 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09408\n",
            "INFO:tensorflow:examples/sec: 134.021\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.014831543, step = 1000500 (47.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09401\n",
            "INFO:tensorflow:examples/sec: 134.017\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (6, 10)\n",
            "INFO:tensorflow:loss = 0.010986328, step = 1000600 (49.507 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.01995\n",
            "INFO:tensorflow:examples/sec: 129.277\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000600...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1000600 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000600...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (7, 0)\n",
            "INFO:tensorflow:loss = 0.00982666, step = 1000700 (73.613 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.35844\n",
            "INFO:tensorflow:examples/sec: 86.9404\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (8, 26)\n",
            "INFO:tensorflow:loss = 0.0107421875, step = 1000800 (47.750 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.0943\n",
            "INFO:tensorflow:examples/sec: 134.035\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (9, 49)\n",
            "INFO:tensorflow:loss = 0.008422852, step = 1000900 (49.454 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.02208\n",
            "INFO:tensorflow:examples/sec: 129.413\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (10, 75)\n",
            "INFO:tensorflow:loss = 0.013305664, step = 1001000 (47.748 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09426\n",
            "INFO:tensorflow:examples/sec: 134.033\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.010314941, step = 1001100 (47.749 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09435\n",
            "INFO:tensorflow:examples/sec: 134.039\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (12, 0)\n",
            "INFO:tensorflow:loss = 0.01184082, step = 1001200 (49.122 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03577\n",
            "INFO:tensorflow:examples/sec: 130.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (13, 26)\n",
            "INFO:tensorflow:loss = 0.011962891, step = 1001300 (47.751 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09412\n",
            "INFO:tensorflow:examples/sec: 134.024\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1001300...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1001300 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1001300...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (14, 0)\n",
            "INFO:tensorflow:loss = 0.008605957, step = 1001400 (73.859 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.35396\n",
            "INFO:tensorflow:examples/sec: 86.6531\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (15, 26)\n",
            "INFO:tensorflow:loss = 0.012634277, step = 1001500 (47.749 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09423\n",
            "INFO:tensorflow:examples/sec: 134.031\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (16, 52)\n",
            "INFO:tensorflow:loss = 0.010620117, step = 1001600 (47.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09402\n",
            "INFO:tensorflow:examples/sec: 134.017\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (17, 75)\n",
            "INFO:tensorflow:loss = 0.010437012, step = 1001700 (49.094 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03693\n",
            "INFO:tensorflow:examples/sec: 130.364\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.011108398, step = 1001800 (47.755 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09406\n",
            "INFO:tensorflow:examples/sec: 134.02\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (19, 1)\n",
            "INFO:tensorflow:loss = 0.011657715, step = 1001900 (47.750 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09426\n",
            "INFO:tensorflow:examples/sec: 134.033\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (20, 24)\n",
            "INFO:tensorflow:loss = 0.008117676, step = 1002000 (49.109 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03621\n",
            "INFO:tensorflow:examples/sec: 130.317\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1002000...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1002000 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1002000...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (21, 0)\n",
            "INFO:tensorflow:loss = 0.010559082, step = 1002100 (73.403 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.36234\n",
            "INFO:tensorflow:examples/sec: 87.1896\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (22, 23)\n",
            "INFO:tensorflow:loss = 0.0072631836, step = 1002200 (47.751 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09425\n",
            "INFO:tensorflow:examples/sec: 134.032\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (23, 49)\n",
            "INFO:tensorflow:loss = 0.008972168, step = 1002300 (47.748 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09425\n",
            "INFO:tensorflow:examples/sec: 134.032\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (24, 75)\n",
            "INFO:tensorflow:loss = 0.0078125, step = 1002400 (49.227 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03141\n",
            "INFO:tensorflow:examples/sec: 130.01\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (25, 98)\n",
            "INFO:tensorflow:loss = 0.0115356445, step = 1002500 (47.752 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.0942\n",
            "INFO:tensorflow:examples/sec: 134.029\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0115356445, step = 1002600 (47.749 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09422\n",
            "INFO:tensorflow:examples/sec: 134.03\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (27, 24)\n",
            "INFO:tensorflow:loss = 0.007385254, step = 1002700 (49.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03598\n",
            "INFO:tensorflow:examples/sec: 130.303\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1002700...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1002700 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1002700...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (28, 0)\n",
            "INFO:tensorflow:loss = 0.009887695, step = 1002800 (72.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.3744\n",
            "INFO:tensorflow:examples/sec: 87.9619\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (29, 26)\n",
            "INFO:tensorflow:loss = 0.0099487305, step = 1002900 (47.752 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09417\n",
            "INFO:tensorflow:examples/sec: 134.027\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (30, 49)\n",
            "INFO:tensorflow:loss = 0.010864258, step = 1003000 (49.445 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.02226\n",
            "INFO:tensorflow:examples/sec: 129.425\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (31, 75)\n",
            "INFO:tensorflow:loss = 0.008666992, step = 1003100 (47.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09404\n",
            "INFO:tensorflow:examples/sec: 134.019\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.008728027, step = 1003200 (47.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09394\n",
            "INFO:tensorflow:examples/sec: 134.012\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (33, 0)\n",
            "INFO:tensorflow:loss = 0.007873535, step = 1003300 (49.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03471\n",
            "INFO:tensorflow:examples/sec: 130.221\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (34, 26)\n",
            "INFO:tensorflow:loss = 0.00793457, step = 1003400 (47.750 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09428\n",
            "INFO:tensorflow:examples/sec: 134.034\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1003400...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1003400 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1003400...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (35, 0)\n",
            "INFO:tensorflow:loss = 0.009765625, step = 1003500 (73.984 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.35164\n",
            "INFO:tensorflow:examples/sec: 86.5052\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (36, 26)\n",
            "INFO:tensorflow:loss = 0.008178711, step = 1003600 (47.749 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09419\n",
            "INFO:tensorflow:examples/sec: 134.028\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (37, 52)\n",
            "INFO:tensorflow:loss = 0.008850098, step = 1003700 (47.751 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.09429\n",
            "INFO:tensorflow:examples/sec: 134.034\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (38, 75)\n",
            "INFO:tensorflow:loss = 0.008728027, step = 1003800 (49.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.03289\n",
            "INFO:tensorflow:examples/sec: 130.105\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1003800...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1003800 into gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1003800...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Done with the session.\n",
            "INFO:tensorflow:Loss for final step: 0.008728027.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYeciUZ_D7T2",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "We now evaluate on the validation sets of the tasks in our mixture. Accuracy results will be logged and added to the TensorBoard above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDGYBcDKSSfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mosestokenizer import *"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYpQCcENUN9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\", disable=[\"tagger\", \"parser\", 'ner', 'textcat', 'lemmatizer'])\n",
        "tokenify = lambda snt: \" \".join(str(x) for x in nlp(snt))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0FBl-hNT0y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from contextlib import contextmanager\n",
        "import logging\n",
        "\n",
        "@contextmanager\n",
        "def all_logging_disabled(highest_level=logging.CRITICAL):\n",
        "    \"\"\"\n",
        "    A context manager that will prevent any logging messages\n",
        "    triggered during the body from being processed.\n",
        "    :param highest_level: the maximum logging level in use.\n",
        "      This would only need to be changed if a custom level greater than CRITICAL\n",
        "      is defined.\n",
        "    \"\"\"\n",
        "    # two kind-of hacks here:\n",
        "    #    * can't get the highest logging level in effect => delegate to the user\n",
        "    #    * can't get the current module-level override => use an undocumented\n",
        "    #       (but non-private!) interface\n",
        "\n",
        "    previous_level = logging.root.manager.disable\n",
        "\n",
        "    logging.disable(highest_level)\n",
        "\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        logging.disable(previous_level)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sY09GFvCRI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVDH7iZBuVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def heal(insent, raw=True):\n",
        "  insent = insent.replace(chr(8263) + \" \", \"<\")\n",
        "  if not raw:\n",
        "    tlist = tokenify(insent).split(\" \")\n",
        "    with MosesDetokenizer('en') as detokenize:\n",
        "      ss = detokenize(tlist)\n",
        "    # outsent = \" \".join(s.capitalize() for s in sent_tokenize(ss, \"english\"))\n",
        "    outsent = outsent.replace(\" - \", \"-\").replace(\" 've\", \"\")\n",
        "    # outsent = re.sub(r'(.*? < br > )(.)(.*?)', lambda m: r'{}'.format(m.group(1)+m.group(2).upper()+m.group(3)), outsent)\n",
        "  outsent = outsent.replace(\" <br> \", \"\\n\").replace(\" < br > \", \"\\n\")\n",
        "  return outsent"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RgVhOk0VzX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from math import ceil\n",
        "\n",
        "from nltk.translate.gleu_score import corpus_gleu\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw_Pj_Mre5LM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9924c46b-dc56-4b80-af77-bfa04a00b285"
      },
      "source": [
        "!gsutil -m cp -r gs://{bucket_name}/t5-base-model/data/correct-target.tsv correct-target.tsv"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/data/correct-target.tsv...\n",
            "/ [1/1 files][ 13.4 MiB/ 13.4 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/13.4 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqkvMe8A1Hks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import signal\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class TimeoutException(Exception): pass\n",
        "\n",
        "@contextmanager\n",
        "def time_limit(seconds):\n",
        "    def signal_handler(signum, frame):\n",
        "        raise TimeoutException(\"Timed out!\")\n",
        "    signal.signal(signal.SIGALRM, signal_handler)\n",
        "    signal.alarm(seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.alarm(0)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "w9Ny7g-m2Umh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a9bea21-cf18-41bf-b81e-8a2c0f84bda2"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR\n",
        "\n",
        "ckpts = !gsutil ls $MODEL_DIR\n",
        "ckptset = set(int(re.search(r\"ckpt-([0-9]+?)\\.data\", s).group(1)) for s in ckpts\n",
        "              if re.search(r\"ckpt-([0-9]+?)\\.data\", s))\n",
        "ckptlist = sorted(list(ckptset))\n",
        "\n",
        "tasks = [\"correct\", \"jfleg\", \"conll\", \"bea\"]\n",
        "test = pd.read_csv(\"correct-target.tsv\", sep=\"\\t\", header=None)\n",
        "test.columns = [\"sources\", \"targets\"]\n",
        "\n",
        "\n",
        "for epoch in ckptlist:\n",
        "  print(\"**** Evaluating checkpoint %s ****\" % str(epoch))\n",
        "\n",
        "  with all_logging_disabled():\n",
        "    model.batch_size = train_batch_size * 4\n",
        "    model.eval(\n",
        "        mixture_or_task_name=\"correctit_all\",\n",
        "        checkpoint_steps=[epoch]\n",
        "    )\n",
        "\n",
        "    for task in tasks:\n",
        "      !gsutil -m cp -r $MODEL_DIR/validation_eval/{task}_{str(epoch)}_predictions {task}_{str(epoch)}.bin\n",
        "      with open(task + \"_\" + str(epoch) + \".bin\", \"rb\") as inf:\n",
        "        exec(task + ' = inf.read().decode().split(\"\\\\n\")[:-1]')\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on raw outputs **\")\n",
        "\n",
        "  refs = list(test[\"targets\"])\n",
        "  tknzd = [tokenify(sent) for sent in refs]\n",
        "\n",
        "  cor_df = pd.DataFrame({\n",
        "      \"predictions\": correct,\n",
        "      \"references\": refs,\n",
        "      \"tokenized\": tknzd\n",
        "  })\n",
        "\n",
        "  picklename = \"corr_test_tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch) + \".pickle\"\n",
        "  cor_df.to_pickle(picklename)\n",
        "\n",
        "  !cp ./{picklename} /content/gdrive/My\\ Drive/heptabot/output\n",
        "\n",
        "  t_preds = [tokenify(sent).split(\" \") for sent in correct]\n",
        "  t_refs = [ref.split(\" \") for ref in tknzd]\n",
        "  gleu = corpus_gleu([[ref] for ref in t_refs], t_preds)\n",
        "  print(\"GLEU:\", gleu)\n",
        "\n",
        "  try:\n",
        "    with time_limit(300):\n",
        "      rouge_scores = rouge.get_scores(correct, refs, avg=True, ignore_empty=False)\n",
        "      print(\"ROUGE-L:\", rouge_scores[\"rouge-l\"])\n",
        "  except TimeoutException as e:\n",
        "    print(\"ROUGE function timed out\")\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on BEA-2019 **\")\n",
        "  outstr = \"\"\n",
        "  for line in bea:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"ABCN.bea19.test.corr\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "\n",
        "  !zip bea-test-{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.zip ABCN.bea19.test.corr\n",
        "  !cp ./bea-test-{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.zip /content/gdrive/My\\ Drive/heptabot/output\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on JFLEG **\")\n",
        "  %cd jfleg\n",
        "  outstr = \"\"\n",
        "\n",
        "  for line in jfleg:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"test.nospc.res\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "  \n",
        "  !cp ./test.nospc.res /content/gdrive/My\\ Drive/heptabot/output/test.nospc.{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.res\n",
        "\n",
        "  !python ./eval/gleu.py -r ./test/test.ref[0-3] -s ./test/test.src --hyp test.nospc.res\n",
        "\n",
        "  %cd ../\n",
        "  print(\"\")\n",
        "\n",
        "  print(\"** Testing on CoNLL-2014 **\")\n",
        "  outstr = \"\"\n",
        "\n",
        "  for line in conll:\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    outstr += tokenify(line) + \"\\n\"\n",
        "\n",
        "  with open(\"conll14_nospc.txt\", \"w\", encoding=\"utf-8\") as outtest:\n",
        "    outtest.write(outstr)\n",
        "  !cp ./conll14_nospc.txt /content/gdrive/My\\ Drive/heptabot/output/conll14_nospc_{\"tf_\" + MODEL_SIZE.lower() + \"_e\" + str(epoch)}.txt\n",
        "\n",
        "  try:\n",
        "    with time_limit(300):\n",
        "      !python2 ./m2scorer/scripts/m2scorer.py ./conll14_nospc.txt ./conll14st-test-data/noalt/official-2014.combined.m2\n",
        "  except TimeoutException as e:\n",
        "    print(\"M2 scorer timed out\")\n",
        "  print(\"\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Evaluating checkpoint 1000600 ****\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/correct_1000600_predictions...\n",
            "/ [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/jfleg_1000600_predictions...\n",
            "/ [1/1 files][ 71.4 KiB/ 71.4 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.4 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/conll_1000600_predictions...\n",
            "/ [1/1 files][158.2 KiB/158.2 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.2 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/bea_1000600_predictions...\n",
            "/ [1/1 files][429.6 KiB/429.6 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/429.6 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7917859573573766\n",
            "ROUGE-L: {'f': 0.9000734671003349, 'p': 0.9026437159332756, 'r': 0.8986302172412965}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.571117', '0.008000', '(0.555,0.587)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6258\n",
            "Recall      : 0.4243\n",
            "F_0.5       : 0.5716\n",
            "\n",
            "**** Evaluating checkpoint 1001300 ****\n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/correct_1001300_predictions...\n",
            "/ [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/jfleg_1001300_predictions...\n",
            "/ [1/1 files][ 71.5 KiB/ 71.5 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.5 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/conll_1001300_predictions...\n",
            "/ [1/1 files][158.1 KiB/158.1 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.1 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/bea_1001300_predictions...\n",
            "/ [1/1 files][429.9 KiB/429.9 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/429.9 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7926109068392986\n",
            "ROUGE-L: {'f': 0.9008293452609956, 'p': 0.9031024138040903, 'r': 0.8996775871592763}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.581644', '0.008050', '(0.566,0.597)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6319\n",
            "Recall      : 0.4240\n",
            "F_0.5       : 0.5755\n",
            "\n",
            "**** Evaluating checkpoint 1002000 ****\n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/correct_1002000_predictions...\n",
            "/ [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/jfleg_1002000_predictions...\n",
            "/ [1/1 files][ 71.4 KiB/ 71.4 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.4 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/conll_1002000_predictions...\n",
            "/ [1/1 files][158.1 KiB/158.1 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.1 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/bea_1002000_predictions...\n",
            "/ [1/1 files][429.7 KiB/429.7 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/429.7 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7922459705090961\n",
            "ROUGE-L: {'f': 0.9010096293379309, 'p': 0.9032728784670588, 'r': 0.8998244350688939}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.591409', '0.008173', '(0.575,0.607)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6287\n",
            "Recall      : 0.4359\n",
            "F_0.5       : 0.5776\n",
            "\n",
            "**** Evaluating checkpoint 1002700 ****\n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/correct_1002700_predictions...\n",
            "/ [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/jfleg_1002700_predictions...\n",
            "/ [1/1 files][ 71.6 KiB/ 71.6 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.6 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/conll_1002700_predictions...\n",
            "/ [1/1 files][158.2 KiB/158.2 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.2 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/bea_1002700_predictions...\n",
            "/ [1/1 files][430.2 KiB/430.2 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/430.2 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7937129617516151\n",
            "ROUGE-L: {'f': 0.9018689536197306, 'p': 0.9038126621963456, 'r': 0.9009994601838136}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.589831', '0.008018', '(0.574,0.606)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6322\n",
            "Recall      : 0.4414\n",
            "F_0.5       : 0.5819\n",
            "\n",
            "**** Evaluating checkpoint 1003400 ****\n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/correct_1003400_predictions...\n",
            "- [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/jfleg_1003400_predictions...\n",
            "/ [1/1 files][ 71.6 KiB/ 71.6 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.6 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/conll_1003400_predictions...\n",
            "/ [1/1 files][158.2 KiB/158.2 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.2 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/bea_1003400_predictions...\n",
            "/ [1/1 files][430.3 KiB/430.3 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/430.3 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7912817355124646\n",
            "ROUGE-L: {'f': 0.901171803546003, 'p': 0.9032699161718608, 'r': 0.9001739154131412}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.592420', '0.008055', '(0.577,0.608)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6260\n",
            "Recall      : 0.4458\n",
            "F_0.5       : 0.5792\n",
            "\n",
            "**** Evaluating checkpoint 1003800 ****\n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/correct_1003800_predictions...\n",
            "- [1/1 files][  6.6 MiB/  6.6 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/6.6 MiB.                                      \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/jfleg_1003800_predictions...\n",
            "/ [1/1 files][ 71.7 KiB/ 71.7 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/71.7 KiB.                                     \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/conll_1003800_predictions...\n",
            "/ [1/1 files][158.4 KiB/158.4 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/158.4 KiB.                                    \n",
            "Copying gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/validation_eval/bea_1003800_predictions...\n",
            "/ [1/1 files][430.2 KiB/430.2 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/430.2 KiB.                                    \n",
            "\n",
            "** Testing on raw outputs **\n",
            "GLEU: 0.7906375387466401\n",
            "ROUGE-L: {'f': 0.9010127793283744, 'p': 0.9029298168322105, 'r': 0.9001882896345804}\n",
            "\n",
            "** Testing on BEA-2019 **\n",
            "  adding: ABCN.bea19.test.corr (deflated 64%)\n",
            "\n",
            "** Testing on JFLEG **\n",
            "/content/jfleg\n",
            "Running GLEU...\n",
            "test.nospc.res\n",
            "[['0.601516', '0.007855', '(0.586,0.617)']]\n",
            "/content\n",
            "\n",
            "** Testing on CoNLL-2014 **\n",
            "Precision   : 0.6284\n",
            "Recall      : 0.4636\n",
            "F_0.5       : 0.5867\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92dClA1SWwIx",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a few random predictions from the validation sets. Note that we measure accuracy based on an *exact match* of the predicted corr_text and the ground-truth corr_text. As a result, some of the corr_texts are semantically correct but are counted wrong by the exact match score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FuqHRuvxOct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "227847f7-a2f2-46d3-92eb-bcd04f6e3d01"
      },
      "source": [
        "import random\n",
        "\n",
        "def print_random_predictions(task_name, n=10):\n",
        "  \"\"\"Print n predictions from the validation split of a task.\"\"\"\n",
        "  # Grab the dataset for this task.\n",
        "  ds = t5.data.TaskRegistry.get(task_name).get_dataset(\n",
        "      split=\"validation\",\n",
        "      sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "      shuffle=False)\n",
        "\n",
        "  def _prediction_file_to_ckpt(path):\n",
        "    \"\"\"Extract the global step from a prediction filename.\"\"\"\n",
        "    return int(path.split(\"_\")[-2])\n",
        "\n",
        "  # Grab the paths of all logged predictions.\n",
        "  prediction_files = tf.io.gfile.glob(\n",
        "      os.path.join(\n",
        "          MODEL_DIR,\n",
        "          \"validation_eval/%s_*_predictions\" % task_name))\n",
        "  # Get most recent prediction file by sorting by their step.\n",
        "  latest_prediction_file = sorted(\n",
        "      prediction_files, key=_prediction_file_to_ckpt)[-1]\n",
        "\n",
        "  # Collect (inputs, targets, prediction) from the dataset and predictions file\n",
        "  results = []\n",
        "  with tf.io.gfile.GFile(latest_prediction_file) as preds:\n",
        "    for ex, pred in zip(tfds.as_numpy(ds), preds):\n",
        "      results.append((tf.compat.as_text(ex[\"inputs_plaintext\"]),\n",
        "                      tf.compat.as_text(ex[\"targets_plaintext\"]),\n",
        "                      pred.strip()))\n",
        "\n",
        "  print(\"<== Random predictions for %s using checkpoint %s ==>\\n\" %\n",
        "        (task_name, \n",
        "         _prediction_file_to_ckpt(latest_prediction_file)))\n",
        "\n",
        "  for inp, tgt, pred in random.choices(results, k=10):\n",
        "    print(\"Input:\", inp)\n",
        "    print(\"Target:\", tgt)\n",
        "    print(\"Prediction:\", pred)\n",
        "    print(\"Counted as Correct?\", tgt == pred)\n",
        "    print()\n",
        "\n",
        "print_random_predictions(\"correct\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<== Random predictions for correct using checkpoint 1003800 ==>\n",
            "\n",
            "Input: correction: Nowadays the problem of great import amounts has become very urgent. There is a tendency to suppose that a country should be able to provide as many food supplies as they need and reduce the import to the minimum. However, each country needs import in large amounts for their people to survive, because of various ecological reasons that can occure to every country. <br> Firstly, a country can suffer from natural desasters. Floods, earthquakes, hurricanes can destroy entire cities, as well as fields with crops. These can't be foreseeen or prevented, so because of that the nation might suffer from famine as the part of food has been destroyed. Due to the import, the vital part of food could be bought from the other countries. <br> Secondly, food supplies can suffer from insects which can also destroy the crops. Using various chemicals can make the situation even worse and damage the crops as well. Italian crops suffering from giant apple snails, Chineese rice suffering from insects caused lots of problems to the government of their countries and without import the concequences of these situations could be even more devastating. <br> One might say that the government can produce special places to keep extra food for such times, which can be used in case of famine. However, even this amount of food might not be enough or it can simply run out of time, because it is impossible to store food forever. <br> All in all, I agree with the idea that import is essential for any country to provide their people everything they need and help them in case they have suffered from numerous ecological problems.\n",
            "Target: Nowadays the problem of great import amounts has become very urgent. There is a tendency to suppose that a country should be able to provide as many food supplies as they need and reduce the import to the minimum. However, each country needs import in large amounts for their people to survive, because of various ecological reasons that can occure to every country. <br> Firstly, a country can suffer from natural desasters. Floods, earthquakes, hurricanes can destroy entire cities, as well as fields with crops. These can't be foreseeen or prevented, so because of that the nation might suffer from famine as the part of food has been destroyed. Due to the import, the vital part of food could be bought from the other countries. <br> Secondly, food supplies can suffer from insects which can also destroy the crops. Using various chemicals can make the situation even worse and damage the crops as well. Italian crops suffering from giant apple snails, Chineese rice suffering from insects caused lots of problems to the government of their countries and without import the concequences of these situations could be even more devastating. <br> One might say that the government can produce special places to keep extra food for such times, which can be used in case of famine. However, even this amount of food might not be enough or it can simply run out of time, because it is impossible to store food forever. <br> All in all, I agree with the idea that import is essential for any country to provide their people everything they need and help them in case they have suffered from numerous ecological problems.\n",
            "Prediction: Nowadays the problem of large import amounts has become very urgent. There is a tendency to suppose that a country should be able to provide as many food supplies as they need and reduce the import to the minimum. However, each country needs import in large amounts for their people to survive, because of various ecological reasons that can occur in every country.  ⁇ br> Firstly, a country can suffer from natural disasters. Floods, earthquakes, hurricanes can destroy entire cities, as well as fields with crops. These can't be foreseeen or prevented, so because of that the nation might suffer from famine as the part of food has been destroyed. Due to the import, the vital part of food could be bought from the other countries.  ⁇ br> Secondly, food supplies can suffer from insects which can also destroy the crops. Using various chemicals can make the situation even worse and damage the crops as well. Italian crops suffering from giant apple snails, Chinese rice suffering from insects caused lots of problems to the government of their countries and without import the consequences of these situations could be even more devastating.  ⁇ br> One might say that the government can produce special places to keep extra food for such times, which can be used in case of famine. However, even this amount of food might not be enough or it can simply run out of time, because it is impossible to store food forever.  ⁇ br> All in all, I agree with the idea that import is essential for any country to provide their people with everything they need and help them in case they have suffered from numerous ecological problems.\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: These four pie charts represent data of age structure of Italy and Yemen populations in 2000 and predictions for 2050. By the beginning of this millenium these countries had relatively different population age structures. An amount of 60+ year old residents was eight times smaller in Yemen rather than in Italy. Unlike Yemen, Italy did not have people younger 14 years as one half of it’s population, their amount was only at the level of about 14%. The difference in 15% may be noted in the number of people aged 15 to 59, where Italy leads. Predictions say, that, by the year 2050, Yemen will go through a slight fluctuations in number of people in generations. Part of young-adults and middle aged residents will increase to 57% along with a growth of an elderly people. In Italy, however, part of residents aged from 15 to 59 will decrease to the level of 46%, giving place for elderly people, whose number will grow by approximately 20%. Overall, we can say that, according to the predictions, there will be more older people in Italy, than in 2000, and the same tendency is seen in Yemen.\n",
            "Target: These four pie charts represent the' of 'the' structure of Italy and Yemen populations in 2000 and predictions for 2050. By the beginning of this millenium these countries had relatively different population age structures. An amount of 60+ year old residents was eight times smaller in Yemen rather than in Italy. Unlike Yemen, Italy did not have people 'younger than' 14 years as one half of it's population, their amount was only at the level of about 14%. The difference in 15% may be noted in the number of people aged 15 to 59, where Italy leads. 'the' say, that, by the year 2050, Yemen will go through slight fluctuations in 'the' of people in generations. Part of young-adults and middle aged residents will increase to 57% along with a growth of elderly people. In Italy, however, 'the' of 'the aged from 15 to 59 will decrease to the level of 46%, giving place for elderly people, whose number will grow by approximately 20%. Overall, we can say that, according to the predictions, there will be more older people in Italy, than in 2000, and the same tendency is seen in Yemen.\n",
            "Prediction: These four pie charts represent data of age structure of Italy and Yemen populations in 2000 and predictions for 2050. By the beginning of this millennium these countries had relatively different population age structures. An amount of 60+ year old residents was eight times smaller in Yemen rather than in Italy. Unlike Yemen, Italy did not have people younger 14 years as one half of it’s population, their amount was only at the level of about 14%. The difference in 15% may be noted in the number of people aged 15 to 59, where Italy leads. Predictions say, that, by the year 2050, Yemen will go through a slight fluctuations in number of people in generations. Part of young-adults and middle aged residents will increase to 57% along with a growth of an elderly people. In Italy, however, part of residents aged from 15 to 59 will decrease to the level of 46%, giving place for elderly people, whose number will grow by approximately 20%. Overall, we can say that, according to the predictions, there will be more older people in Italy, than in 2000, and the same tendency is seen in Yemen.\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: \"The rise of the amount of crimes being committed by young people is a complex problem which is caused by multiple factors. In the last decades children had less time with their families. Instead their timeschedule has changed from the traditional approach, which had the prevailing role of the family and community, to the modern one, which radically reduces their influence with their moral values and gives the main part of teaching the youngsters to kindergartens, scools, colleges and universities in less extent. This tendention can be described as sistematization of the educational process which results in less attention for kids individuality, his or her concerns and interests and a strict demand for accomplishing several points, like this exam. Children feel less obliged to follow norms in the society as long as they deal with these points. Also it makes them less relaxed and more agressive what brings us to high crime rates. The second reason is constant urbanization in the would which is characterized by increasing part of people who live in cities. They face mostly people whom they do not know at all and have no feelings for them. That is why crimes become more probable and we register them more frequently. Young people do not feel ashamed for anybody as they see them for the first and the last time in their opinion. <br> Parents and teachers should make kids responsible for their acts. For example show them how they should get ridd of litter by their own examples. That is important because people tend to leave litter in the places which are already full with it. That corresponds with the \"\"theory of broken windows\"\". Raise them with the idea of equality of people.\"\n",
            "Target: \"The increase in the amount of crimes committed by young people is a complex problem which is caused by multiple factors. In the last decades children have been spending less time with their families. Instead, their time schedule has changed from the traditional one, which gave the prevailing role to family and community, to the modern one, which radically reduces their influence on the childrens moral values and gives the main part of teaching the young to kindergartens, school, colleges and universities , to a lesser extent. This tendency can be described as systematization of the educational process , which results in less attention for a childs individuality as well as his or her concerns and interests , and a strict demand for accomplishing various goals, like this exam. Children feel less obliged to follow norms of the society as long as they deal with these points. Also it makes them less relaxed and more agressive , which leads to high crime rates. The second reason is the constant urbanization which is characterized by an increasing part of people who live in cities. They mostly face people whom they do not know at all and have no feelings for them. That is why crimes become more probable and we register them more frequently. Young people do not feel ashamed of anybody , as they see them for the first and the last time , in their opinion. <br> Parents and teachers should make children feel responsible for their acts. For example, they could show the children how they should get rid of litter , serving as examples. That is important because people tend to leave litter in places which are already full of it. That corresponds with the \"\"theory of broken windows\"\". You should raise your children with the idea of equality of people.\"\n",
            "Prediction: \"The rise of the amount of crimes being committed by young people is a complex problem which is caused by multiple factors. In the last decades children have had less time with their families. Instead their schedule has changed from the traditional approach, which had the prevailing role of the family and community, to the modern one, which radically reduces their influence on their moral values and gives the main part of teaching the youngsters to kindergartens, schools, colleges and universities to a lesser extent. This tendency can be described as systematization of the educational process which results in less attention to kids individuality, their concerns and interests and a strict demand for accomplishing several points, like this exam. Children feel less obliged to follow norms in society as long as they deal with these points. Also it makes them less relaxed and more aggressive, which brings us to high crime rates. The second reason is constant urbanization in the world, which is characterized by an increasing proportion of people who live in cities. They mostly face people whom they do not know at all and who have no feelings for. That is why crimes become more probable and we register them more frequently. Young people do not feel ashamed for anybody as they see them for the first and the last time in their opinion.  ⁇ br> Parents and teachers should make kids responsible for their acts. For example, show them how they should get rid of litter by their own examples. That is important because people tend to leave litter in places which are already full with it. That corresponds with the \"\"theory of broken windows\"\". Raise them with the idea of equality of people.\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: Im 26 years old on friday I like to have dinner eat cake, listen to music and play party games <br> the party is my house. it starts at 6 oclock.\n",
            "Target: Hi, Im 26 years old on Friday. I like to have dinner, eat cake, listen to music, and play party games. <br> The party is at my house. It starts at 6 oclock. <br> XXX\n",
            "Prediction: Im 26 years old on Friday. I like to have dinner, eat cake, listen to music and play party games.  ⁇ br> The party is at my house. It starts at 6 oclock.\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: The graph gives information about the amount of population in the age group of 65 and over in time period between 1940 and 2040 in three countries: Japan, Sweden and USA. <br> In 1940 the proportion of population in these three countries was almost similar: it was in the frames from 5 to 8 procent. To the present the trends have changed. The best item now has Sweden: the percentage of people aged 65 and over is 20. The lowest proportion is in Japan: it’s nearly 8%, but compared with the percentage of population of old people in Japan in 1940, that was 5, the present situation is better. In the United States the proportion of population aged 65 and over since 1940 to the present has grown on 6%. <br> By 2040 the percentage of population aged 65 and over will reach its peak in all three countries: Japan that had almost all time the lowest items of population on proportion by 2040 can become the country with the population of old people of nearly 27%. Sweden will have the percentage of 25 and the USA will have the promotion of population aged 65 and over of 23%. <br> Thus, the proportion of population in age group of 65 and over in three countries has grown up dramatically from 5-10% to 24% and more.\n",
            "Target: The graph gives information about the amount of population in the age group of 65 and over in time period between 1940 and 2040 in three countries: Japan, Sweden and the USA. <br> In 1940 the proportion of population in these three countries was almost similar: it was within the frames of 5 to 8 percent. at the present time the trends have changed. Sweden leads now: the percentage of people aged 65 and over is 20. The lowest proportion is in Japan: it's nearly 8%, but compared with the percentage of population of old people in Japan in 1940, that was 5, the present situation is better. In the United States the proportion of population aged 65 and over since 1940 to the present has grown by 6%. <br> By 2040 the percentage of population aged 65 and over will reach its peak in all three countries: Japan that had almost all time the lowest figures of older population on proportion by 2040 can become the country with the population of old people of nearly 27%. Sweden will have the percentage of 25 and the USA will have the growth of population aged 65 and over of 23%. <br> Thus, the proportion of population in age group of 65 and over in three countries has grown up dramatically from 5-10% to 24% and more.\n",
            "Prediction: The graph gives information about the amount of population in the age group of 65 and over in the time period between 1940 and 2040 in three countries: Japan, Sweden and USA.  ⁇ br> In 1940 the proportion of population in these three countries was almost similar: it was in the frames from 5 to 8 procent. To the present the trends have changed. The best item now has Sweden: the percentage of people aged 65 and over is 20. The lowest proportion is in Japan: it’s nearly 8%, but compared with the percentage of population of old people in Japan in 1940, that was 5, the present situation is better. In the United States the proportion of population aged 65 and over from 1940 to the present has grown by 6%.  ⁇ br> By 2040 the percentage of population aged 65 and over will reach its peak in all three countries: Japan, which had almost all time the lowest proportion of population by 2040, could become the country with the population of old people of nearly 27%. Sweden will have the percentage of 25 and the USA will have the promotion of population aged 65 and over of 23%.  ⁇ br> Thus, the proportion of population in the age group of 65 and over in three countries has grown up dramatically from 5-10% to 24% and more.\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: \"Nowadays, the sport activity is a widespread ubiqouty throughout the whole world. Therefore, as any significant phenomenon in our contemporary community, the sport issue of the day has an overwhelming number of advantages and downsides. In fact, the vast majority of people assume, that illegitimate using drugs in the sport sphere should be panished by banning sportmen for a several period of time, despite others being strongly believe, that athlets should be banned for the rest of their life for their serious commitment and crime. Both of view should not be underestimated and must be examined very carefully. <br> The first point to be considered is a restricted period of competing activity of athletes, whom used the forbidden drugs. It is the obvious answer, that sportmen must be panished, neverthe less, the tough cross on their carrier can not be done. Today, it is trully normal phenomenon of drugs destribution in our society, but the nationalwide authority and local citizens must take a chance to retrieve sportmens character. Otherwise, a considerable amount of athletes, should not be participated on the international competitions and introduced the glory of their own countries. Undoubtedly, it would an agonyzing and significant impact for same and public community. <br> On the other hand, banning the professional carrier for any sportmen is a plausible decision to restrict the illegal activity in the healthcare sphere. \"\"To get rid\"\" every athlet for commitment the rule is a prosprective way to handle this clumsy challenge. However, it is trully be to say, that this essential decision should be accsepted by an international counsil, where every state can represent its opinion about resolving the deep issue. <br> To conclude, the contemporary society has receantly devided on two opposite sides: prohibit the profiesional carrier for a limited period or ban every athlet, who using drugs for the rest of their lives. In my opinions, there is no a certainly way to tackle this deep issue, becoming any worse, nowadays, athough the goverment should make an afford to resolve it in the nearest future.\"\n",
            "Target: \"Nowadays, the sport activity is a widespread ubiqouty throughout the whole world. Therefore, as any significant phenomenon in our contemporary community, the sport issue of the day has an overwhelming number of advantages and downsides. In fact, the vast majority of people assume, that illegitimate using drugs in the sport sphere should be panished by banning sportmen for a several period of time, despite others being strongly believe, that athlets should be banned for the rest of their life for their serious commitment and crime. Both of view should not be underestimated and must be examined very carefully. <br> The first point to be considered is a restricted period of competing activity of athletes, whom used the forbidden drugs. It is the obvious answer, that sportmen must be panished, neverthe less, the tough cross on their carrier can not be done. Today, it is trully normal phenomenon of drugs destribution in our society, but the nationalwide authority and local citizens must take a chance to retrieve sportmens character. Otherwise, a considerable amount of athletes, should not be participated on the international competitions and introduced the glory of their own countries. Undoubtedly, it would an agonyzing and significant impact for same and public community. <br> On the other hand, banning the professional carrier for any sportmen is a plausible decision to restrict the illegal activity in the healthcare sphere. \"\"To get rid\"\" every athlet for commitment the rule is a prosprective way to handle this clumsy challenge. However, it is trully be to say, that this essential decision should be accsepted by an international counsil, where every state can represent its opinion about resolving the deep issue. <br> To conclude, the contemporary society has receantly devided on two opposite sides: prohibit the profiesional carrier for a limited period or ban every athlet, who using drugs for the rest of their lives. In my opinions, there is no a certainly way to tackle this deep issue, becoming any worse, nowadays, athough the goverment should make an afford to resolve it in the nearest future.\"\n",
            "Prediction: \"Nowadays, sports activity is a widespread phenomenon throughout the whole world. Therefore, as any significant phenomenon in our contemporary community, the sport issue of the day has an overwhelming number of advantages and downsides. In fact, the vast majority of people assume, that illegitimate using drugs in the sport sphere should be prevented by banning sportsmen for a period of time, despite others strongly believing that athletes should be banned for the rest of their lives for their serious commitment and crime. Both of these views should not be underestimated and must be examined very carefully.  ⁇ br> The first point to be considered is a restricted period of competing activity of athletes who use the forbidden drugs. It is the obvious answer, that sportsmen must be banned. Nevertheless, the tough cross on their carrier cannot be done. Today, it is a true normal phenomenon of drugs distribution in our society, but the nationalwide authority and local citizens must take a chance to retrieve sportsmens character. Otherwise, a considerable number of athletes should not participate in international competitions and introduce the glory of their own countries. Undoubtedly, it would have an agonyzing and significant impact on the same and the public community.  ⁇ br> On the other hand, banning the professional carrier for any sportsmen is a plausible decision to restrict the illegal activity in the healthcare sphere. \"\"To get rid\"\" every athlete for commitment the rule is a proprective way to handle this clumsy challenge. However, it is true to say that this essential decision should be accsepted by an international council, where every state can represent its opinion about resolving the deep issue.  ⁇ br> To conclude, contemporary society has receantly divided on two opposite sides: prohibiting professional carriers for a limited period or banning every athlete who uses drugs for the rest of their lives. In my opinion, there is no a certain way to tackle this deep issue, becoming worse, nowadays, though the government should make an afford to resolve it in the near future.\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: Good evening,How are you?Im fine,thanks.Were busy,good night!\n",
            "Target: Good evening, How are you?Im fine, thanks. Were busy, good night!\n",
            "Prediction: Good evening, How are you? Im fine, thanks. Were busy, good night!\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: \"Id like to tell you about my favorite restaurant its name is \"\"Lemon\"\" I go there every week it has different food from any restaurants I'd like chicken crispy with garlic sauce It's Excellent choice for me and My favorite appitizer is susage and in order that desert I'd rather \"\"Vadge\"\" cake with chocolate sauce I feel I;m at ease when I go there I enjoy classical music over having a lunch about the service It's very good and all staff are respectable I can't imagine one week without going there that drives me nuts I advise any one to go there and enjoy a time there , also this restaurant has a relative advantage in hygiene really Its excellent This strikingly thing for anyone despite all of these advantages prices are not expensive .\"\n",
            "Target: \"Id like to tell you about my favorite restaurant. It's name is \"\"Lemon\"\". I go there every week. It has different food to other restaurants. I like crispy chicken with garlic sauce. It's excellent choice for me. And my favorite appetizer is sausage and in order that dessert I like \"\"Vadge\"\" cake with chocolate sauce. I feel at ease when I go there. I enjoy classical music while having lunch. About the service: it's very good and all the staff are respectable. I can't imagine one week without going there. That would drive me nuts. I advise everyone to go there and enjoy their time there. Also, this restaurant has a relative advantage in hygiene really. Its excellent. The striking thing for anyone is that despite all of these advantages, the prices are not expensive.\"\n",
            "Prediction: \"Id like to tell you about my favorite restaurant. Its name is \"\"Lemon\"\". I go there every week. It has different food from any restaurant. I like chicken crispy with garlic sauce. It's an excellent choice for me. My favorite appetizer is susage and in order for that dessert I'd rather have \"\"Vadge\"\" cake with chocolate sauce. I feel I'm at ease when I go there. I enjoy classical music over having a lunch. About the service, it's very good and all the staff are respectable. I can't imagine one week without going there. That drives me nuts. I advise anyone to go there and have a good time there. Also, this restaurant has a relative advantage in hygiene. Really, its excellent. This is a striking thing for anyone. Despite all of these advantages, prices are not expensive.\"\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: Last but not least, reducing public spending on the aged helps to stress filial obligation. Because little medical and welfare programs are launched when decrease government expenditure on the aging population, many people may have not enough money to send their old parents to rest home. They have to spend more time to take care of their parents by themselves. So, there are more opportunities for descendants to express their love and gratitude to their parents and grandparents. As a result, family sentiment is enhanced significantly in the society. <br> In conclusion, because of enhancing economic growth, ensuring better lives for future generation of older people and stressing filial obligation, limiting public spending on the aged in order to invest in other prior areas of country's development is a good approach to solve global aging problem. In order to make the most of the limited budget for aging population, government should concentrate to support people who really need aid such as older single women.\n",
            "Target: Last but not least, reducing public spending on the aged helps to stress filial obligation. Because little medical and welfare programs are launched when decrease government expenditure on the aging population, many people may have not enough money to send their old parents to rest home. They have to spend more time to take care of their parents by themselves. So, there are more opportunities for descendants to express their love and gratitude to their parents and grandparents. As a result, family sentiment is enhanced significantly in the society. <br> In conclusion, because of enhanced economic growth, ensuring better lives for future generations of older people and stressing filial obligation, limiting public spending on the aged in order to invest in other areas of the country's development is a good approach to solve the global aging problem. In order to make the most of the limited budget for the aging population, the government should concentrate on supporting people who really need aid such as older single women.\n",
            "Prediction: Last but not least, reducing public spending on the aged helps to stress filial obligation. Because little medical and welfare programs are launched when decrease government expenditure on the aging population, many people may have not enough money to send their old parents to rest home. They have to spend more time to take care of their parents by themselves. So, there are more opportunities for descendants to express their love and gratitude to their parents and grandparents. As a result, family sentiment is enhanced significantly in the society.  ⁇ br> In conclusion, because of enhancing economic growth, ensuring better lives for future generations of older people and stressing filial obligation, limiting public spending on the aged in order to invest in other prior areas of the country's development is a good approach to solve the global aging problem. In order to make the most of the limited budget for aging population, the government should concentrate on supporting people who really need aid such as older single women.\n",
            "Counted as Correct? False\n",
            "\n",
            "Input: correction: Dear Sir or Madam, <br> I am writting to you about the show. I was very disappointed after this show. I would like to have my money back. At first the show started at 10.15, and it should be at 19.30. <br> After your show I wanted to visit my friends, and because of it, I didnt do it. <br> Also, you wrote about discounts, but it's not true! The woman, who sold a ticket was very roud with me, actually she started swearing at me. <br> During the show, I was very hungry but I hoped to visit restaurant, it's a pity, but restaurant was closed because it was too late. <br> And, I forget to say about actor, I didnt enjoy them. <br> So, I hope I will get my money back, it was very disappointing evening out in my life! I hope you understand me! <br> your sincerely <br> Ongelina\n",
            "Target: Dear Sir or Madam, <br> I am writing to you about the show. I was very disappointed after the show. I would like to have my money back. First the show started at 10.15, and it should have started at 19.30. <br> After your show I wanted to visit my friends, and because of it, I couldnt do that. <br> Also, you wrote about discounts, but it was not true! The woman who sold me a ticket was very rude to me, actually she started swearing at me. <br> During the show, I was very hungry but I hoped to visit the restaurant, which was a pity, because the restaurant was closed because it was too late. <br> And, I forget to say something about the actors, I didnt enjoy them. <br> So, I hope I will get my money back. It was a very disappointing evening in my life! I hope you understand me! <br> Yours sincerely <br> Ongelina\n",
            "Prediction: Dear Sir or Madam,  ⁇ br> I am writing to you about the show. I was very disappointed after this show. I would like to have my money back. At first the show started at 10.15, and it should have been at 19.30.  ⁇ br> After your show I wanted to visit my friends, and because of that, I didnt do it.  ⁇ br> Also, you wrote about discounts, but it's not true! The woman who sold a ticket was very rude to me. Actually she started swearing at me.  ⁇ br> During the show, I was very hungry but I hoped to visit a restaurant. It's a pity, but the restaurant was closed because it was too late.  ⁇ br> And, I forgot to say about the actors. I didnt enjoy them.  ⁇ br> So, I hope I will get my money back. It was a very disappointing evening out in my life! I hope you understand me!  ⁇ br> yours sincerely  ⁇ br> Ongelina\n",
            "Counted as Correct? False\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxElhphBZMD5",
        "colab_type": "text"
      },
      "source": [
        "# Export Model for Serving\n",
        "\n",
        "As mentioned in the previous section, exporting a [`SavedModel`](https://www.tensorflow.org/guide/saved_model) can be useful for improving performance during inference or allowing your model to be deployed on a variety of platforms (e.g., TFLite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub).\n",
        "\n",
        "**Note:** we currently only support exporting a SavedModel that runs on both CPU and GPU, not TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_YuEL9FZ-UR",
        "colab_type": "text"
      },
      "source": [
        "## Export SavedModel\n",
        "\n",
        "We first export the SavedModel. We set a batch size of 1 for simplicity, but it may be more efficient to use a larger batch size if you want to handle multiple requests per call.\n",
        "\n",
        "For 3B and 11B models the export will take approximately 30-45 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWu8lbh3aHjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9b44a45-e376-4d4a-8ed5-367006b31578"
      },
      "source": [
        "%%time\n",
        "\n",
        "export_dir = os.path.join(MODEL_DIR, \"export\")\n",
        "\n",
        "model.batch_size = 1 # make one prediction per call\n",
        "saved_model_path = model.export(\n",
        "    export_dir,\n",
        "    checkpoint_step=1003800,  # use most recent\n",
        "    beam_size=1,  # no beam search\n",
        "    temperature=1.0,  # sample according to predicted distribution\n",
        ")\n",
        "print(\"Model saved to:\", saved_model_path)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://ml-bucket-isikus/t5-base-model/models/base-basedrei', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.42.24.138:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.42.24.138:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.42.24.138:8470', '_evaluation_master': 'grpc://10.42.24.138:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc709ddb860>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running infer on CPU/GPU\n",
            "INFO:tensorflow:feature inputs : Tensor(\"Reshape:0\", shape=(1, 512), dtype=int32)\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mesh_tensorflow/transformer/dataset.py:375: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias size 384          slice_size 384          Shape[heads=12, buckets=32]                                 \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/k                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/o                size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/q                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/v                size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable decoder/final_layer_norm/scale                               size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 384          slice_size 384          Shape[heads=12, buckets=32]                                 \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/k                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/o                  size 589824       slice_size 589824       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/q                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/v                  size 589824       slice_size 589824       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 2359296      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 2359296      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/layer_norm/scale                 size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable encoder/final_layer_norm/scale                               size 768          slice_size 768          Shape[d_model=768]                                          \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 24674304     slice_size 24674304     Shape[vocab=32128, d_model=768]                             \n",
            "INFO:tensorflow:Trainable Variables            count: 257     Total size: 222903552        Total slice_size: 222903552      \n",
            "INFO:tensorflow:All Variables                  count: 257     Total size: 222903552        Total slice_size: 222903552      \n",
            "INFO:tensorflow:Counters:\n",
            "einsum: 1.57e+11\n",
            "einsum_unique: 1.57e+11\n",
            "output: 1.96e+09\n",
            " output/AddOperation: 4.55e+08\n",
            " output/BinaryOpWithBroadcasting: 1.57e+07\n",
            " output/Constant: 9.44e+06\n",
            " output/EinsumOperation: 4.12e+08\n",
            " output/ImportOperation: 623\n",
            " output/MinMaxOperation: 9.44e+06\n",
            " output/OneHotOperation: 2.34e+08\n",
            " output/RangeOperation: 1.02e+03\n",
            " output/ReduceOperation: 4.74e+05\n",
            " output/ReshapeOperation: 8.06e+07\n",
            " output/ScalarAddOperation: 1.26e+07\n",
            " output/ScalarMultiplyOperation: 2.87e+07\n",
            " output/ShiftOperation: 512\n",
            " output/SlicewiseOperation: 3.52e+08\n",
            " output/StopGradient: 1.13e+08\n",
            " output/Variable: 2.23e+08\n",
            " output/WhileLoopOperation: 9.44e+06\n",
            "output_unique: 1.96e+09\n",
            " output_unique/AddOperation: 4.55e+08\n",
            " output_unique/BinaryOpWithBroadcasting: 1.57e+07\n",
            " output_unique/Constant: 9.44e+06\n",
            " output_unique/EinsumOperation: 4.12e+08\n",
            " output_unique/ImportOperation: 623\n",
            " output_unique/MinMaxOperation: 9.44e+06\n",
            " output_unique/OneHotOperation: 2.34e+08\n",
            " output_unique/RangeOperation: 1.02e+03\n",
            " output_unique/ReduceOperation: 4.74e+05\n",
            " output_unique/ReshapeOperation: 8.06e+07\n",
            " output_unique/ScalarAddOperation: 1.26e+07\n",
            " output_unique/ScalarMultiplyOperation: 2.87e+07\n",
            " output_unique/ShiftOperation: 512\n",
            " output_unique/SlicewiseOperation: 3.52e+08\n",
            " output_unique/StopGradient: 1.13e+08\n",
            " output_unique/Variable: 2.23e+08\n",
            " output_unique/WhileLoopOperation: 9.44e+06\n",
            "variables: 2.23e+08\n",
            " variables/trainable: 2.23e+08\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/model.ckpt-1003800\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/export/temp-1596970550/saved_model.pb\n",
            "Model saved to: b'gs://ml-bucket-isikus/t5-base-model/models/base-basedrei/export/1596970550'\n",
            "CPU times: user 16.2 s, sys: 2.37 s, total: 18.6 s\n",
            "Wall time: 1min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}